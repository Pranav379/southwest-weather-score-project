{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eff8274",
   "metadata": {},
   "source": [
    "# Merged BTS and Weather Data"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
=======
   "execution_count": 6,
   "id": "14a8d33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BTS On-Time Performance Data Downloader\n",
      "Downloading data for January through December 2024...\n",
      "============================================================\n",
      "\n",
      "Downloading data for 2024-01...\n",
      "  URL: https://www.transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2024_1.zip\n",
      "  ‚úì Downloaded 547,271 rows for 2024-01\n",
      "  Waiting 3 seconds...\n",
      "\n",
      "  ‚úì Downloaded 547,271 rows for 2024-01\n",
      "  Waiting 3 seconds...\n",
      "\n",
      "Downloading data for 2024-02...\n",
      "  URL: https://www.transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2024_2.zip\n",
      "Downloading data for 2024-02...\n",
      "  URL: https://www.transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2024_2.zip\n",
      "  ‚úì Downloaded 519,221 rows for 2024-02\n",
      "  Waiting 3 seconds...\n",
      "\n",
      "  ‚úì Downloaded 519,221 rows for 2024-02\n",
      "  Waiting 3 seconds...\n",
      "\n",
      "Downloading data for 2024-03...\n",
      "  URL: https://www.transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2024_3.zip\n",
      "Downloading data for 2024-03...\n",
      "  URL: https://www.transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2024_3.zip\n",
      "  ‚úì Downloaded 591,767 rows for 2024-03\n",
      "  Waiting 3 seconds...\n",
      "\n",
      "  ‚úì Downloaded 591,767 rows for 2024-03\n",
      "  Waiting 3 seconds...\n",
      "\n",
      "Downloading data for 2024-04...\n",
      "  URL: https://www.transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2024_4.zip\n",
      "Downloading data for 2024-04...\n",
      "  URL: https://www.transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2024_4.zip\n",
      "  ‚úì Downloaded 582,185 rows for 2024-04\n",
      "  Waiting 3 seconds...\n",
      "\n",
      "  ‚úì Downloaded 582,185 rows for 2024-04\n",
      "  Waiting 3 seconds...\n",
      "\n",
      "Downloading data for 2024-05...\n",
      "  URL: https://www.transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2024_5.zip\n",
      "Downloading data for 2024-05...\n",
      "  URL: https://www.transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2024_5.zip\n",
      "  ‚úì Downloaded 609,743 rows for 2024-05\n",
      "  Waiting 3 seconds...\n",
      "\n",
      "  ‚úì Downloaded 609,743 rows for 2024-05\n",
      "  Waiting 3 seconds...\n",
      "\n",
      "Downloading data for 2024-06...\n",
      "  URL: https://www.transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2024_6.zip\n",
      "Downloading data for 2024-06...\n",
      "  URL: https://www.transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2024_6.zip\n",
      "  ‚úì Downloaded 611,132 rows for 2024-06\n",
      "  Waiting 3 seconds...\n",
      "\n",
      "  ‚úì Downloaded 611,132 rows for 2024-06\n",
      "  Waiting 3 seconds...\n",
      "\n",
      "Downloading data for 2024-07...\n",
      "  URL: https://www.transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2024_7.zip\n",
      "Downloading data for 2024-07...\n",
      "  URL: https://www.transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2024_7.zip\n",
      "  ‚úì Downloaded 634,613 rows for 2024-07\n",
      "  ‚úì Downloaded 634,613 rows for 2024-07\n",
      "  Waiting 3 seconds...\n",
      "\n",
      "  Waiting 3 seconds...\n",
      "\n",
      "Downloading data for 2024-08...\n",
      "  URL: https://www.transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2024_8.zip\n",
      "Downloading data for 2024-08...\n",
      "  URL: https://www.transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2024_8.zip\n",
      "  ‚úì Downloaded 619,025 rows for 2024-08\n",
      "  Waiting 3 seconds...\n",
      "\n",
      "  ‚úì Downloaded 619,025 rows for 2024-08\n",
      "  Waiting 3 seconds...\n",
      "\n",
      "Downloading data for 2024-09...\n",
      "  URL: https://www.transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2024_9.zip\n",
      "Downloading data for 2024-09...\n",
      "  URL: https://www.transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2024_9.zip\n",
      "  ‚úì Downloaded 582,622 rows for 2024-09\n",
      "  Waiting 3 seconds...\n",
      "\n",
      "  ‚úì Downloaded 582,622 rows for 2024-09\n",
      "  Waiting 3 seconds...\n",
      "\n",
      "Downloading data for 2024-10...\n",
      "  URL: https://www.transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2024_10.zip\n",
      "Downloading data for 2024-10...\n",
      "  URL: https://www.transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2024_10.zip\n",
      "  ‚úì Downloaded 615,497 rows for 2024-10\n",
      "  Waiting 3 seconds...\n",
      "\n",
      "  ‚úì Downloaded 615,497 rows for 2024-10\n",
      "  Waiting 3 seconds...\n",
      "\n",
      "Downloading data for 2024-11...\n",
      "  URL: https://www.transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2024_11.zip\n",
      "Downloading data for 2024-11...\n",
      "  URL: https://www.transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2024_11.zip\n",
      "  ‚úì Downloaded 575,404 rows for 2024-11\n",
      "  Waiting 3 seconds...\n",
      "\n",
      "  ‚úì Downloaded 575,404 rows for 2024-11\n",
      "  Waiting 3 seconds...\n",
      "\n",
      "Downloading data for 2024-12...\n",
      "  URL: https://www.transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2024_12.zip\n",
      "Downloading data for 2024-12...\n",
      "  URL: https://www.transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2024_12.zip\n",
      "  ‚úì Downloaded 590,581 rows for 2024-12\n",
      "\n",
      "============================================================\n",
      "Download Summary:\n",
      "  Successful: 12 months\n",
      "    Months: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "============================================================\n",
      "\n",
      "Combining data...\n",
      "  ‚úì Downloaded 590,581 rows for 2024-12\n",
      "\n",
      "============================================================\n",
      "Download Summary:\n",
      "  Successful: 12 months\n",
      "    Months: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "============================================================\n",
      "\n",
      "Combining data...\n",
      "‚úì Combined 7,079,061 total rows from 12 months\n",
      "\n",
      "Saving to bts_ontime_2024_combined.csv...\n",
      "‚úì Combined 7,079,061 total rows from 12 months\n",
      "\n",
      "Saving to bts_ontime_2024_combined.csv...\n",
      "\n",
      "============================================================\n",
      "‚úì SUCCESS! Data saved to: bts_ontime_2024_combined.csv\n",
      "============================================================\n",
      "Statistics:\n",
      "  Total rows: 7,079,061\n",
      "  Total columns: 110\n",
      "  File size: 2479.7 MB\n",
      "\n",
      "Column names (110 total):\n",
      "  1. Year\n",
      "  2. Quarter\n",
      "  3. Month\n",
      "  4. DayofMonth\n",
      "  5. DayOfWeek\n",
      "  6. FlightDate\n",
      "  7. Reporting_Airline\n",
      "  8. DOT_ID_Reporting_Airline\n",
      "  9. IATA_CODE_Reporting_Airline\n",
      "  10. Tail_Number\n",
      "  11. Flight_Number_Reporting_Airline\n",
      "  12. OriginAirportID\n",
      "  13. OriginAirportSeqID\n",
      "  14. OriginCityMarketID\n",
      "  15. Origin\n",
      "  16. OriginCityName\n",
      "  17. OriginState\n",
      "  18. OriginStateFips\n",
      "  19. OriginStateName\n",
      "  20. OriginWac\n",
      "  21. DestAirportID\n",
      "  22. DestAirportSeqID\n",
      "  23. DestCityMarketID\n",
      "  24. Dest\n",
      "  25. DestCityName\n",
      "  26. DestState\n",
      "  27. DestStateFips\n",
      "  28. DestStateName\n",
      "  29. DestWac\n",
      "  30. CRSDepTime\n",
      "  31. DepTime\n",
      "  32. DepDelay\n",
      "  33. DepDelayMinutes\n",
      "  34. DepDel15\n",
      "  35. DepartureDelayGroups\n",
      "  36. DepTimeBlk\n",
      "  37. TaxiOut\n",
      "  38. WheelsOff\n",
      "  39. WheelsOn\n",
      "  40. TaxiIn\n",
      "  41. CRSArrTime\n",
      "  42. ArrTime\n",
      "  43. ArrDelay\n",
      "  44. ArrDelayMinutes\n",
      "  45. ArrDel15\n",
      "  46. ArrivalDelayGroups\n",
      "  47. ArrTimeBlk\n",
      "  48. Cancelled\n",
      "  49. CancellationCode\n",
      "  50. Diverted\n",
      "  51. CRSElapsedTime\n",
      "  52. ActualElapsedTime\n",
      "  53. AirTime\n",
      "  54. Flights\n",
      "  55. Distance\n",
      "  56. DistanceGroup\n",
      "  57. CarrierDelay\n",
      "  58. WeatherDelay\n",
      "  59. NASDelay\n",
      "  60. SecurityDelay\n",
      "  61. LateAircraftDelay\n",
      "  62. FirstDepTime\n",
      "  63. TotalAddGTime\n",
      "  64. LongestAddGTime\n",
      "  65. DivAirportLandings\n",
      "  66. DivReachedDest\n",
      "  67. DivActualElapsedTime\n",
      "  68. DivArrDelay\n",
      "  69. DivDistance\n",
      "  70. Div1Airport\n",
      "  71. Div1AirportID\n",
      "  72. Div1AirportSeqID\n",
      "  73. Div1WheelsOn\n",
      "  74. Div1TotalGTime\n",
      "  75. Div1LongestGTime\n",
      "  76. Div1WheelsOff\n",
      "  77. Div1TailNum\n",
      "  78. Div2Airport\n",
      "  79. Div2AirportID\n",
      "  80. Div2AirportSeqID\n",
      "  81. Div2WheelsOn\n",
      "  82. Div2TotalGTime\n",
      "  83. Div2LongestGTime\n",
      "  84. Div2WheelsOff\n",
      "  85. Div2TailNum\n",
      "  86. Div3Airport\n",
      "  87. Div3AirportID\n",
      "  88. Div3AirportSeqID\n",
      "  89. Div3WheelsOn\n",
      "  90. Div3TotalGTime\n",
      "  91. Div3LongestGTime\n",
      "  92. Div3WheelsOff\n",
      "  93. Div3TailNum\n",
      "  94. Div4Airport\n",
      "  95. Div4AirportID\n",
      "  96. Div4AirportSeqID\n",
      "  97. Div4WheelsOn\n",
      "  98. Div4TotalGTime\n",
      "  99. Div4LongestGTime\n",
      "  100. Div4WheelsOff\n",
      "  101. Div4TailNum\n",
      "  102. Div5Airport\n",
      "  103. Div5AirportID\n",
      "  104. Div5AirportSeqID\n",
      "  105. Div5WheelsOn\n",
      "  106. Div5TotalGTime\n",
      "  107. Div5LongestGTime\n",
      "  108. Div5WheelsOff\n",
      "  109. Div5TailNum\n",
      "  110. Unnamed: 109\n",
      "\n",
      "Data preview (first 3 rows):\n",
      "\n",
      "============================================================\n",
      "‚úì SUCCESS! Data saved to: bts_ontime_2024_combined.csv\n",
      "============================================================\n",
      "Statistics:\n",
      "  Total rows: 7,079,061\n",
      "  Total columns: 110\n",
      "  File size: 2479.7 MB\n",
      "\n",
      "Column names (110 total):\n",
      "  1. Year\n",
      "  2. Quarter\n",
      "  3. Month\n",
      "  4. DayofMonth\n",
      "  5. DayOfWeek\n",
      "  6. FlightDate\n",
      "  7. Reporting_Airline\n",
      "  8. DOT_ID_Reporting_Airline\n",
      "  9. IATA_CODE_Reporting_Airline\n",
      "  10. Tail_Number\n",
      "  11. Flight_Number_Reporting_Airline\n",
      "  12. OriginAirportID\n",
      "  13. OriginAirportSeqID\n",
      "  14. OriginCityMarketID\n",
      "  15. Origin\n",
      "  16. OriginCityName\n",
      "  17. OriginState\n",
      "  18. OriginStateFips\n",
      "  19. OriginStateName\n",
      "  20. OriginWac\n",
      "  21. DestAirportID\n",
      "  22. DestAirportSeqID\n",
      "  23. DestCityMarketID\n",
      "  24. Dest\n",
      "  25. DestCityName\n",
      "  26. DestState\n",
      "  27. DestStateFips\n",
      "  28. DestStateName\n",
      "  29. DestWac\n",
      "  30. CRSDepTime\n",
      "  31. DepTime\n",
      "  32. DepDelay\n",
      "  33. DepDelayMinutes\n",
      "  34. DepDel15\n",
      "  35. DepartureDelayGroups\n",
      "  36. DepTimeBlk\n",
      "  37. TaxiOut\n",
      "  38. WheelsOff\n",
      "  39. WheelsOn\n",
      "  40. TaxiIn\n",
      "  41. CRSArrTime\n",
      "  42. ArrTime\n",
      "  43. ArrDelay\n",
      "  44. ArrDelayMinutes\n",
      "  45. ArrDel15\n",
      "  46. ArrivalDelayGroups\n",
      "  47. ArrTimeBlk\n",
      "  48. Cancelled\n",
      "  49. CancellationCode\n",
      "  50. Diverted\n",
      "  51. CRSElapsedTime\n",
      "  52. ActualElapsedTime\n",
      "  53. AirTime\n",
      "  54. Flights\n",
      "  55. Distance\n",
      "  56. DistanceGroup\n",
      "  57. CarrierDelay\n",
      "  58. WeatherDelay\n",
      "  59. NASDelay\n",
      "  60. SecurityDelay\n",
      "  61. LateAircraftDelay\n",
      "  62. FirstDepTime\n",
      "  63. TotalAddGTime\n",
      "  64. LongestAddGTime\n",
      "  65. DivAirportLandings\n",
      "  66. DivReachedDest\n",
      "  67. DivActualElapsedTime\n",
      "  68. DivArrDelay\n",
      "  69. DivDistance\n",
      "  70. Div1Airport\n",
      "  71. Div1AirportID\n",
      "  72. Div1AirportSeqID\n",
      "  73. Div1WheelsOn\n",
      "  74. Div1TotalGTime\n",
      "  75. Div1LongestGTime\n",
      "  76. Div1WheelsOff\n",
      "  77. Div1TailNum\n",
      "  78. Div2Airport\n",
      "  79. Div2AirportID\n",
      "  80. Div2AirportSeqID\n",
      "  81. Div2WheelsOn\n",
      "  82. Div2TotalGTime\n",
      "  83. Div2LongestGTime\n",
      "  84. Div2WheelsOff\n",
      "  85. Div2TailNum\n",
      "  86. Div3Airport\n",
      "  87. Div3AirportID\n",
      "  88. Div3AirportSeqID\n",
      "  89. Div3WheelsOn\n",
      "  90. Div3TotalGTime\n",
      "  91. Div3LongestGTime\n",
      "  92. Div3WheelsOff\n",
      "  93. Div3TailNum\n",
      "  94. Div4Airport\n",
      "  95. Div4AirportID\n",
      "  96. Div4AirportSeqID\n",
      "  97. Div4WheelsOn\n",
      "  98. Div4TotalGTime\n",
      "  99. Div4LongestGTime\n",
      "  100. Div4WheelsOff\n",
      "  101. Div4TailNum\n",
      "  102. Div5Airport\n",
      "  103. Div5AirportID\n",
      "  104. Div5AirportSeqID\n",
      "  105. Div5WheelsOn\n",
      "  106. Div5TotalGTime\n",
      "  107. Div5LongestGTime\n",
      "  108. Div5WheelsOff\n",
      "  109. Div5TailNum\n",
      "  110. Unnamed: 109\n",
      "\n",
      "Data preview (first 3 rows):\n",
      "   Year  Quarter  Month  DayofMonth  DayOfWeek  ... Div5TotalGTime Div5LongestGTime  Div5WheelsOff Div5TailNum  \\\n",
      "0  2024        1      1           8          1  ...            NaN              NaN            NaN         NaN   \n",
      "1  2024        1      1           9          2  ...            NaN              NaN            NaN         NaN   \n",
      "2  2024        1      1          10          3  ...            NaN              NaN            NaN         NaN   \n",
      "\n",
      "  Unnamed: 109  \n",
      "0          NaN  \n",
      "1          NaN  \n",
      "2          NaN  \n",
      "\n",
      "[3 rows x 110 columns]\n",
      "============================================================\n",
      "   Year  Quarter  Month  DayofMonth  DayOfWeek  ... Div5TotalGTime Div5LongestGTime  Div5WheelsOff Div5TailNum  \\\n",
      "0  2024        1      1           8          1  ...            NaN              NaN            NaN         NaN   \n",
      "1  2024        1      1           9          2  ...            NaN              NaN            NaN         NaN   \n",
      "2  2024        1      1          10          3  ...            NaN              NaN            NaN         NaN   \n",
      "\n",
      "  Unnamed: 109  \n",
      "0          NaN  \n",
      "1          NaN  \n",
      "2          NaN  \n",
      "\n",
      "[3 rows x 110 columns]\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "\n",
    "def download_bts_data(year, month):\n",
    "    \"\"\"\n",
    "    Download BTS on-time performance data for a specific month\n",
    "    \n",
    "    Args:\n",
    "        year: Year (e.g., 2024)\n",
    "        month: Month (1-12)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with the monthly data\n",
    "    \"\"\"\n",
    "    # Using the direct download endpoint\n",
    "    url = 'https://www.transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_{year}_{month}.zip'.format(\n",
    "        year=year, \n",
    "        month=month\n",
    "    )\n",
    "    \n",
    "    print(f\"Downloading data for {year}-{month:02d}...\")\n",
    "    print(f\"  URL: {url}\")\n",
    "    \n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers, timeout=180, stream=True)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # The file should be a zip\n",
    "        try:\n",
    "            z = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "            csv_files = [name for name in z.namelist() if name.endswith('.csv')]\n",
    "            \n",
    "            if not csv_files:\n",
    "                print(f\"  ‚úó Error: No CSV file found in zip for {year}-{month:02d}\")\n",
    "                print(f\"  Files in zip: {z.namelist()}\")\n",
    "                return None\n",
    "            \n",
    "            # Read the CSV with flexible parsing\n",
    "            with z.open(csv_files[0]) as f:\n",
    "                # Read with error handling for bad lines\n",
    "                df = pd.read_csv(\n",
    "                    f, \n",
    "                    encoding='utf-8',\n",
    "                    on_bad_lines='skip',  # Skip bad lines instead of failing\n",
    "                    engine='python'  # Use python engine which is more flexible\n",
    "                )\n",
    "            \n",
    "            print(f\"  ‚úì Downloaded {len(df):,} rows for {year}-{month:02d}\")\n",
    "            return df\n",
    "            \n",
    "        except zipfile.BadZipFile:\n",
    "            print(f\"  ‚úó Error: Not a valid zip file for {year}-{month:02d}\")\n",
    "            # Save response for debugging\n",
    "            with open(f'debug_response_{year}_{month:02d}.txt', 'wb') as f:\n",
    "                f.write(response.content[:1000])\n",
    "            print(f\"  Response saved to debug_response_{year}_{month:02d}.txt\")\n",
    "            return None\n",
    "        \n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        if e.response.status_code == 404:\n",
    "            print(f\"  ‚úó Data not available for {year}-{month:02d} (404)\")\n",
    "        else:\n",
    "            print(f\"  ‚úó HTTP error for {year}-{month:02d}: {e}\")\n",
    "        return None\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"  ‚úó Timeout error for {year}-{month:02d}\")\n",
    "        return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"  ‚úó Network error for {year}-{month:02d}: {str(e)}\")\n",
    "        return None\n",
    "    except pd.errors.ParserError as e:\n",
    "        print(f\"  ‚úó CSV parsing error for {year}-{month:02d}: {str(e)}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó Unexpected error for {year}-{month:02d}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def combine_monthly_data(year, months=range(1, 13), delay=3):\n",
    "    \"\"\"\n",
    "    Download and combine monthly data for an entire year\n",
    "    \n",
    "    Args:\n",
    "        year: Year to download (e.g., 2024)\n",
    "        months: Range of months (default: 1-12)\n",
    "        delay: Seconds to wait between requests (default: 3)\n",
    "    \n",
    "    Returns:\n",
    "        Combined DataFrame with all monthly data\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    successful_months = []\n",
    "    failed_months = []\n",
    "    \n",
    "    for month in months:\n",
    "        df = download_bts_data(year, month)\n",
    "        \n",
    "        if df is not None and len(df) > 0:\n",
    "            all_data.append(df)\n",
    "            successful_months.append(month)\n",
    "        else:\n",
    "            failed_months.append(month)\n",
    "        \n",
    "        # Wait between requests\n",
    "        if month < max(months):\n",
    "            print(f\"  Waiting {delay} seconds...\\n\")\n",
    "            time.sleep(delay)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Download Summary:\")\n",
    "    print(f\"  Successful: {len(successful_months)} months\")\n",
    "    if successful_months:\n",
    "        print(f\"    Months: {successful_months}\")\n",
    "    if failed_months:\n",
    "        print(f\"  Failed: {len(failed_months)} months\")\n",
    "        print(f\"    Months: {failed_months}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    if all_data:\n",
    "        print(\"Combining data...\")\n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        print(f\"‚úì Combined {len(combined_df):,} total rows from {len(all_data)} months\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"‚úó No data was successfully downloaded\")\n",
    "        return None\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*60)\n",
    "    print(\"BTS On-Time Performance Data Downloader\")\n",
    "    print(\"Downloading data for January through December 2024...\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # Download data for January through December 2024\n",
    "    combined_data = combine_monthly_data(2024)\n",
    "    \n",
    "    if combined_data is not None:\n",
    "        # Save to CSV\n",
    "        output_file = \"bts_ontime_2024_combined.csv\"\n",
    "        print(f\"\\nSaving to {output_file}...\")\n",
    "        combined_data.to_csv(output_file, index=False)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"‚úì SUCCESS! Data saved to: {output_file}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Statistics:\")\n",
    "        print(f\"  Total rows: {len(combined_data):,}\")\n",
    "        print(f\"  Total columns: {len(combined_data.columns)}\")\n",
    "        \n",
    "        # Get file size\n",
    "        file_size = os.path.getsize(output_file) / (1024 * 1024)\n",
    "        print(f\"  File size: {file_size:.1f} MB\")\n",
    "        \n",
    "        print(f\"\\nColumn names ({len(combined_data.columns)} total):\")\n",
    "        for i, col in enumerate(combined_data.columns, 1):\n",
    "            print(f\"  {i}. {col}\")\n",
    "        \n",
    "        print(f\"\\nData preview (first 3 rows):\")\n",
    "        pd.set_option('display.max_columns', 10)\n",
    "        pd.set_option('display.width', 120)\n",
    "        print(combined_data.head(3))\n",
    "        print(f\"{'='*60}\")\n",
    "    else:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"‚úó FAILED to create combined dataset\")\n",
    "        print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
>>>>>>> 4766a69 (Updated with separate aggregated delay and weather data)
   "execution_count": null,
   "id": "3f21c867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faster solution to above for multiple years worth of data\n",
    "import requests\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from datetime import datetime\n",
    "\n",
    "def download_bts_data(year, month):\n",
    "    \"\"\"Download BTS data for a specific month\"\"\"\n",
    "    url = f'https://www.transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_{year}_{month}.zip'\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(\n",
    "            url, \n",
    "            headers={'User-Agent': 'Mozilla/5.0'},\n",
    "            timeout=300,\n",
    "            stream=True  # Stream large files\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Extract CSV from zip\n",
    "        with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "            csv_file = next((name for name in z.namelist() if name.endswith('.csv')), None)\n",
    "            if not csv_file:\n",
    "                return (year, month, None, \"No CSV in zip\")\n",
    "            \n",
    "            with z.open(csv_file) as f:\n",
    "                df = pd.read_csv(f, encoding='utf-8', low_memory=False)\n",
    "            \n",
    "            return (year, month, df, None)\n",
    "            \n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        return (year, month, None, f\"HTTP {e.response.status_code}\")\n",
    "    except Exception as e:\n",
    "        return (year, month, None, str(e)[:50])\n",
    "\n",
    "def download_years_parallel(start_year, end_year, max_workers=20):\n",
    "    \"\"\"\n",
    "    Download multiple years of data with maximum parallelization\n",
    "    \n",
    "    Args:\n",
    "        start_year: Starting year (e.g., 2005)\n",
    "        end_year: Ending year (e.g., 2024)\n",
    "        max_workers: Number of parallel downloads (default: 20)\n",
    "    \n",
    "    Returns:\n",
    "        Combined DataFrame\n",
    "    \"\"\"\n",
    "    # Generate all year-month combinations\n",
    "    months_to_download = []\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        for month in range(1, 13):\n",
    "            # Skip future months\n",
    "            current_date = datetime.now()\n",
    "            if year > current_date.year or (year == current_date.year and month > current_date.month):\n",
    "                continue\n",
    "            months_to_download.append((year, month))\n",
    "    \n",
    "    total_months = len(months_to_download)\n",
    "    print(f\"Downloading {total_months} months ({start_year}-{end_year})\")\n",
    "    print(f\"Using {max_workers} parallel workers\")\n",
    "    print(f\"Estimated time: {(total_months * 2.5) / max_workers:.1f} minutes\\n\")\n",
    "    \n",
    "    results = {}\n",
    "    completed = 0\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # Download all months in parallel\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {\n",
    "            executor.submit(download_bts_data, year, month): (year, month) \n",
    "            for year, month in months_to_download\n",
    "        }\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            year, month, df, error = future.result()\n",
    "            completed += 1\n",
    "            \n",
    "            if df is not None:\n",
    "                results[(year, month)] = df\n",
    "                print(f\"‚úì {year}-{month:02d} ({len(df):,} rows) [{completed}/{total_months}]\")\n",
    "            else:\n",
    "                results[(year, month)] = None\n",
    "                print(f\"‚úó {year}-{month:02d} - {error} [{completed}/{total_months}]\")\n",
    "    \n",
    "    elapsed = (datetime.now() - start_time).total_seconds() / 60\n",
    "    \n",
    "    # Sort and combine successful downloads\n",
    "    successful = [(k, v) for k, v in sorted(results.items()) if v is not None]\n",
    "    failed = [(k, v) for k, v in sorted(results.items()) if v is None]\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Download Complete in {elapsed:.1f} minutes\")\n",
    "    print(f\"  Successful: {len(successful)}/{total_months} months\")\n",
    "    if failed:\n",
    "        print(f\"  Failed: {len(failed)} months - {[f'{y}-{m:02d}' for (y,m), _ in failed[:10]]}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    if successful:\n",
    "        print(\"Combining all data...\")\n",
    "        combine_start = datetime.now()\n",
    "        \n",
    "        # Combine all dataframes at once\n",
    "        combined_df = pd.concat([df for _, df in successful], ignore_index=True, copy=False)\n",
    "        \n",
    "        combine_time = (datetime.now() - combine_start).total_seconds()\n",
    "        print(f\"‚úì Combined {len(combined_df):,} rows in {combine_time:.1f}s\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"‚úó No data downloaded\")\n",
    "        return None\n",
    "\n",
    "def save_to_csv_fast(df, filename):\n",
    "    \"\"\"Save DataFrame to CSV with progress\"\"\"\n",
    "    print(f\"\\nSaving to {filename}...\")\n",
    "    start = datetime.now()\n",
    "    \n",
    "    # Save with efficient settings\n",
    "    df.to_csv(filename, index=False)\n",
    "    \n",
    "    elapsed = (datetime.now() - start).total_seconds()\n",
    "    file_size = os.path.getsize(filename) / (1024 * 1024)\n",
    "    \n",
    "    print(f\"‚úì Saved in {elapsed:.1f}s ({file_size:.1f} MB)\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*70)\n",
    "    print(\"BTS On-Time Performance Data - ULTRA FAST DOWNLOADER\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # Configure your date range here\n",
    "    START_YEAR = 2005\n",
    "    END_YEAR = 2024\n",
    "    MAX_WORKERS = 12  # Balanced: fast but respectful to server\n",
    "    \n",
    "    print(f\"Configuration:\")\n",
    "    print(f\"  Years: {START_YEAR} to {END_YEAR}\")\n",
    "    print(f\"  Parallel workers: {MAX_WORKERS}\")\n",
    "    print(f\"  Note: 240 months = 20 years (2005-2024)\\n\")\n",
    "    \n",
    "    # Download all data\n",
    "    combined_data = download_years_parallel(START_YEAR, END_YEAR, max_workers=MAX_WORKERS)\n",
    "    \n",
    "    if combined_data is not None:\n",
    "        output_file = f\"bts_ontime_{START_YEAR}_{END_YEAR}_combined.csv\"\n",
    "        save_to_csv_fast(combined_data, output_file)\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"‚úì SUCCESS!\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"  File: {output_file}\")\n",
    "        print(f\"  Total rows: {len(combined_data):,}\")\n",
    "        print(f\"  Columns: {len(combined_data.columns)}\")\n",
    "        print(f\"  Date range: {combined_data['FL_DATE'].min()} to {combined_data['FL_DATE'].max()}\")\n",
    "        print(f\"\\nFirst 10 columns:\")\n",
    "        for i, col in enumerate(combined_data.columns[:10], 1):\n",
    "            print(f\"  {i}. {col}\")\n",
    "        print(f\"{'='*70}\")\n",
    "    else:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"‚úó FAILED - No data downloaded\")\n",
    "        print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
<<<<<<< HEAD
=======
   "execution_count": 38,
>>>>>>> 62e6e95 (Changes with combined delay data)
=======
>>>>>>> 4766a69 (Updated with separate aggregated delay and weather data)
   "id": "4c48f4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FL_DATE                   0.000000\n",
      "OP_UNIQUE_CARRIER         0.000000\n",
      "ORIGIN_AIRPORT_ID         0.000000\n",
      "ORIGIN_AIRPORT_SEQ_ID     0.000000\n",
      "ORIGIN_CITY_MARKET_ID     0.000000\n",
      "ORIGIN                    0.000000\n",
      "ORIGIN_CITY_NAME          0.000000\n",
      "ORIGIN_STATE_ABR          0.000000\n",
      "ORIGIN_STATE_FIPS         0.000000\n",
      "ORIGIN_STATE_NM           0.000000\n",
      "ORIGIN_WAC                0.000000\n",
      "DEST_AIRPORT_ID           0.000000\n",
      "DEST_AIRPORT_SEQ_ID       0.000000\n",
      "DEST_CITY_MARKET_ID       0.000000\n",
      "DEST                      0.000000\n",
      "DEST_CITY_NAME            0.000000\n",
      "DEST_STATE_ABR            0.000000\n",
      "DEST_STATE_FIPS           0.000000\n",
      "DEST_STATE_NM             0.000000\n",
      "DEST_WAC                  0.000000\n",
      "CRS_DEP_TIME              0.000000\n",
      "DEP_TIME                  1.661803\n",
      "DEP_DELAY                 1.669160\n",
      "DEP_DELAY_NEW             1.669160\n",
      "DEP_DEL15                 1.669160\n",
      "DEP_DELAY_GROUP           1.669160\n",
      "DEP_TIME_BLK              0.000000\n",
      "TAXI_OUT                  1.706187\n",
      "WHEELS_OFF                1.706187\n",
      "WHEELS_ON                 1.750631\n",
      "TAXI_IN                   1.750631\n",
      "CRS_ARR_TIME              0.000000\n",
      "ARR_TIME                  1.750511\n",
      "ARR_DELAY                 1.942218\n",
      "ARR_DELAY_NEW             1.942218\n",
      "ARR_DEL15                 1.942218\n",
      "ARR_DELAY_GROUP           1.942218\n",
      "ARR_TIME_BLK              0.000000\n",
      "CANCELLED                 0.000000\n",
      "CANCELLATION_CODE        98.280667\n",
      "DIVERTED                  0.000000\n",
      "CRS_ELAPSED_TIME          0.000060\n",
      "ACTUAL_ELAPSED_TIME       1.942218\n",
      "AIR_TIME                  1.942218\n",
      "FLIGHTS                   0.000000\n",
      "DISTANCE                  0.000000\n",
      "DISTANCE_GROUP            0.000000\n",
      "CARRIER_DELAY            80.070785\n",
      "WEATHER_DELAY            80.070785\n",
      "NAS_DELAY                80.070785\n",
      "SECURITY_DELAY           80.070785\n",
      "LATE_AIRCRAFT_DELAY      80.070785\n",
      "FIRST_DEP_TIME           99.291546\n",
      "TOTAL_ADD_GTIME          99.291546\n",
      "LONGEST_ADD_GTIME        99.291546\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
<<<<<<< HEAD
<<<<<<< HEAD
    "# Reading 2024 data\n",
    "df = pd.read_csv(\"bts_ontime_2024_combined.csv\")\n",
=======
    "# Reading Jan and Feb data\n",
    "files = [\"Data/Jan_2024_Delays.csv\", \"Data/Feb_2024_Delays.csv\", \"Data/March_2024_Delays.csv\"]\n",
    "\n",
    "# Load all CSVs into a list\n",
    "dfs = [pd.read_csv(f) for f in files]\n",
    "\n",
    "# Combine into one DataFrame\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
>>>>>>> 62e6e95 (Changes with combined delay data)
=======
    "# Reading 2024 data\n",
    "df = pd.read_csv(\"bts_ontime_2024_combined.csv\")\n",
>>>>>>> 4766a69 (Updated with separate aggregated delay and weather data)
    "\n",
    "percent_M = df.isna().mean() * 100\n",
    "\n",
    "print(percent_M)\n"
   ]
<<<<<<< HEAD
<<<<<<< HEAD
=======
>>>>>>> 4766a69 (Updated with separate aggregated delay and weather data)
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "14997704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ABQ (USW00023050) ...\n",
      "Parsing data for ABQ (USW00023050) ...\n",
      "Parsing data for ABQ (USW00023050) ...\n",
      "Saved CSV for ABQ: sw_airport_daily_weather_bulk\\ABQ_daily_weather.csv\n",
      "Downloading AMA (USW00023020) ...\n",
      "Saved CSV for ABQ: sw_airport_daily_weather_bulk\\ABQ_daily_weather.csv\n",
      "Downloading AMA (USW00023020) ...\n",
      "Parsing data for AMA (USW00023020) ...\n",
      "Parsing data for AMA (USW00023020) ...\n",
      "Saved CSV for AMA: sw_airport_daily_weather_bulk\\AMA_daily_weather.csv\n",
      "Downloading ATL (USW00013874) ...\n",
      "Saved CSV for AMA: sw_airport_daily_weather_bulk\\AMA_daily_weather.csv\n",
      "Downloading ATL (USW00013874) ...\n",
      "Parsing data for ATL (USW00013874) ...\n",
      "Parsing data for ATL (USW00013874) ...\n",
      "Saved CSV for ATL: sw_airport_daily_weather_bulk\\ATL_daily_weather.csv\n",
      "Downloading AUS (USW00013958) ...\n",
      "Saved CSV for ATL: sw_airport_daily_weather_bulk\\ATL_daily_weather.csv\n",
      "Downloading AUS (USW00013958) ...\n",
      "Parsing data for AUS (USW00013958) ...\n",
      "Parsing data for AUS (USW00013958) ...\n",
      "Saved CSV for AUS: sw_airport_daily_weather_bulk\\AUS_daily_weather.csv\n",
      "Downloading BWI (USW00093721) ...\n",
      "Saved CSV for AUS: sw_airport_daily_weather_bulk\\AUS_daily_weather.csv\n",
      "Downloading BWI (USW00093721) ...\n",
      "Parsing data for BWI (USW00093721) ...\n",
      "Parsing data for BWI (USW00093721) ...\n",
      "Saved CSV for BWI: sw_airport_daily_weather_bulk\\BWI_daily_weather.csv\n",
      "Downloading BUR (USW00093195) ...\n",
      "Saved CSV for BWI: sw_airport_daily_weather_bulk\\BWI_daily_weather.csv\n",
      "Downloading BUR (USW00093195) ...\n",
      "‚ùå Failed to download station USW00093195 for airport BUR\n",
      "Downloading CHS (USW00093824) ...\n",
      "‚ùå Failed to download station USW00093195 for airport BUR\n",
      "Downloading CHS (USW00093824) ...\n",
      "Parsing data for CHS (USW00093824) ...\n",
      "Parsing data for CHS (USW00093824) ...\n",
      "Saved CSV for CHS: sw_airport_daily_weather_bulk\\CHS_daily_weather.csv\n",
      "Downloading CLE (USW00014830) ...\n",
      "Saved CSV for CHS: sw_airport_daily_weather_bulk\\CHS_daily_weather.csv\n",
      "Downloading CLE (USW00014830) ...\n",
      "Parsing data for CLE (USW00014830) ...\n",
      "Parsing data for CLE (USW00014830) ...\n",
      "Saved CSV for CLE: sw_airport_daily_weather_bulk\\CLE_daily_weather.csv\n",
      "Downloading CMH (USW00094846) ...\n",
      "Saved CSV for CLE: sw_airport_daily_weather_bulk\\CLE_daily_weather.csv\n",
      "Downloading CMH (USW00094846) ...\n",
      "Parsing data for CMH (USW00094846) ...\n",
      "Parsing data for CMH (USW00094846) ...\n",
      "Saved CSV for CMH: sw_airport_daily_weather_bulk\\CMH_daily_weather.csv\n",
      "Downloading DAL (USW00013960) ...\n",
      "Saved CSV for CMH: sw_airport_daily_weather_bulk\\CMH_daily_weather.csv\n",
      "Downloading DAL (USW00013960) ...\n",
      "Parsing data for DAL (USW00013960) ...\n",
      "Parsing data for DAL (USW00013960) ...\n",
      "Saved CSV for DAL: sw_airport_daily_weather_bulk\\DAL_daily_weather.csv\n",
      "Downloading DEN (USW00023040) ...\n",
      "Saved CSV for DAL: sw_airport_daily_weather_bulk\\DAL_daily_weather.csv\n",
      "Downloading DEN (USW00023040) ...\n",
      "Parsing data for DEN (USW00023040) ...\n",
      "Parsing data for DEN (USW00023040) ...\n",
      "Saved CSV for DEN: sw_airport_daily_weather_bulk\\DEN_daily_weather.csv\n",
      "Downloading HOU (USW00012918) ...\n",
      "Saved CSV for DEN: sw_airport_daily_weather_bulk\\DEN_daily_weather.csv\n",
      "Downloading HOU (USW00012918) ...\n",
      "Parsing data for HOU (USW00012918) ...\n",
      "Parsing data for HOU (USW00012918) ...\n",
      "Saved CSV for HOU: sw_airport_daily_weather_bulk\\HOU_daily_weather.csv\n",
      "Downloading LAS (USW00024049) ...\n",
      "Saved CSV for HOU: sw_airport_daily_weather_bulk\\HOU_daily_weather.csv\n",
      "Downloading LAS (USW00024049) ...\n",
      "‚ùå Failed to download station USW00024049 for airport LAS\n",
      "Downloading LAX (USW00023174) ...\n",
      "‚ùå Failed to download station USW00024049 for airport LAS\n",
      "Downloading LAX (USW00023174) ...\n",
      "Parsing data for LAX (USW00023174) ...\n",
      "Parsing data for LAX (USW00023174) ...\n",
      "Saved CSV for LAX: sw_airport_daily_weather_bulk\\LAX_daily_weather.csv\n",
      "‚úÖ Station file already exists for MDW (USW00094846)\n",
      "Parsing data for MDW (USW00094846) ...\n",
      "Saved CSV for LAX: sw_airport_daily_weather_bulk\\LAX_daily_weather.csv\n",
      "‚úÖ Station file already exists for MDW (USW00094846)\n",
      "Parsing data for MDW (USW00094846) ...\n",
      "Saved CSV for MDW: sw_airport_daily_weather_bulk\\MDW_daily_weather.csv\n",
      "Downloading MCO (USW00012839) ...\n",
      "Saved CSV for MDW: sw_airport_daily_weather_bulk\\MDW_daily_weather.csv\n",
      "Downloading MCO (USW00012839) ...\n",
      "Parsing data for MCO (USW00012839) ...\n",
      "Parsing data for MCO (USW00012839) ...\n",
      "Saved CSV for MCO: sw_airport_daily_weather_bulk\\MCO_daily_weather.csv\n",
      "Downloading OAK (USW00023272) ...\n",
      "Saved CSV for MCO: sw_airport_daily_weather_bulk\\MCO_daily_weather.csv\n",
      "Downloading OAK (USW00023272) ...\n",
      "Parsing data for OAK (USW00023272) ...\n",
      "Parsing data for OAK (USW00023272) ...\n",
      "Saved CSV for OAK: sw_airport_daily_weather_bulk\\OAK_daily_weather.csv\n",
      "Downloading PHX (USW00023183) ...\n",
      "Saved CSV for OAK: sw_airport_daily_weather_bulk\\OAK_daily_weather.csv\n",
      "Downloading PHX (USW00023183) ...\n",
      "Parsing data for PHX (USW00023183) ...\n",
      "Parsing data for PHX (USW00023183) ...\n",
      "Saved CSV for PHX: sw_airport_daily_weather_bulk\\PHX_daily_weather.csv\n",
      "Downloading SAN (USW00023188) ...\n",
      "Saved CSV for PHX: sw_airport_daily_weather_bulk\\PHX_daily_weather.csv\n",
      "Downloading SAN (USW00023188) ...\n",
      "Parsing data for SAN (USW00023188) ...\n",
      "Parsing data for SAN (USW00023188) ...\n",
      "Saved CSV for SAN: sw_airport_daily_weather_bulk\\SAN_daily_weather.csv\n",
      "Downloading SFO (USW00023234) ...\n",
      "Saved CSV for SAN: sw_airport_daily_weather_bulk\\SAN_daily_weather.csv\n",
      "Downloading SFO (USW00023234) ...\n",
      "Parsing data for SFO (USW00023234) ...\n",
      "Parsing data for SFO (USW00023234) ...\n",
      "Saved CSV for SFO: sw_airport_daily_weather_bulk\\SFO_daily_weather.csv\n",
      "Downloading SJC (USW00023282) ...\n",
      "Saved CSV for SFO: sw_airport_daily_weather_bulk\\SFO_daily_weather.csv\n",
      "Downloading SJC (USW00023282) ...\n",
      "‚ùå Failed to download station USW00023282 for airport SJC\n",
      "Downloading SMF (USW00023266) ...\n",
      "‚ùå Failed to download station USW00023282 for airport SJC\n",
      "Downloading SMF (USW00023266) ...\n",
      "‚ùå Failed to download station USW00023266 for airport SMF\n",
      "Downloading STL (USW00093928) ...\n",
      "‚ùå Failed to download station USW00023266 for airport SMF\n",
      "Downloading STL (USW00093928) ...\n",
      "Parsing data for STL (USW00093928) ...\n",
      "Parsing data for STL (USW00093928) ...\n",
      "Saved CSV for STL: sw_airport_daily_weather_bulk\\STL_daily_weather.csv\n",
      "‚úÖ Station file already exists for TPA (USW00012839)\n",
      "Parsing data for TPA (USW00012839) ...\n",
      "Saved CSV for STL: sw_airport_daily_weather_bulk\\STL_daily_weather.csv\n",
      "‚úÖ Station file already exists for TPA (USW00012839)\n",
      "Parsing data for TPA (USW00012839) ...\n",
      "Saved CSV for TPA: sw_airport_daily_weather_bulk\\TPA_daily_weather.csv\n",
      "Combining all airport CSVs into one bulk CSV...\n",
      "Saved CSV for TPA: sw_airport_daily_weather_bulk\\TPA_daily_weather.csv\n",
      "Combining all airport CSVs into one bulk CSV...\n",
      "üéØ Bulk CSV saved: sw_airport_daily_weather_bulk\\SW_airports_all_daily_weather.csv\n",
      "         DATE  AWND  PRCP  TMAX  TMIN AIRPORT\n",
      "0  1931-03-01   NaN   0.0   7.2  -2.2     ABQ\n",
      "1  1931-03-02   NaN   0.0  13.3  -3.9     ABQ\n",
      "2  1931-03-03   NaN   0.0  17.8   1.1     ABQ\n",
      "3  1931-03-04   NaN   0.0  18.3   0.6     ABQ\n",
      "4  1931-03-05   NaN   0.0  11.1   5.0     ABQ\n",
      "üéØ Bulk CSV saved: sw_airport_daily_weather_bulk\\SW_airports_all_daily_weather.csv\n",
      "         DATE  AWND  PRCP  TMAX  TMIN AIRPORT\n",
      "0  1931-03-01   NaN   0.0   7.2  -2.2     ABQ\n",
      "1  1931-03-02   NaN   0.0  13.3  -3.9     ABQ\n",
      "2  1931-03-03   NaN   0.0  17.8   1.1     ABQ\n",
      "3  1931-03-04   NaN   0.0  18.3   0.6     ABQ\n",
      "4  1931-03-05   NaN   0.0  11.1   5.0     ABQ\n"
     ]
    }
   ],
   "source": [
    "# Combined weather data\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import glob\n",
    "\n",
    "# -----------------------------\n",
    "# Airport ‚Üí GHCN‚ÄëDaily Station ID mapping\n",
    "# -----------------------------\n",
    "stations = {\n",
    "    \"ABQ\": \"USW00023050\",\n",
    "    \"AMA\": \"USW00023020\",\n",
    "    \"ATL\": \"USW00013874\",\n",
    "    \"AUS\": \"USW00013958\",\n",
    "    \"BWI\": \"USW00093721\",\n",
    "    \"BUR\": \"USW00093195\",\n",
    "    \"CHS\": \"USW00093824\",\n",
    "    \"CLE\": \"USW00014830\",\n",
    "    \"CMH\": \"USW00094846\",\n",
    "    \"DAL\": \"USW00013960\",\n",
    "    \"DEN\": \"USW00023040\",\n",
    "    \"HOU\": \"USW00012918\",\n",
    "    \"LAS\": \"USW00024049\",\n",
    "    \"LAX\": \"USW00023174\",\n",
    "    \"MDW\": \"USW00094846\",\n",
    "    \"MCO\": \"USW00012839\",\n",
    "    \"OAK\": \"USW00023272\",\n",
    "    \"PHX\": \"USW00023183\",\n",
    "    \"SAN\": \"USW00023188\",\n",
    "    \"SFO\": \"USW00023234\",\n",
    "    \"SJC\": \"USW00023282\",\n",
    "    \"SMF\": \"USW00023266\",\n",
    "    \"STL\": \"USW00093928\",\n",
    "    \"TPA\": \"USW00012839\",\n",
    "    # Add more airports and station IDs as needed\n",
    "}\n",
    "\n",
    "BASE_URL = \"https://www.ncei.noaa.gov/pub/data/ghcn/daily/all/{}.dly\"\n",
    "OUTPUT_DIR = \"sw_airport_daily_weather_bulk\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Variables to extract\n",
    "VARS = {\"TMAX\", \"TMIN\", \"PRCP\", \"AWND\"}\n",
    "\n",
    "def parse_ghcn_dly(file_path):\n",
    "    rows = []\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            year = int(line[11:15])\n",
    "            month = int(line[15:17])\n",
    "            variable = line[17:21]\n",
    "            if variable not in VARS:\n",
    "                continue\n",
    "            for day in range(31):\n",
    "                raw = line[21 + day*8:26 + day*8]\n",
    "                try:\n",
    "                    value = int(raw)\n",
    "                except:\n",
    "                    continue\n",
    "                if value == -9999:\n",
    "                    continue\n",
    "                date = f\"{year}-{month:02d}-{day+1:02d}\"\n",
    "                if variable in [\"TMAX\", \"TMIN\"]:\n",
    "                    val = value / 10.0\n",
    "                elif variable == \"PRCP\":\n",
    "                    val = value / 10.0\n",
    "                elif variable == \"AWND\":\n",
    "                    val = value / 10.0\n",
    "                else:\n",
    "                    val = value\n",
    "                rows.append({\"DATE\": date, \"VARIABLE\": variable, \"VALUE\": val})\n",
    "    df = pd.DataFrame(rows)\n",
    "    df = df.pivot(index=\"DATE\", columns=\"VARIABLE\", values=\"VALUE\").reset_index()\n",
    "    return df\n",
    "\n",
    "# -----------------------------\n",
    "# Download, parse, save per airport\n",
    "# -----------------------------\n",
    "for airport, stn in stations.items():\n",
    "    url = BASE_URL.format(stn)\n",
    "    local_file = os.path.join(OUTPUT_DIR, f\"{stn}.dly\")\n",
    "    \n",
    "    if not os.path.exists(local_file):\n",
    "        print(f\"Downloading {airport} ({stn}) ...\")\n",
    "        r = requests.get(url, timeout=60)\n",
    "        if r.status_code != 200:\n",
    "            print(f\"‚ùå Failed to download station {stn} for airport {airport}\")\n",
    "            continue\n",
    "        with open(local_file, \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "    else:\n",
    "        print(f\"‚úÖ Station file already exists for {airport} ({stn})\")\n",
    "    \n",
    "    print(f\"Parsing data for {airport} ({stn}) ...\")\n",
    "    try:\n",
    "        df = parse_ghcn_dly(local_file)\n",
    "        df[\"AIRPORT\"] = airport  # add airport column\n",
    "        out_csv = os.path.join(OUTPUT_DIR, f\"{airport}_daily_weather.csv\")\n",
    "        df.to_csv(out_csv, index=False)\n",
    "        print(f\"Saved CSV for {airport}: {out_csv}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error parsing {airport} ({stn}): {e}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Combine all airport CSVs into one bulk CSV\n",
    "# -----------------------------\n",
    "print(\"Combining all airport CSVs into one bulk CSV...\")\n",
    "all_files = glob.glob(os.path.join(OUTPUT_DIR, \"*_daily_weather.csv\"))\n",
    "df_all = pd.concat([pd.read_csv(f) for f in all_files], ignore_index=True)\n",
    "bulk_csv_path = os.path.join(OUTPUT_DIR, \"SW_airports_all_daily_weather.csv\")\n",
    "df_all.to_csv(bulk_csv_path, index=False)\n",
    "print(f\"üéØ Bulk CSV saved: {bulk_csv_path}\")\n",
    "print(df_all.head())\n"
   ]
<<<<<<< HEAD
=======
>>>>>>> 62e6e95 (Changes with combined delay data)
=======
>>>>>>> 4766a69 (Updated with separate aggregated delay and weather data)
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
