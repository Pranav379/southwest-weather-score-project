{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "from datetime import datetime\n",
    "from meteostat import Point, Daily, Hourly\n",
    "from pathlib import Path\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BTS Data for Southwest Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOUTHWEST_STATE_AIRPORTS = {\n",
    "    # Arizona\n",
    "    \"PHX\", \"TUS\",\n",
    "    # New Mexico\n",
    "    \"ABQ\",\n",
    "    # Texas\n",
    "    \"DAL\", \"HOU\", \"AUS\", \"SAT\", \"ELP\", \"LBB\", \"MAF\", \"HRL\",\n",
    "    # Oklahoma\n",
    "    \"OKC\", \"TUL\",\n",
    "    # California\n",
    "    \"LAX\", \"SAN\", \"OAK\", \"SJC\", \"BUR\", \"SNA\", \"ONT\", \"SMF\"\n",
    "}\n",
    "\n",
    "OUTPUT_FOLDER = \"new_bts_data\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "def download_bts_data(year, month):\n",
    "    \"\"\"Download BTS data for a specific month\"\"\"\n",
    "    url = f'https://www.transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_{year}_{month}.zip'\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(\n",
    "            url, \n",
    "            headers={'User-Agent': 'Mozilla/5.0'},\n",
    "            timeout=300,\n",
    "            stream=True\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "            csv_file = next((name for name in z.namelist() if name.endswith('.csv')), None)\n",
    "            if not csv_file:\n",
    "                return (year, month, None, \"No CSV in zip\")\n",
    "            \n",
    "            with z.open(csv_file) as f:\n",
    "                df = pd.read_csv(f, encoding='utf-8', low_memory=False)\n",
    "            \n",
    "            df = df[\n",
    "                (df[\"Reporting_Airline\"] == \"WN\") &\n",
    "                (df[\"Origin\"].isin(SOUTHWEST_STATE_AIRPORTS))\n",
    "            ]\n",
    "            \n",
    "            if not df.empty:\n",
    "                month_filename = os.path.join(OUTPUT_FOLDER, f\"bts_wn_{year}_{month:02d}.csv\")\n",
    "                df.to_csv(month_filename, index=False)\n",
    "                print(f\"Saved {year}-{month:02d}: {len(df):,} rows to {month_filename}\")\n",
    "            \n",
    "            return (year, month, df, None)\n",
    "            \n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        return (year, month, None, f\"HTTP {e.response.status_code}\")\n",
    "    except Exception as e:\n",
    "        return (year, month, None, str(e)[:50])\n",
    "\n",
    "def download_years_parallel(start_year, end_year, max_workers=12):\n",
    "    \"\"\"Download multiple years of data with maximum parallelization\"\"\"\n",
    "    months_to_download = []\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        for month in range(1, 13):\n",
    "            current_date = datetime.now()\n",
    "            if year > current_date.year or (year == current_date.year and month > current_date.month):\n",
    "                continue\n",
    "            months_to_download.append((year, month))\n",
    "    \n",
    "    total_months = len(months_to_download)\n",
    "    print(f\"Downloading {total_months} months ({start_year}-{end_year})\")\n",
    "    print(f\"Using {max_workers} parallel workers\\n\")\n",
    "    \n",
    "    results = {}\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {\n",
    "            executor.submit(download_bts_data, year, month): (year, month)\n",
    "            for year, month in months_to_download\n",
    "        }\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            year, month, df, error = future.result()\n",
    "            if df is not None and not df.empty:\n",
    "                results[(year, month)] = df\n",
    "            else:\n",
    "                print(f\"âœ— {year}-{month:02d}: {error or 'No matching rows'}\")\n",
    "    \n",
    "    elapsed = (datetime.now() - start_time).total_seconds() / 60\n",
    "    print(f\"\\nFinished in {elapsed:.1f} minutes\")\n",
    "    \n",
    "    successful = [df for df in results.values() if df is not None]\n",
    "    if successful:\n",
    "        combined_df = pd.concat(successful, ignore_index=True)\n",
    "        print(f\"Combined total: {len(combined_df):,} rows\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"No data downloaded\")\n",
    "        return None\n",
    "\n",
    "def save_to_csv_fast(df, filename):\n",
    "    \"\"\"Save filtered DataFrame to CSV\"\"\"\n",
    "    print(f\"\\nSaving combined dataset to {filename}...\")\n",
    "    start = datetime.now()\n",
    "    df.to_csv(filename, index=False)\n",
    "    elapsed = (datetime.now() - start).total_seconds()\n",
    "    size_mb = os.path.getsize(filename) / (1024 * 1024)\n",
    "    print(f\"Saved in {elapsed:.1f}s ({size_mb:.1f} MB)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    START_YEAR = 2015 # 10 years worth of data\n",
    "    END_YEAR = 2025\n",
    "    MAX_WORKERS = 10\n",
    "\n",
    "    combined_data = download_years_parallel(START_YEAR, END_YEAR, max_workers=MAX_WORKERS)\n",
    "    \n",
    "    if combined_data is not None:\n",
    "        output_file = os.path.join(OUTPUT_FOLDER, f\"bts_wn_southwest_combined_{START_YEAR}_{END_YEAR}.csv\")\n",
    "        save_to_csv_fast(combined_data, output_file)\n",
    "        print(f\"\\nCombined file saved as {output_file} ({len(combined_data):,} rows)\")\n",
    "        print(\"\\nHead of combined dataset:\")\n",
    "        print(combined_data.head())\n",
    "    else:\n",
    "        print(\"No data available after filtering.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meteostat Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SWA_AIRPORTS = {\n",
    "    # Arizona\n",
    "    \"PHX\": (33.4342, -112.0116),\n",
    "    \"TUS\": (32.1161, -110.9410),\n",
    "    # New Mexico\n",
    "    \"ABQ\": (35.0494, -106.6172),\n",
    "    # Texas\n",
    "    \"DAL\": (32.8471, -96.8517),\n",
    "    \"HOU\": (29.6454, -95.2789),\n",
    "    \"AUS\": (30.1975, -97.6664),\n",
    "    \"SAT\": (29.5337, -98.4698),\n",
    "    \"ELP\": (31.8070, -106.3779),\n",
    "    \"LBB\": (33.6609, -101.8214),\n",
    "    \"MAF\": (31.9369, -102.2016),\n",
    "    \"HRL\": (26.2285, -97.6544),\n",
    "    # Oklahoma\n",
    "    \"OKC\": (35.3931, -97.6008),\n",
    "    \"TUL\": (36.1986, -95.8880),\n",
    "    # California\n",
    "    \"LAX\": (33.9425, -118.4081),\n",
    "    \"SAN\": (32.7338, -117.1933),\n",
    "    \"OAK\": (37.7126, -122.2197),\n",
    "    \"SJC\": (37.3639, -121.9289),\n",
    "    \"BUR\": (34.2007, -118.3587),\n",
    "    \"SNA\": (33.6757, -117.8682),\n",
    "    \"ONT\": (34.0559, -117.6009),\n",
    "    \"SMF\": (38.6950, -121.5908)\n",
    "}\n",
    "\n",
    "OUTPUT_DIR = Path(\"meteostat_hourly\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "START_YEAR = 2015\n",
    "END_YEAR = 2025\n",
    "MAX_WORKERS = 4\n",
    "\n",
    "def fetch_airport_hourly(airport, coords):\n",
    "    lat, lon = coords\n",
    "    point = Point(lat, lon)\n",
    "    all_data = []\n",
    "    \n",
    "    for year in range(START_YEAR, END_YEAR + 1):\n",
    "        start = datetime(year, 1, 1)\n",
    "        end = datetime(year, 12, 31)\n",
    "        try:\n",
    "            data = Hourly(point, start, end).fetch()\n",
    "            if not data.empty:\n",
    "                data = data.reset_index().rename(columns = {\"time\": \"date\"})\n",
    "                data['airport'] = airport\n",
    "                all_data.append(data)\n",
    "                print(f\"Fetched {airport} {year} ({len(data)} rows)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {airport} {year}: {e}\")\n",
    "        time.sleep(0.05)\n",
    "    \n",
    "    if all_data:\n",
    "        df = pd.concat(all_data)\n",
    "        out_file = OUTPUT_DIR / f\"{airport}.csv\"\n",
    "        df.to_csv(out_file, index=True)\n",
    "        print(f\"Saved {airport} to {out_file}\")\n",
    "\n",
    "def main():\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        futures = [executor.submit(fetch_airport_hourly, airport, coords)\n",
    "                   for airport, coords in SWA_AIRPORTS.items()]\n",
    "        for fut in as_completed(futures):\n",
    "            fut.result()\n",
    "\n",
    "    all_files = list(OUTPUT_DIR.glob(\"*.csv\"))\n",
    "    combined = pd.concat((pd.read_csv(f, index_col=0) for f in all_files), ignore_index=True)\n",
    "    combined.to_csv(OUTPUT_DIR / \"weather_data.csv\", index=False)\n",
    "    print(f\"\\nAll airports combined: {len(combined)} rows\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining BTS and Meteostat Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(\"combined_data.zip\") as z:\n",
    "    csv_name = next(name for name in z.namelist() if name.endswith(\".csv\"))\n",
    "    flights = pl.scan_csv(io.BytesIO(z.read(csv_name)), low_memory=True)\n",
    "\n",
    "weather = pl.scan_csv(\"Meteostat_daily_2000-2025.csv\", low_memory=True)\n",
    "\n",
    "joined = flights.join(\n",
    "    weather,\n",
    "    how=\"inner\",\n",
    "    left_on=[\"Origin\", \"FlightDate\"],\n",
    "    right_on=[\"airport\", \"date\"]\n",
    ")\n",
    "\n",
    "joined.sink_csv(\"joined_data.csv\")\n",
    "\n",
    "print(\"Streaming join completed successfully. File saved as 'joined_data.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
