{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "DsMLKjt6kVS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
        "import numpy as np\n",
        "import polars as pl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, root_mean_squared_error, r2_score, accuracy_score, recall_score, roc_auc_score, ConfusionMatrixDisplay, RocCurveDisplay, precision_score\n",
        "from lightgbm import LGBMRegressor as lgbm, early_stopping\n",
        "import optuna\n",
        "from boruta import BorutaPy\n",
        "\n",
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "import os\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "from datetime import datetime\n",
        "from meteostat import Point, Daily\n",
        "from pathlib import Path\n",
        "import time"
      ],
      "metadata": {
        "id": "7n2BrKv9kWff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Data Collection and Preprocessing"
      ],
      "metadata": {
        "id": "P3StZHpukYdn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Collection"
      ],
      "metadata": {
        "id": "XJIXoh8bkbDp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Data"
      ],
      "metadata": {
        "id": "0vjccpwgkcpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"FINAL_DATASET.csv\")"
      ],
      "metadata": {
        "id": "hgAzR9nqkZX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Display Data"
      ],
      "metadata": {
        "id": "GIOjS0ypkf2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())\n",
        "print(\"Data Range:\")\n",
        "print(df['FlightDate'].iloc[[0, -1]])"
      ],
      "metadata": {
        "id": "_w2xfrK4kgxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_grouped = df.groupby(['Origin', 'Dest', 'Month', 'DayOfWeek']).agg({\n",
        "    'DepDelayMinutes': 'mean',\n",
        "    'wspd': 'mean', 'tavg': 'mean', 'tmin': 'mean', 'tmax': 'mean', 'prcp': 'mean',\n",
        "    'snow': 'mean', 'wdir': 'mean', 'wspd': 'mean', 'wpgt': 'mean', 'pres': 'mean',\n",
        "}).reset_index()\n"
      ],
      "metadata": {
        "id": "E2xlQ7QEkiFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "zEHGNAP6kxHc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converting to Imperial"
      ],
      "metadata": {
        "id": "AykIlX1gkzqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"FINAL_DATASET.csv\")\n",
        "\n",
        "# Temperature: Convert ¬∞C to ¬∞F\n",
        "for col in [\"tavg\", \"tmin\", \"tmax\"]:\n",
        "    if col in df.columns:\n",
        "        df[col] = (df[col] * 9/5 + 32).round().astype(\"Int64\")\n",
        "\n",
        "# Precipitation & Snow: Convert mm to inches\n",
        "for col in [\"prcp\", \"snow\"]:\n",
        "    if col in df.columns:\n",
        "        df[col] = df[col] / 25.4\n",
        "\n",
        "# Wind Speed: Convert km/h to mph\n",
        "if \"wspd\" in df.columns:\n",
        "    df[\"wspd\"] = df[\"wspd\"] / 1.60934\n",
        "\n",
        "# Pressure: Convert hPa to inHg\n",
        "if \"pres\" in df.columns:\n",
        "    df[\"pres\"] = df[\"pres\"] * 0.02953\n",
        "\n",
        "print(df.head(1))"
      ],
      "metadata": {
        "id": "PMfOdhCVki5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converting Time to Cyclical"
      ],
      "metadata": {
        "id": "wwmC0gY9k1fu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforming departure time to cyclical\n",
        "print(df['CRSDepTime'])\n",
        "\n",
        "df['DepHour'] = df['CRSDepTime'] // 100\n",
        "df['DepMinute'] = df['CRSDepTime'] % 100\n",
        "df['DepTotalMinutes'] = df['DepHour'] * 60 + df['DepMinute']\n",
        "\n",
        "df['DepTime_sin'] = np.sin(2*np.pi*df['DepTotalMinutes'] / 1440)\n",
        "df['DepTime_cos'] = np.cos(2*np.pi*df['DepTotalMinutes'] / 1440)\n",
        "\n",
        "print(df['DepTime_sin'].head())\n",
        "print(df['DepTime_cos'].head())\n",
        "\n",
        "# Same for arrival\n",
        "print(df['CRSArrTime'])\n",
        "\n",
        "df['ArrHour'] = df['CRSArrTime'] // 100\n",
        "df['ArrMinute'] = df['CRSArrTime'] % 100\n",
        "df['ArrTotalMinutes'] = df['ArrHour'] * 60 + df['ArrMinute']\n",
        "\n",
        "df['ArrTime_sin'] = np.sin(2*np.pi*df['ArrTotalMinutes'] / 1440)\n",
        "df['ArrTime_cos'] = np.cos(2*np.pi*df['ArrTotalMinutes'] / 1440)\n",
        "\n",
        "print(df['ArrTime_sin'].head())\n",
        "print(df['ArrTime_cos'].head())"
      ],
      "metadata": {
        "id": "Jl1zK4I3lBUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Treating Missing Values"
      ],
      "metadata": {
        "id": "PBIGaNE2lCZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Percentage NA by column\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "percent_na = df.isna().mean() * 100\n",
        "print(percent_na.sort_values(ascending = False).head(75))\n",
        "\n",
        "# Removing >95% missing cols and IATA Code column due to redundancy\n",
        "cols_to_drop = percent_na[percent_na > 95].index\n",
        "df = df.drop(columns = cols_to_drop)\n",
        "df = df.drop(columns = [\"IATA_CODE_Reporting_Airline\"])\n",
        "\n",
        "print(f\"\\nDropped {len(cols_to_drop)} columns\")\n",
        "print(\"Remaining columns: \", len(df.columns))\n",
        "\n",
        "# Printing percentage again\n",
        "percent_na = df.isna().mean() * 100\n",
        "print(percent_na.sort_values(ascending = False))"
      ],
      "metadata": {
        "id": "N4BwwksQlDa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Weather Graphs"
      ],
      "metadata": {
        "id": "65Xep8JLlE3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['wspd'] = np.clip(df['wspd'], 0, 50) # Clip for realism\n",
        "df['prcp'] = np.clip(df['prcp'], 0, 50) # Clip for realism\n",
        "# Convert Snow_Presence to a descriptive category for better plotting\n",
        "df['snow'] = df['snow'].map({0: 'No Snow', 1: 'Snow'})\n",
        "# ----------------------------------------------------\n",
        "\n",
        "# Set a style for better visualization\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "## üìä Plotting Continuous Variables (Temperature, Wind Speed, Precipitation)\n",
        "\n",
        "# Create a figure with subplots\n",
        "fig, axes = plt.subplots(3, 1, figsize=(10, 15))\n",
        "plt.suptitle('Distribution of Key Weather Factors', fontsize=16, y=1.02)\n",
        "\n",
        "# --- 1. Temperature Distribution (Histogram with KDE) ---\n",
        "sns.histplot(df['tavg'], bins=30, kde=True, color='skyblue', ax=axes[0])\n",
        "axes[0].set_title('Temperature Distribution', fontsize=14)\n",
        "axes[0].set_xlabel('Temperature (¬∞F)')\n",
        "axes[0].set_ylabel('Count / Density')\n",
        "#\n",
        "\n",
        "# --- 2. Wind Speed Distribution (Histogram with KDE) ---\n",
        "sns.histplot(df['wspd'], bins=30, kde=True, color='orange', ax=axes[1])\n",
        "axes[1].set_title('Wind Speed Distribution', fontsize=14)\n",
        "axes[1].set_xlabel('Wind Speed (mph)')\n",
        "axes[1].set_ylabel('Count / Density')\n",
        "# Add a vertical line for the mean to highlight the average condition\n",
        "axes[1].axvline(df['wspd'].mean(), color='r', linestyle='--', label=f\"Mean: {df['wspd'].mean():.2f}\")\n",
        "axes[1].legend()\n",
        "\n",
        "# --- 3. Precipitation Distribution (KDE Plot for Skewed Data) ---\n",
        "# A KDE plot is often better for highly skewed data like precipitation,\n",
        "# where many values are zero or near-zero.\n",
        "sns.kdeplot(df['prcp'], fill=True, color='green', ax=axes[2],\n",
        "            bw_adjust=0.5) # bw_adjust controls smoothness\n",
        "axes[2].set_title('Precipitation Distribution', fontsize=14)\n",
        "axes[2].set_xlabel('Precipitation (in)')\n",
        "axes[2].set_ylabel('Density')\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aHNfDZ1glFv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weather_df = df[['tavg', 'tmin', 'tmax', 'prcp', 'snow', 'wspd', 'pres']]\n",
        "plt.figure(figsize = (15, 10))\n",
        "sns.heatmap(weather_df.corr(), annot = True, cmap = \"coolwarm\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2pJ5Sl0elHXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weather_cols = ['tavg', 'tmin', 'tmax', 'prcp', 'snow', 'wspd', 'pres']\n",
        "n_cols = len(weather_cols)\n",
        "fig, axes = plt.subplots(\n",
        "    nrows=(n_cols + 1) // 2,\n",
        "    ncols=2,\n",
        "    figsize=(15, 5 * ((n_cols + 1) // 2))\n",
        ")\n",
        "\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, col in enumerate(weather_cols):\n",
        "    # 1. Create Bins (Categories) for the continuous variable\n",
        "\n",
        "    # Use qcut to create 4 bins with roughly equal number of flights in each (quartiles)\n",
        "    if col in ['tavg', 'tmin', 'tmax', 'wspd', 'pres']:\n",
        "        df[f'{col}_Category'] = pd.qcut(df[col], q=4, labels=[f'Q1 ({col})', f'Q2 ({col})', f'Q3 ({col})', f'Q4 ({col})'])\n",
        "        x_label = f'Quartile of {col} (e.g., Temperature, Wind Speed)'\n",
        "    # Special handling for precipitation (prcp) and snow, as they often have many zeros\n",
        "    elif col in ['prcp', 'snow']:\n",
        "        # Create simple categories: Zero, Low, Medium, High\n",
        "        if col == 'prcp':\n",
        "            bins = [-np.inf, 0.01, 0.2, 1.0, np.inf]\n",
        "            labels = ['None', 'Light', 'Moderate', 'Heavy']\n",
        "        else: # snow\n",
        "            bins = [-np.inf, 0.1, 1.0, 5.0, np.inf]\n",
        "            labels = ['None/Trace', 'Light', 'Medium', 'Heavy']\n",
        "\n",
        "        df[f'{col}_Category'] = pd.cut(df[col], bins=bins, labels=labels, right=True)\n",
        "        x_label = f'{col} Category'\n",
        "\n",
        "    # 2. Calculate the delay rate (mean of DepDel15) for each new category\n",
        "    delay_summary = df.groupby(f'{col}_Category')['DepDel15'].mean().reset_index()\n",
        "    delay_summary['Delay_Rate_Pct'] = delay_summary['DepDel15'] * 100\n",
        "\n",
        "    # 3. Plot the result\n",
        "    sns.barplot(\n",
        "        x=f'{col}_Category',\n",
        "        y='Delay_Rate_Pct',\n",
        "        data=delay_summary,\n",
        "        ax=axes[i],\n",
        "        palette='coolwarm',\n",
        "        edgecolor='black'\n",
        "    )\n",
        "\n",
        "    # Add values on top of the bars\n",
        "    for index, row in delay_summary.iterrows():\n",
        "        axes[i].text(row.name, row['Delay_Rate_Pct'] + 0.5, f\"{row['Delay_Rate_Pct']:.1f}%\",\n",
        "                        color='black', ha=\"center\", fontsize=8)\n",
        "\n",
        "    axes[i].set_title(f'Delay Rate vs. {col.upper()}', fontsize=12)\n",
        "    axes[i].set_xlabel(x_label, fontsize=10)\n",
        "    axes[i].set_ylabel('Delay Rate (%)', fontsize=10)\n",
        "    axes[i].set_ylim(0, delay_summary['Delay_Rate_Pct'].max() + 5)\n",
        "    axes[i].tick_params(axis='x', rotation=15)\n",
        "    axes[i].grid(axis='y', linestyle=':', alpha=0.6)\n",
        "\n",
        "# Hide any unused subplots\n",
        "for i in range(n_cols, len(axes)):\n",
        "    fig.delaxes(axes[i])\n",
        "\n",
        "fig.suptitle('Impact of Individual Weather Factors', fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.subplots_adjust(bottom=1, right=0.8, top=1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cLb1UHzLlIKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "delay_col='DepDel15'\n",
        "weather_cols=['tavg', 'prcp']\n",
        "# Calculate the number of subplots needed\n",
        "n_cols = len(weather_cols)\n",
        "# Determine the number of rows based on the number of factors (2 plots per row)\n",
        "nrows = (n_cols + 1) // 2\n",
        "fig, axes = plt.subplots(\n",
        "    nrows=nrows,\n",
        "    ncols=2,\n",
        "    figsize=(15, 5 * nrows)\n",
        ")\n",
        "\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, col in enumerate(weather_cols):\n",
        "\n",
        "    # 1. Create Bins (Categories) for the continuous variable based on the factor\n",
        "\n",
        "    if col in ['tavg', 'tmin', 'tmax']:\n",
        "        # Custom bins for Temperature to capture cold and hot extremes (bimodal impact)\n",
        "        bins = [-np.inf, 30, 45, 75, 90, np.inf]\n",
        "        labels = ['Very Cold (<30¬∞F)', 'Cold (30-45¬∞F)', 'Moderate (45-75¬∞F)', 'Hot (75-90¬∞F)', 'Very Hot (>90¬∞F)']\n",
        "        # x_label = f'{col.upper()} Range (¬∞F)'\n",
        "        x_label = ''\n",
        "        df[f'{col}_Category'] = pd.cut(df[col], bins=bins, labels=labels, right=True)\n",
        "\n",
        "    elif col == 'wspd':\n",
        "        # Custom bins for Wind Speed to capture critical thresholds (nonlinear impact)\n",
        "        bins = [-np.inf, 10, 20, 30, np.inf]\n",
        "        labels = ['Low (<10 mph)', 'Medium (10-20 mph)', 'High (20-30 mph)', 'Severe (>30 mph)']\n",
        "        # x_label = 'Wind Speed Range (MPH)'\n",
        "        df[f'{col}_Category'] = pd.cut(df[col], bins=bins, labels=labels, right=True)\n",
        "\n",
        "\n",
        "    # 2. Calculate the delay rate (mean of DepDel15) for each new category\n",
        "    category_col = f'{col}_Category'\n",
        "    delay_summary = df.groupby(category_col)[delay_col].mean().reset_index()\n",
        "    delay_summary['Delay_Rate_Pct'] = delay_summary[delay_col] * 100\n",
        "\n",
        "    # 3. Plot the result\n",
        "    sns.barplot(\n",
        "        x=category_col,\n",
        "        y='Delay_Rate_Pct',\n",
        "        data=delay_summary,\n",
        "        ax=axes[i],\n",
        "        palette='coolwarm',\n",
        "        edgecolor='black'\n",
        "    )\n",
        "\n",
        "    # Add values on top of the bars\n",
        "    for index, row in delay_summary.iterrows():\n",
        "        axes[i].text(row.name, row['Delay_Rate_Pct'] + 0.5, f\"{row['Delay_Rate_Pct']:.1f}%\",\n",
        "                        color='black', ha=\"center\", fontsize=8)\n",
        "\n",
        "    axes[i].set_title(f'Delay Rate vs. {col.upper()}', fontsize=12)\n",
        "    axes[i].set_xlabel(x_label, fontsize=10)\n",
        "    axes[i].set_ylabel('Delay Rate (%)', fontsize=10)\n",
        "    axes[i].set_ylim(0, delay_summary['Delay_Rate_Pct'].max() + 5)\n",
        "    axes[i].tick_params(axis='x', rotation=15)\n",
        "    axes[i].grid(axis='y', linestyle=':', alpha=0.6)\n",
        "\n",
        "# Hide any unused subplots\n",
        "for i in range(n_cols, len(axes)):\n",
        "    fig.delaxes(axes[i])\n",
        "\n",
        "fig.suptitle('Impact of Individual Weather Factors', fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ft4fEODwlKPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_delayed = df\n",
        "df['TotalDisruptionMinutes'] = np.where(\n",
        "    (df['Cancelled'] == 1.0) | (df['Diverted'] == 1.0),\n",
        "    1000,\n",
        "    df['DepDelayMinutes']\n",
        ")"
      ],
      "metadata": {
        "id": "ctdV_nN5lLSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.1 IsSevereDisruption (Total disruption flag, used for reference)\n",
        "df['IsSevereDisruption'] = np.where(\n",
        "    (df['Cancelled'] == 1.0) | (df['Diverted'] == 1.0) | (df['DepDel15'] == 1.0),\n",
        "    1.0,\n",
        "    0.0\n",
        ")\n",
        "\n",
        "# 1.2 IsCancelledOrDiverted (The worst outcomes)\n",
        "df['IsCancelledOrDiverted'] = np.where(\n",
        "    (df['Cancelled'] == 1.0) | (df['Diverted'] == 1.0),\n",
        "    1.0,\n",
        "    0.0\n",
        ")\n",
        "\n",
        "# 1.3 IsDelayedOnly (15+ minute delays that were NOT Cancelled/Diverted)\n",
        "df['IsDelayedOnly'] = np.where(\n",
        "    (df['DepDel15'] == 1.0) & (df['IsCancelledOrDiverted'] == 0.0),\n",
        "    1.0,\n",
        "    0.0\n",
        ")"
      ],
      "metadata": {
        "id": "dWv6o-ULlMPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weather_cols = ['wspd']\n",
        "n_cols = len(weather_cols)\n",
        "\n",
        "# Set up the plot figure for a single bar chart\n",
        "fig, axes = plt.subplots(\n",
        "    nrows=1,\n",
        "    ncols=1,\n",
        "    figsize=(10, 7) # Increased size for clarity\n",
        ")\n",
        "axes = [axes]\n",
        "\n",
        "for i, col in enumerate(weather_cols):\n",
        "\n",
        "    # 1. Create Bins (Categories) for Wind Speed\n",
        "    if col == 'wspd':\n",
        "        bins = [-np.inf, 10, 20, 30, np.inf]\n",
        "        labels = ['Low (<10 mph)', 'Medium (10-20 mph)', 'High (20-30 mph)', 'Severe (>30 mph)']\n",
        "        x_label = 'Wind Speed Range (MPH)'\n",
        "        df[f'{col}_Category'] = pd.cut(df[col], bins=bins, labels=labels, right=True)\n",
        "\n",
        "    # 2. Calculate the rate for each mutually exclusive category\n",
        "    category_col = f'{col}_Category'\n",
        "    delay_summary = df.groupby(category_col).agg(\n",
        "        Cancelled_Diverted_Rate=('IsCancelledOrDiverted', 'mean'),\n",
        "        Delayed_Only_Rate=('IsDelayedOnly', 'mean'),\n",
        "        Total_Disruption_Rate=('IsSevereDisruption', 'mean') # Calculate total for annotations\n",
        "    ).reset_index()\n",
        "\n",
        "    # 3. Reshape the data for a stacked bar plot (melt)\n",
        "    summary_melted = delay_summary.melt(\n",
        "        id_vars=[category_col, 'Total_Disruption_Rate'],\n",
        "        value_vars=['Cancelled_Diverted_Rate', 'Delayed_Only_Rate'],\n",
        "        var_name='Disruption_Type',\n",
        "        value_name='Rate'\n",
        "    )\n",
        "    summary_melted['Rate_Pct'] = summary_melted['Rate'] * 100\n",
        "\n",
        "    # Rename types for legend clarity\n",
        "    summary_melted['Disruption_Type'] = summary_melted['Disruption_Type'].replace({\n",
        "        'Cancelled_Diverted_Rate': 'Cancellation / Diversion',\n",
        "        'Delayed_Only_Rate': '15+ Minute Delay Only'\n",
        "    })\n",
        "\n",
        "    # 4. Plot the stacked result\n",
        "    sns.barplot(\n",
        "        x=category_col,\n",
        "        y='Rate_Pct',\n",
        "        hue='Disruption_Type', # Use hue to create the segments\n",
        "        data=summary_melted,\n",
        "        ax=axes[i],\n",
        "        palette='Set2', # A clearer palette for two groups\n",
        "        edgecolor='black',\n",
        "        dodge=False # CRITICAL: Set to False to make the bars stack\n",
        "    )\n",
        "\n",
        "    axes[i].set_title(f'Composition of Flight Disruption by Wind Speed', fontsize=14)\n",
        "    axes[i].set_xlabel(x_label, fontsize=12)\n",
        "    axes[i].set_ylabel('Disruption Rate (%)', fontsize=12)\n",
        "    axes[i].set_ylim(0, delay_summary['Total_Disruption_Rate'].max() * 100 + 5)\n",
        "    axes[i].tick_params(axis='x', rotation=15)\n",
        "    axes[i].legend(title='Disruption Type')\n",
        "    axes[i].grid(axis='y', linestyle=':', alpha=0.6)\n",
        "\n",
        "fig.suptitle('Breakdown of Severe Disruption Risk (Delay vs. Cancellation/Diversion)', fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eCKL4bG9lNJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "delay_magnitude_col = 'TotalDisruptionMinutes'\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# --- Option A: Scatter Plot (Delay Magnitude vs. Wind Speed) ---\n",
        "sns.scatterplot(\n",
        "    x='wspd',\n",
        "    y=delay_magnitude_col,\n",
        "    data=df_delayed,\n",
        "    alpha=0.4,\n",
        "    ax=ax1,\n",
        "    color='darkblue'\n",
        ")\n",
        "# Add a linear regression line for context\n",
        "sns.regplot(\n",
        "    x='wspd',\n",
        "    y=delay_magnitude_col,\n",
        "    data=df_delayed,\n",
        "    scatter=False,\n",
        "    color='red',\n",
        "    line_kws={'linestyle': '--', 'label': 'Linear Trend'},\n",
        "    ax=ax1\n",
        ")\n",
        "\n",
        "ax1.set_title('Scatter Plot: Wind Speed (wspd) vs. Delay Magnitude', fontsize=14)\n",
        "ax1.set_xlabel('Wind Speed (Knots/MPH)', fontsize=12)\n",
        "ax1.set_ylabel('Delay Magnitude (Minutes)', fontsize=12)\n",
        "ax1.grid(axis='both', linestyle=':', alpha=0.6)\n",
        "\n",
        "# --- Option B: Box Plot (Delay Magnitude by Wind Speed Bin) ---\n",
        "wind_bins = [-np.inf, 10, 20, 30, np.inf]\n",
        "wind_labels = ['<10 MPH', '10-20 MPH', '20-30 MPH', '>30 MPH']\n",
        "\n",
        "# Use the same logic as in Slide 4 to create ordered bins\n",
        "df_delayed['Wind_Bin_Label'] = pd.cut(\n",
        "    df_delayed['wspd'],\n",
        "    bins=wind_bins,\n",
        "    labels=wind_labels,\n",
        "    right=True,\n",
        "    duplicates='drop'\n",
        ")\n",
        "\n",
        "sns.boxplot(\n",
        "    x='Wind_Bin_Label', # Use the fixed bins\n",
        "    y=delay_magnitude_col,\n",
        "    data=df_delayed,\n",
        "    ax=ax2,\n",
        "    palette='plasma',\n",
        "    notch=True\n",
        ")\n",
        "\n",
        "ax2.set_title('Box Plot: Delay Magnitude by Wind Speed Range', fontsize=14)\n",
        "ax2.set_xlabel('Wind Speed Range', fontsize=12)\n",
        "ax2.set_ylabel('Delay Magnitude (Minutes)', fontsize=12)\n",
        "ax2.set_xticklabels(ax2.get_xticklabels(), rotation=15, ha='right')\n",
        "ax2.grid(axis='y', linestyle=':', alpha=0.6)\n",
        "\n",
        "fig.suptitle('Correlation Between Continuous Weather Factors and Delay Magnitude', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.95]) # Adjust layout for suptitle\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4qSrDDuzlOL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Heatmap After Removing Highly Correlated and Unwanted Variables"
      ],
      "metadata": {
        "id": "CjO7Vx45lPMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_df = df.select_dtypes(include = ['number']).drop(columns = ['DepTime', 'Year', 'Quarter', 'DayofMonth', 'DivAirportLandings', 'DestAirportSeqID', 'Flights', 'OriginAirportID', 'OriginStateFips', 'DestWac', 'OriginWac', 'CRSDepTime', 'CRSArrTime', 'Cancelled', 'DestStateFips', 'DayOfWeek', 'Month', 'DOT_ID_Reporting_Airline', 'Flight_Number_Reporting_Airline', 'OriginCityMarketID', 'Diverted', 'OriginAirportSeqID', 'DestAirportID', 'DestCityMarketID'], errors = 'ignore')\n",
        "plt.figure(figsize = (30, 12))\n",
        "sns.heatmap(numeric_df.corr(method='spearman'), annot = True, cmap = \"coolwarm\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Dpmt2m-VlPhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(df['DepDelayMinutes'], bins=100)\n",
        "plt.xlim(0, 300)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RoiXlgA0lTNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The extreme right skew, combined with the low correlation coefficients, indicates we need to perform some feature engineering to help the model find relations between weather data and delay lengths."
      ],
      "metadata": {
        "id": "5OOAK2jelUkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "percent_na = df.isna().mean() * 100\n",
        "print(percent_na.sort_values(ascending = False))"
      ],
      "metadata": {
        "id": "cXlV1p24lVNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Delays Over Time"
      ],
      "metadata": {
        "id": "W8GSaQYmlWWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure date column is datetime\n",
        "df['FlightDate'] = pd.to_datetime(df['FlightDate'])\n",
        "\n",
        "# ------------------------\n",
        "# 2Ô∏è‚É£ Aggregate: average delay per day per airport\n",
        "# ------------------------\n",
        "avg_delay = (\n",
        "    df.groupby(['FlightDate', 'Origin'])['DepDelayMinutes']\n",
        "      .mean()\n",
        "      .reset_index()\n",
        ")\n",
        "\n",
        "# ------------------------\n",
        "# 3Ô∏è‚É£ Get unique airports\n",
        "# ------------------------\n",
        "airports = avg_delay['Origin'].unique()\n",
        "num_airports = len(airports)\n",
        "\n",
        "# ------------------------\n",
        "# 4Ô∏è‚É£ Create subplots (one per airport)\n",
        "# ------------------------\n",
        "# Calculate number of rows and columns for subplots\n",
        "cols = 2  # Adjust if you want more columns\n",
        "rows = (num_airports + cols - 1) // cols\n",
        "\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(14, 4*rows), sharex=True, sharey=True)\n",
        "axes = axes.flatten()  # Flatten to easily index\n",
        "\n",
        "# Plot each airport\n",
        "for i, airport in enumerate(airports):\n",
        "    airport_data = avg_delay[avg_delay['Origin'] == airport]\n",
        "    sns.lineplot(\n",
        "        data=airport_data,\n",
        "        x='FlightDate',\n",
        "        y='DepDelayMinutes',\n",
        "        ax=axes[i],\n",
        "        color='blue'\n",
        "    )\n",
        "    axes[i].set_title(f\"Airport: {airport}\")\n",
        "    axes[i].set_xlabel(\"Date\")\n",
        "    axes[i].set_ylabel(\"Avg Departure Delay (mins)\")\n",
        "    axes[i].grid(True)\n",
        "\n",
        "# Remove unused subplots if any\n",
        "for j in range(i+1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle(\"Average Departure Delay Over Time by Airport\", y=1.02, fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VqnK1IDwlXPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transform Target"
      ],
      "metadata": {
        "id": "zz49tPqYlZlM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Log Transformation"
      ],
      "metadata": {
        "id": "v_JRGpxClboc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['DepDelayMinutes_log'] = np.log1p(df['DepDelayMinutes'])"
      ],
      "metadata": {
        "id": "3F8ZM3KGlagW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Box-Cox Transformation"
      ],
      "metadata": {
        "id": "Pmh1-egDlf08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['DepDelay_shifted'] = df['DepDelayMinutes'] + 1e-3  # avoid zeros\n",
        "pt_boxcox = PowerTransformer(method='box-cox', standardize=False)\n",
        "df['DepDelayMinutes_boxcox'] = pt_boxcox.fit_transform(df['DepDelay_shifted'].values.reshape(-1,1))"
      ],
      "metadata": {
        "id": "aJ6gUHh-lhar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Yeo-Johnson Transformation"
      ],
      "metadata": {
        "id": "0N_WicjRlinA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pt_yeojohnson = PowerTransformer(method='yeo-johnson', standardize=False)\n",
        "df['DepDelayMinutes_yeojohnson'] = pt_yeojohnson.fit_transform(df['DepDelayMinutes'].values.reshape(-1,1))"
      ],
      "metadata": {
        "id": "xovMTwrgllHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Engineering"
      ],
      "metadata": {
        "id": "vlZiGF-wlp13"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extra Features"
      ],
      "metadata": {
        "id": "Wul10msils9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['FlightDate'] = pd.to_datetime(df['FlightDate'])\n",
        "\n",
        "# DayOfYear: Day of the year (1-365/366)\n",
        "df['DayOfYear'] = df['FlightDate'].dt.dayofyear\n",
        "\n",
        "# IsWeekend: 1 if the flight date is Saturday or Sunday, else 0\n",
        "df['IsWeekend'] = df['DayOfWeek'].isin([6,7]).astype(int)\n",
        "\n",
        "# IsHoliday: 1 if the flight date is a US federal holiday, else 0\n",
        "cal = USFederalHolidayCalendar()\n",
        "holidays = cal.holidays(start=df['FlightDate'].min(), end=df['FlightDate'].max())\n",
        "df['IsHoliday'] = df['FlightDate'].isin(holidays)\n",
        "\n",
        "# IsHolidayWindow: within 1 day before or after a holiday\n",
        "holiday_window = pd.concat([\n",
        "    pd.Series(holidays - pd.Timedelta(days=1)),\n",
        "    pd.Series(holidays),\n",
        "    pd.Series(holidays + pd.Timedelta(days=1))\n",
        "])\n",
        "df['IsHolidayWindow'] = df['FlightDate'].isin(holiday_window)\n",
        "\n",
        "# NumDepartures: number of departures from the origin airport on that day\n",
        "daily_departures = (\n",
        "    df.groupby(['OriginAirportID', 'FlightDate'])\n",
        "      .size()\n",
        "      .rename('NumDepartures')\n",
        "      .reset_index()\n",
        ")\n",
        "\n",
        "df = df.merge(daily_departures, on=['OriginAirportID', 'FlightDate'], how='left')\n",
        "\n",
        "# Ensure Date column exists and is datetime\n",
        "df['Date'] = pd.to_datetime(df['FlightDate'])\n",
        "\n",
        "# Sort so rolling ops are time-consistent\n",
        "df = df.sort_values(['OriginAirportID', 'Date'])\n",
        "\n",
        "# RouteDelayMean_7d: Route-level rolling delay mean\n",
        "df['Route'] = df['OriginAirportID'].astype(str) + '_' + df['DestAirportID'].astype(str)\n",
        "df['RouteDelayMean_7d'] = (\n",
        "    df.groupby('Route')['DepDelayMinutes']\n",
        "      .transform(lambda x: x.shift().rolling(7, min_periods=1).mean())\n",
        ")\n",
        "\n",
        "# OriginDelayMean_7d: Airport-level (origin) rolling delay mean\n",
        "df['OriginDelayMean_7d'] = (\n",
        "    df.groupby('OriginAirportID')['DepDelayMinutes']\n",
        "      .transform(lambda x: x.shift().rolling(7, min_periods=1).mean())\n",
        ")\n",
        "\n",
        "# DestArrivals_7d: Destination congestion indicator\n",
        "df['DestArrivals_7d'] = (\n",
        "    df.groupby('DestAirportID')['Flight_Number_Reporting_Airline']\n",
        "      .transform(lambda x: x.shift().rolling(7, min_periods=1).count())\n",
        ")\n",
        "\n",
        "# Replace any remaining NaN (from shift/rolling) with reasonable defaults\n",
        "df.fillna({\n",
        "    'RouteDelayMean_7d': 0,\n",
        "    'OriginDelayMean_7d': 0,\n",
        "    'DestArrivals_7d': 0\n",
        "}, inplace=True)\n",
        "\n",
        "# Departures_Today: Day-level departure volume (can help with congestion)\n",
        "df['Departures_Today'] = df.groupby(['OriginAirportID', 'Date'])['Flight_Number_Reporting_Airline'].transform('count')\n",
        "\n",
        "# Interaction features (useful for nonlinear models like LightGBM)\n",
        "df['Dist_x_Wspd'] = df['Distance'] * df['wspd']\n",
        "df['TempRange'] = df['tmax'] - df['tmin']\n",
        "df['MonthxWeekday'] = df['Month'] * df['DayOfWeek']\n",
        "\n",
        "# CongestionRatio: Ratio of today's departures to average departures for that airport\n",
        "df['AvgDepartures_Past30d'] = (\n",
        "    df.groupby('OriginAirportID')['NumDepartures']\n",
        "      .transform(lambda x: x.shift().rolling(30, min_periods=1).mean())\n",
        ")\n",
        "df['CongestionRatio'] = df['NumDepartures'] / df['AvgDepartures_Past30d']\n",
        "\n",
        "# OriginDelayTrend_3d: Captures worsening congestion trend\n",
        "df['OriginDelayTrend_3d'] = df.groupby('OriginAirportID')['OriginDelayMean_7d'].transform(lambda x: x.diff(3))\n",
        "\n"
      ],
      "metadata": {
        "id": "tovfhxSBlo_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Modeling"
      ],
      "metadata": {
        "id": "YIsWGA28lvjY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LightGBM"
      ],
      "metadata": {
        "id": "8T9jTXf4lw9V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Selection & Splitting"
      ],
      "metadata": {
        "id": "t45u2Jftlx6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = [\n",
        "\n",
        "    # 'Year', Removing Year to focus on dynamic predictors\n",
        "    'Month', 'DayOfWeek', 'Flight_Number_Reporting_Airline',\n",
        "    'OriginAirportID', 'OriginCityMarketID', 'OriginStateFips',\n",
        "    'DestAirportID', 'DestCityMarketID', 'DestStateFips',\n",
        "    'CRSDepTime', 'CRSArrTime', 'Distance', 'tavg', 'tmin', 'tmax',\n",
        "    'prcp', 'wspd', 'pres', 'IsWeekend', 'IsHoliday', 'IsHolidayWindow',\n",
        "    'NumDepartures', 'RouteDelayMean_7d', 'OriginDelayMean_7d',\n",
        "    'DestArrivals_7d', 'Departures_Today', 'Dist_x_Wspd', 'TempRange',\n",
        "    'MonthxWeekday',\n",
        "]\n",
        "# One-hot encoded categorical variables\n",
        "one_hot_cols = [col for col in df.columns if col.startswith('Origin_') or\n",
        "                col.startswith('Month_') or\n",
        "                col.startswith('DayOfWeek_') or\n",
        "                col.startswith('Dest_')]\n",
        "\n",
        "target = 'DepDelayMinutes_log'\n",
        "\n",
        "# Define X and y\n",
        "X = df[features + one_hot_cols]\n",
        "y = df[target]\n",
        "\n",
        "# Keep FlightDate in a separate series for splitting\n",
        "flight_dates = df['FlightDate']\n",
        "\n",
        "# Subsample for speed\n",
        "df_model = df[features + one_hot_cols + [target]].dropna().sample(500_000, random_state=42)\n",
        "flight_dates_model = flight_dates.loc[df_model.index]\n",
        "\n",
        "# Time-based split\n",
        "train_end = '2023-12-31'\n",
        "valid_end = '2024-12-31'\n",
        "\n",
        "train_mask = flight_dates_model <= train_end\n",
        "valid_mask = (flight_dates_model > train_end) & (flight_dates_model <= valid_end)\n",
        "test_mask = flight_dates_model > valid_end\n",
        "\n",
        "# Define X and y\n",
        "X = df_model[features + one_hot_cols]  # do NOT include FlightDate\n",
        "y = df_model[target]\n",
        "\n",
        "# Split and convert numeric to float32\n",
        "X_train = X[train_mask].astype(np.float32)\n",
        "X_valid = X[valid_mask].astype(np.float32)\n",
        "X_test  = X[test_mask].astype(np.float32)\n",
        "\n",
        "y_train = y[train_mask]\n",
        "y_valid = y[valid_mask]\n",
        "y_test  = y[test_mask]"
      ],
      "metadata": {
        "id": "u3F43mTZlmzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "8l2KPnfpl0ij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "    params = {\n",
        "        'n_estimators': 2000,\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.05, log=True),\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 200),\n",
        "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 20, 500),\n",
        "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
        "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
        "        'lambda_l1': trial.suggest_float('lambda_l1', 0.0, 1.0),\n",
        "        'lambda_l2': trial.suggest_float('lambda_l2', 0.0, 1.0),\n",
        "        'random_state': 42,\n",
        "        'n_jobs': -1\n",
        "    }\n",
        "\n",
        "    model = lgbm(**params)\n",
        "    model.fit(\n",
        "        X_train, y_train,  # y_train is log-transformed\n",
        "        eval_set=[(X_valid, y_valid)],\n",
        "        eval_metric='mae',\n",
        "        callbacks=[early_stopping(stopping_rounds=100)]\n",
        "    )\n",
        "\n",
        "    # Back-transform predictions to minutes\n",
        "    y_pred_log = model.predict(X_valid)\n",
        "    y_pred = np.expm1(y_pred_log)\n",
        "    y_valid_orig = np.expm1(y_valid)\n",
        "\n",
        "    mae = mean_absolute_error(y_valid_orig, y_pred)\n",
        "    return mae  # Optuna minimizes MAE in minutes\n",
        "\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=10)  # adjust trials as needed\n",
        "\n",
        "print(\"Best hyperparameters:\")\n",
        "print(study.best_params)\n",
        "print(\"Best MAE:\", study.best_value)"
      ],
      "metadata": {
        "id": "bcCg79hYl1fO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forecasting"
      ],
      "metadata": {
        "id": "rfXqnnJrl41f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Train LightGBM\n",
        "# =========================\n",
        "best_params = study.best_params\n",
        "lgb = lgbm(**best_params)\n",
        "\n",
        "lgb.fit(X_train, y_train)\n",
        "\n",
        "# =========================\n",
        "# Predict, back-transform, and evaluate\n",
        "# =========================\n",
        "\n",
        "y_pred_log = lgb.predict(X_test)\n",
        "y_pred = np.expm1(y_pred_log)       # back to minutes\n",
        "y_test_original = np.expm1(y_test)  # back to minutes\n",
        "\n",
        "# Evaluate in log space\n",
        "mae_log = mean_absolute_error(y_test, y_pred_log)\n",
        "rmse_log = np.sqrt(mean_squared_error(y_test, y_pred_log))\n",
        "r2_log = r2_score(y_test, y_pred_log)\n",
        "\n",
        "# Evaluate in original minutes\n",
        "mae = mean_absolute_error(y_test_original, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_test_original, y_pred))\n",
        "r2 = r2_score(y_test_original, y_pred)\n",
        "\n",
        "print(\"\\nModel Performance (LightGBM):\")\n",
        "print(\"\\nIn Log Space:\")\n",
        "print(f\"R¬≤:  {r2_log:.4f}\")\n",
        "print(\"\\nIn Original Minutes:\")\n",
        "print(f\"MAE:  {mae:.2f}\")\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"R¬≤:   {r2:.4f}\")\n",
        "\n",
        "# =========================\n",
        "# Feature importance plot\n",
        "# =========================\n",
        "importances = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'Importance': lgb.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x='Importance', y='Feature', data=importances.head(15), palette='viridis', legend=False)\n",
        "plt.title(\"Top 15 Most Important Features (LightGBM)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GrM_oKyyl7-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest"
      ],
      "metadata": {
        "id": "Ibw3bTkql-44"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipeline"
      ],
      "metadata": {
        "id": "FOB13rJDmAZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- New, Simplified Training Pipeline ---\n",
        "print(\"Starting classification training...\")\n",
        "\n",
        "# 1. Initialize a simple Classifier\n",
        "# We use 'class_weight' because most flights are NOT delayed\n",
        "rf_classifier = RandomForestClassifier(\n",
        "    n_estimators=100,        # Start simple\n",
        "    max_depth=10,            # A reasonable depth to prevent overfitting\n",
        "    min_samples_leaf=50,     # Don't let it learn from tiny groups\n",
        "    n_jobs=-1,               # Use all cores\n",
        "    random_state=42,\n",
        "    class_weight='balanced'  # <-- CRITICAL!\n",
        ")\n",
        "\n",
        "# 2. Train the model\n",
        "start_time = time.time()\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "print(f\"Training finished in {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "# 3. Get predictions for the Test set\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "y_pred_proba = rf_classifier.predict_proba(X_test)[:, 1] # Get 'Delayed' probability\n",
        "\n",
        "# 4. Print new Classification Metrics\n",
        "print(\"\\n--- Test Set Metrics ---\")\n",
        "print(f\"Accuracy:  {accuracy_score(y_test, y_pred):.3f}\")\n",
        "print(f\"Recall:    {recall_score(y_test, y_pred):.3f}\")     # <-- How many delays did we catch?\n",
        "print(f\"Precision: {precision_score(y_test, y_pred):.3f}\")  # <-- How many of our alarms were real?\n",
        "print(f\"ROC-AUC:   {roc_auc_score(y_test, y_pred_proba):.3f}\") # <-- The best overall metric"
      ],
      "metadata": {
        "id": "i1C3DCNcl_RH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "5wrrGdsOmCqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "    # 1. Define the parameters to search\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 100, 500), # Shortened for speed\n",
        "        'max_depth': trial.suggest_int('max_depth', 5, 20),\n",
        "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 10, 100),\n",
        "    }\n",
        "\n",
        "    # 2. Create the classifier\n",
        "    model = RandomForestClassifier(\n",
        "        **params,\n",
        "        n_jobs=-1,\n",
        "        random_state=42,\n",
        "        class_weight='balanced' # <-- Still critical\n",
        "    )\n",
        "\n",
        "    # 3. Train on a smaller sample to make tuning fast\n",
        "    X_train_sample = X_train.sample(n=100000, random_state=42)\n",
        "    y_train_sample = y_train.loc[X_train_sample.index]\n",
        "\n",
        "    model.fit(X_train_sample, y_train_sample)\n",
        "\n",
        "    # 4. Score it on the validation set\n",
        "    # We want to maximize the ROC-AUC score\n",
        "    val_preds_proba = model.predict_proba(X_val)[:, 1]\n",
        "    auc = roc_auc_score(y_val, val_preds_proba)\n",
        "\n",
        "    return auc\n",
        "\n",
        "# --- Run the study ---\n",
        "print(\"Starting Optuna study...\")\n",
        "# We want to MAXIMIZE the score\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=20) # Run for 20 trials\n",
        "\n",
        "print(\"\\nBest trial:\")\n",
        "print(f\"  Value (ROC-AUC): {study.best_value:.4f}\")\n",
        "print(\"  Params: \")\n",
        "for key, value in study.best_params.items():\n",
        "    print(f\"    {key}: {value}\")\n",
        "\n",
        "# You can now use these best_params in your final model\n",
        "best_params = study.best_params"
      ],
      "metadata": {
        "id": "pVJP_Rd9mEPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training & Results"
      ],
      "metadata": {
        "id": "klBpPHrAmFgx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FINAL: Train Tuned Model, Print Metrics, and Plot ---\n",
        "\n",
        "# 1. Initialize the FINAL Tuned Classifier\n",
        "print(\"Using best params from Optuna study...\")\n",
        "rf_classifier = RandomForestClassifier(\n",
        "    n_estimators=442,        # <-- From Optuna\n",
        "    max_depth=8,             # <-- From Optuna\n",
        "    min_samples_leaf=34,     # <-- From Optuna\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        "    class_weight='balanced'\n",
        ")\n",
        "\n",
        "# 2. Train the model\n",
        "print(\"Training the final model...\")\n",
        "start_time = time.time()\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "print(f\"Training finished in {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "# 3. Get predictions for the Test set\n",
        "print(\"Generating predictions...\")\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "y_pred_proba = rf_classifier.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# 4. Print new Classification Metrics\n",
        "print(\"\\n--- Test Set Metrics ---\")\n",
        "print(f\"Accuracy:  {accuracy_score(y_test, y_pred):.3f}\")\n",
        "print(f\"Recall:    {recall_score(y_test, y_pred):.3f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred):.3f}\")\n",
        "print(f\"ROC-AUC:   {roc_auc_score(y_test, y_pred_proba):.3f}\")\n",
        "\n",
        "# --- Plotting Results ---\n",
        "print(\"\\n--- Plotting Results ---\")\n",
        "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# 5. Plot Confusion Matrix\n",
        "ConfusionMatrixDisplay.from_estimator(\n",
        "    rf_classifier,\n",
        "    X_test,\n",
        "    y_test,\n",
        "    ax=ax[0],\n",
        "    cmap='Blues',\n",
        "    normalize='true'\n",
        ")\n",
        "ax[0].set_title('Confusion Matrix (Normalized)')\n",
        "\n",
        "# 6. Plot ROC Curve\n",
        "RocCurveDisplay.from_estimator(\n",
        "    rf_classifier,\n",
        "    X_test,\n",
        "    y_test,\n",
        "    ax=ax[1]\n",
        ")\n",
        "ax[1].set_title('ROC-AUC Curve')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Wtn3vjKOmFyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## General Model (awaiting)"
      ],
      "metadata": {
        "id": "b-vnmXSGmHX5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kU_-NgjGmLWH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}