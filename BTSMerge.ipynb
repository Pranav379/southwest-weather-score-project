{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eff8274",
   "metadata": {},
   "source": [
    "# Merged BTS and Weather Data"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "id": "3f21c867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faster solution to above for multiple years worth of data\n",
    "import requests\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from datetime import datetime\n",
    "\n",
    "def download_bts_data(year, month):\n",
    "    \"\"\"Download BTS data for a specific month\"\"\"\n",
    "    url = f'https://www.transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_{year}_{month}.zip'\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(\n",
    "            url, \n",
    "            headers={'User-Agent': 'Mozilla/5.0'},\n",
    "            timeout=300,\n",
    "            stream=True  # Stream large files\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Extract CSV from zip\n",
    "        with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "            csv_file = next((name for name in z.namelist() if name.endswith('.csv')), None)\n",
    "            if not csv_file:\n",
    "                return (year, month, None, \"No CSV in zip\")\n",
    "            \n",
    "            with z.open(csv_file) as f:\n",
    "                df = pd.read_csv(f, encoding='utf-8', low_memory=False)\n",
    "            \n",
    "            return (year, month, df, None)\n",
    "            \n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        return (year, month, None, f\"HTTP {e.response.status_code}\")\n",
    "    except Exception as e:\n",
    "        return (year, month, None, str(e)[:50])\n",
    "\n",
    "def download_years_parallel(start_year, end_year, max_workers=20):\n",
    "    \"\"\"\n",
    "    Download multiple years of data with maximum parallelization\n",
    "    \n",
    "    Args:\n",
    "        start_year: Starting year (e.g., 2005)\n",
    "        end_year: Ending year (e.g., 2024)\n",
    "        max_workers: Number of parallel downloads (default: 20)\n",
    "    \n",
    "    Returns:\n",
    "        Combined DataFrame\n",
    "    \"\"\"\n",
    "    # Generate all year-month combinations\n",
    "    months_to_download = []\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        for month in range(1, 13):\n",
    "            # Skip future months\n",
    "            current_date = datetime.now()\n",
    "            if year > current_date.year or (year == current_date.year and month > current_date.month):\n",
    "                continue\n",
    "            months_to_download.append((year, month))\n",
    "    \n",
    "    total_months = len(months_to_download)\n",
    "    print(f\"Downloading {total_months} months ({start_year}-{end_year})\")\n",
    "    print(f\"Using {max_workers} parallel workers\")\n",
    "    print(f\"Estimated time: {(total_months * 2.5) / max_workers:.1f} minutes\\n\")\n",
    "    \n",
    "    results = {}\n",
    "    completed = 0\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # Download all months in parallel\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {\n",
    "            executor.submit(download_bts_data, year, month): (year, month) \n",
    "            for year, month in months_to_download\n",
    "        }\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            year, month, df, error = future.result()\n",
    "            completed += 1\n",
    "            \n",
    "            if df is not None:\n",
    "                results[(year, month)] = df\n",
    "                print(f\"âœ“ {year}-{month:02d} ({len(df):,} rows) [{completed}/{total_months}]\")\n",
    "            else:\n",
    "                results[(year, month)] = None\n",
    "                print(f\"âœ— {year}-{month:02d} - {error} [{completed}/{total_months}]\")\n",
    "    \n",
    "    elapsed = (datetime.now() - start_time).total_seconds() / 60\n",
    "    \n",
    "    # Sort and combine successful downloads\n",
    "    successful = [(k, v) for k, v in sorted(results.items()) if v is not None]\n",
    "    failed = [(k, v) for k, v in sorted(results.items()) if v is None]\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Download Complete in {elapsed:.1f} minutes\")\n",
    "    print(f\"  Successful: {len(successful)}/{total_months} months\")\n",
    "    if failed:\n",
    "        print(f\"  Failed: {len(failed)} months - {[f'{y}-{m:02d}' for (y,m), _ in failed[:10]]}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    if successful:\n",
    "        print(\"Combining all data...\")\n",
    "        combine_start = datetime.now()\n",
    "        \n",
    "        # Combine all dataframes at once\n",
    "        combined_df = pd.concat([df for _, df in successful], ignore_index=True, copy=False)\n",
    "        \n",
    "        combine_time = (datetime.now() - combine_start).total_seconds()\n",
    "        print(f\"âœ“ Combined {len(combined_df):,} rows in {combine_time:.1f}s\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"âœ— No data downloaded\")\n",
    "        return None\n",
    "\n",
    "def save_to_csv_fast(df, filename):\n",
    "    \"\"\"Save DataFrame to CSV with progress\"\"\"\n",
    "    print(f\"\\nSaving to {filename}...\")\n",
    "    start = datetime.now()\n",
    "    \n",
    "    # Save with efficient settings\n",
    "    df.to_csv(filename, index=False)\n",
    "    \n",
    "    elapsed = (datetime.now() - start).total_seconds()\n",
    "    file_size = os.path.getsize(filename) / (1024 * 1024)\n",
    "    \n",
    "    print(f\"âœ“ Saved in {elapsed:.1f}s ({file_size:.1f} MB)\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*70)\n",
    "    print(\"BTS On-Time Performance Data - ULTRA FAST DOWNLOADER\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # Configure your date range here\n",
    "    START_YEAR = 2005\n",
    "    END_YEAR = 2024\n",
    "    MAX_WORKERS = 12  # Balanced: fast but respectful to server\n",
    "    \n",
    "    print(f\"Configuration:\")\n",
    "    print(f\"  Years: {START_YEAR} to {END_YEAR}\")\n",
    "    print(f\"  Parallel workers: {MAX_WORKERS}\")\n",
    "    print(f\"  Note: 240 months = 20 years (2005-2024)\\n\")\n",
    "    \n",
    "    # Download all data\n",
    "    combined_data = download_years_parallel(START_YEAR, END_YEAR, max_workers=MAX_WORKERS)\n",
    "    \n",
    "    if combined_data is not None:\n",
    "        output_file = f\"bts_ontime_{START_YEAR}_{END_YEAR}_combined.csv\"\n",
    "        save_to_csv_fast(combined_data, output_file)\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"âœ“ SUCCESS!\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"  File: {output_file}\")\n",
    "        print(f\"  Total rows: {len(combined_data):,}\")\n",
    "        print(f\"  Columns: {len(combined_data.columns)}\")\n",
    "        print(f\"  Date range: {combined_data['FL_DATE'].min()} to {combined_data['FL_DATE'].max()}\")\n",
    "        print(f\"\\nFirst 10 columns:\")\n",
    "        for i, col in enumerate(combined_data.columns[:10], 1):\n",
    "            print(f\"  {i}. {col}\")\n",
    "        print(f\"{'='*70}\")\n",
    "    else:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"âœ— FAILED - No data downloaded\")\n",
    "        print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
=======
   "execution_count": 38,
>>>>>>> 62e6e95 (Changes with combined delay data)
   "id": "4c48f4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FL_DATE                   0.000000\n",
      "OP_UNIQUE_CARRIER         0.000000\n",
      "ORIGIN_AIRPORT_ID         0.000000\n",
      "ORIGIN_AIRPORT_SEQ_ID     0.000000\n",
      "ORIGIN_CITY_MARKET_ID     0.000000\n",
      "ORIGIN                    0.000000\n",
      "ORIGIN_CITY_NAME          0.000000\n",
      "ORIGIN_STATE_ABR          0.000000\n",
      "ORIGIN_STATE_FIPS         0.000000\n",
      "ORIGIN_STATE_NM           0.000000\n",
      "ORIGIN_WAC                0.000000\n",
      "DEST_AIRPORT_ID           0.000000\n",
      "DEST_AIRPORT_SEQ_ID       0.000000\n",
      "DEST_CITY_MARKET_ID       0.000000\n",
      "DEST                      0.000000\n",
      "DEST_CITY_NAME            0.000000\n",
      "DEST_STATE_ABR            0.000000\n",
      "DEST_STATE_FIPS           0.000000\n",
      "DEST_STATE_NM             0.000000\n",
      "DEST_WAC                  0.000000\n",
      "CRS_DEP_TIME              0.000000\n",
      "DEP_TIME                  1.661803\n",
      "DEP_DELAY                 1.669160\n",
      "DEP_DELAY_NEW             1.669160\n",
      "DEP_DEL15                 1.669160\n",
      "DEP_DELAY_GROUP           1.669160\n",
      "DEP_TIME_BLK              0.000000\n",
      "TAXI_OUT                  1.706187\n",
      "WHEELS_OFF                1.706187\n",
      "WHEELS_ON                 1.750631\n",
      "TAXI_IN                   1.750631\n",
      "CRS_ARR_TIME              0.000000\n",
      "ARR_TIME                  1.750511\n",
      "ARR_DELAY                 1.942218\n",
      "ARR_DELAY_NEW             1.942218\n",
      "ARR_DEL15                 1.942218\n",
      "ARR_DELAY_GROUP           1.942218\n",
      "ARR_TIME_BLK              0.000000\n",
      "CANCELLED                 0.000000\n",
      "CANCELLATION_CODE        98.280667\n",
      "DIVERTED                  0.000000\n",
      "CRS_ELAPSED_TIME          0.000060\n",
      "ACTUAL_ELAPSED_TIME       1.942218\n",
      "AIR_TIME                  1.942218\n",
      "FLIGHTS                   0.000000\n",
      "DISTANCE                  0.000000\n",
      "DISTANCE_GROUP            0.000000\n",
      "CARRIER_DELAY            80.070785\n",
      "WEATHER_DELAY            80.070785\n",
      "NAS_DELAY                80.070785\n",
      "SECURITY_DELAY           80.070785\n",
      "LATE_AIRCRAFT_DELAY      80.070785\n",
      "FIRST_DEP_TIME           99.291546\n",
      "TOTAL_ADD_GTIME          99.291546\n",
      "LONGEST_ADD_GTIME        99.291546\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
<<<<<<< HEAD
    "# Reading 2024 data\n",
    "df = pd.read_csv(\"bts_ontime_2024_combined.csv\")\n",
=======
    "# Reading Jan and Feb data\n",
    "files = [\"Data/Jan_2024_Delays.csv\", \"Data/Feb_2024_Delays.csv\", \"Data/March_2024_Delays.csv\"]\n",
    "\n",
    "# Load all CSVs into a list\n",
    "dfs = [pd.read_csv(f) for f in files]\n",
    "\n",
    "# Combine into one DataFrame\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
>>>>>>> 62e6e95 (Changes with combined delay data)
    "\n",
    "percent_M = df.isna().mean() * 100\n",
    "\n",
    "print(percent_M)\n"
   ]
<<<<<<< HEAD
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "14997704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ABQ (USW00023050) ...\n",
      "Parsing data for ABQ (USW00023050) ...\n",
      "Parsing data for ABQ (USW00023050) ...\n",
      "Saved CSV for ABQ: sw_airport_daily_weather_bulk\\ABQ_daily_weather.csv\n",
      "Downloading AMA (USW00023020) ...\n",
      "Saved CSV for ABQ: sw_airport_daily_weather_bulk\\ABQ_daily_weather.csv\n",
      "Downloading AMA (USW00023020) ...\n",
      "Parsing data for AMA (USW00023020) ...\n",
      "Parsing data for AMA (USW00023020) ...\n",
      "Saved CSV for AMA: sw_airport_daily_weather_bulk\\AMA_daily_weather.csv\n",
      "Downloading ATL (USW00013874) ...\n",
      "Saved CSV for AMA: sw_airport_daily_weather_bulk\\AMA_daily_weather.csv\n",
      "Downloading ATL (USW00013874) ...\n",
      "Parsing data for ATL (USW00013874) ...\n",
      "Parsing data for ATL (USW00013874) ...\n",
      "Saved CSV for ATL: sw_airport_daily_weather_bulk\\ATL_daily_weather.csv\n",
      "Downloading AUS (USW00013958) ...\n",
      "Saved CSV for ATL: sw_airport_daily_weather_bulk\\ATL_daily_weather.csv\n",
      "Downloading AUS (USW00013958) ...\n",
      "Parsing data for AUS (USW00013958) ...\n",
      "Parsing data for AUS (USW00013958) ...\n",
      "Saved CSV for AUS: sw_airport_daily_weather_bulk\\AUS_daily_weather.csv\n",
      "Downloading BWI (USW00093721) ...\n",
      "Saved CSV for AUS: sw_airport_daily_weather_bulk\\AUS_daily_weather.csv\n",
      "Downloading BWI (USW00093721) ...\n",
      "Parsing data for BWI (USW00093721) ...\n",
      "Parsing data for BWI (USW00093721) ...\n",
      "Saved CSV for BWI: sw_airport_daily_weather_bulk\\BWI_daily_weather.csv\n",
      "Downloading BUR (USW00093195) ...\n",
      "Saved CSV for BWI: sw_airport_daily_weather_bulk\\BWI_daily_weather.csv\n",
      "Downloading BUR (USW00093195) ...\n",
      "âŒ Failed to download station USW00093195 for airport BUR\n",
      "Downloading CHS (USW00093824) ...\n",
      "âŒ Failed to download station USW00093195 for airport BUR\n",
      "Downloading CHS (USW00093824) ...\n",
      "Parsing data for CHS (USW00093824) ...\n",
      "Parsing data for CHS (USW00093824) ...\n",
      "Saved CSV for CHS: sw_airport_daily_weather_bulk\\CHS_daily_weather.csv\n",
      "Downloading CLE (USW00014830) ...\n",
      "Saved CSV for CHS: sw_airport_daily_weather_bulk\\CHS_daily_weather.csv\n",
      "Downloading CLE (USW00014830) ...\n",
      "Parsing data for CLE (USW00014830) ...\n",
      "Parsing data for CLE (USW00014830) ...\n",
      "Saved CSV for CLE: sw_airport_daily_weather_bulk\\CLE_daily_weather.csv\n",
      "Downloading CMH (USW00094846) ...\n",
      "Saved CSV for CLE: sw_airport_daily_weather_bulk\\CLE_daily_weather.csv\n",
      "Downloading CMH (USW00094846) ...\n",
      "Parsing data for CMH (USW00094846) ...\n",
      "Parsing data for CMH (USW00094846) ...\n",
      "Saved CSV for CMH: sw_airport_daily_weather_bulk\\CMH_daily_weather.csv\n",
      "Downloading DAL (USW00013960) ...\n",
      "Saved CSV for CMH: sw_airport_daily_weather_bulk\\CMH_daily_weather.csv\n",
      "Downloading DAL (USW00013960) ...\n",
      "Parsing data for DAL (USW00013960) ...\n",
      "Parsing data for DAL (USW00013960) ...\n",
      "Saved CSV for DAL: sw_airport_daily_weather_bulk\\DAL_daily_weather.csv\n",
      "Downloading DEN (USW00023040) ...\n",
      "Saved CSV for DAL: sw_airport_daily_weather_bulk\\DAL_daily_weather.csv\n",
      "Downloading DEN (USW00023040) ...\n",
      "Parsing data for DEN (USW00023040) ...\n",
      "Parsing data for DEN (USW00023040) ...\n",
      "Saved CSV for DEN: sw_airport_daily_weather_bulk\\DEN_daily_weather.csv\n",
      "Downloading HOU (USW00012918) ...\n",
      "Saved CSV for DEN: sw_airport_daily_weather_bulk\\DEN_daily_weather.csv\n",
      "Downloading HOU (USW00012918) ...\n",
      "Parsing data for HOU (USW00012918) ...\n",
      "Parsing data for HOU (USW00012918) ...\n",
      "Saved CSV for HOU: sw_airport_daily_weather_bulk\\HOU_daily_weather.csv\n",
      "Downloading LAS (USW00024049) ...\n",
      "Saved CSV for HOU: sw_airport_daily_weather_bulk\\HOU_daily_weather.csv\n",
      "Downloading LAS (USW00024049) ...\n",
      "âŒ Failed to download station USW00024049 for airport LAS\n",
      "Downloading LAX (USW00023174) ...\n",
      "âŒ Failed to download station USW00024049 for airport LAS\n",
      "Downloading LAX (USW00023174) ...\n",
      "Parsing data for LAX (USW00023174) ...\n",
      "Parsing data for LAX (USW00023174) ...\n",
      "Saved CSV for LAX: sw_airport_daily_weather_bulk\\LAX_daily_weather.csv\n",
      "âœ… Station file already exists for MDW (USW00094846)\n",
      "Parsing data for MDW (USW00094846) ...\n",
      "Saved CSV for LAX: sw_airport_daily_weather_bulk\\LAX_daily_weather.csv\n",
      "âœ… Station file already exists for MDW (USW00094846)\n",
      "Parsing data for MDW (USW00094846) ...\n",
      "Saved CSV for MDW: sw_airport_daily_weather_bulk\\MDW_daily_weather.csv\n",
      "Downloading MCO (USW00012839) ...\n",
      "Saved CSV for MDW: sw_airport_daily_weather_bulk\\MDW_daily_weather.csv\n",
      "Downloading MCO (USW00012839) ...\n",
      "Parsing data for MCO (USW00012839) ...\n",
      "Parsing data for MCO (USW00012839) ...\n",
      "Saved CSV for MCO: sw_airport_daily_weather_bulk\\MCO_daily_weather.csv\n",
      "Downloading OAK (USW00023272) ...\n",
      "Saved CSV for MCO: sw_airport_daily_weather_bulk\\MCO_daily_weather.csv\n",
      "Downloading OAK (USW00023272) ...\n",
      "Parsing data for OAK (USW00023272) ...\n",
      "Parsing data for OAK (USW00023272) ...\n",
      "Saved CSV for OAK: sw_airport_daily_weather_bulk\\OAK_daily_weather.csv\n",
      "Downloading PHX (USW00023183) ...\n",
      "Saved CSV for OAK: sw_airport_daily_weather_bulk\\OAK_daily_weather.csv\n",
      "Downloading PHX (USW00023183) ...\n",
      "Parsing data for PHX (USW00023183) ...\n",
      "Parsing data for PHX (USW00023183) ...\n",
      "Saved CSV for PHX: sw_airport_daily_weather_bulk\\PHX_daily_weather.csv\n",
      "Downloading SAN (USW00023188) ...\n",
      "Saved CSV for PHX: sw_airport_daily_weather_bulk\\PHX_daily_weather.csv\n",
      "Downloading SAN (USW00023188) ...\n",
      "Parsing data for SAN (USW00023188) ...\n",
      "Parsing data for SAN (USW00023188) ...\n",
      "Saved CSV for SAN: sw_airport_daily_weather_bulk\\SAN_daily_weather.csv\n",
      "Downloading SFO (USW00023234) ...\n",
      "Saved CSV for SAN: sw_airport_daily_weather_bulk\\SAN_daily_weather.csv\n",
      "Downloading SFO (USW00023234) ...\n",
      "Parsing data for SFO (USW00023234) ...\n",
      "Parsing data for SFO (USW00023234) ...\n",
      "Saved CSV for SFO: sw_airport_daily_weather_bulk\\SFO_daily_weather.csv\n",
      "Downloading SJC (USW00023282) ...\n",
      "Saved CSV for SFO: sw_airport_daily_weather_bulk\\SFO_daily_weather.csv\n",
      "Downloading SJC (USW00023282) ...\n",
      "âŒ Failed to download station USW00023282 for airport SJC\n",
      "Downloading SMF (USW00023266) ...\n",
      "âŒ Failed to download station USW00023282 for airport SJC\n",
      "Downloading SMF (USW00023266) ...\n",
      "âŒ Failed to download station USW00023266 for airport SMF\n",
      "Downloading STL (USW00093928) ...\n",
      "âŒ Failed to download station USW00023266 for airport SMF\n",
      "Downloading STL (USW00093928) ...\n",
      "Parsing data for STL (USW00093928) ...\n",
      "Parsing data for STL (USW00093928) ...\n",
      "Saved CSV for STL: sw_airport_daily_weather_bulk\\STL_daily_weather.csv\n",
      "âœ… Station file already exists for TPA (USW00012839)\n",
      "Parsing data for TPA (USW00012839) ...\n",
      "Saved CSV for STL: sw_airport_daily_weather_bulk\\STL_daily_weather.csv\n",
      "âœ… Station file already exists for TPA (USW00012839)\n",
      "Parsing data for TPA (USW00012839) ...\n",
      "Saved CSV for TPA: sw_airport_daily_weather_bulk\\TPA_daily_weather.csv\n",
      "Combining all airport CSVs into one bulk CSV...\n",
      "Saved CSV for TPA: sw_airport_daily_weather_bulk\\TPA_daily_weather.csv\n",
      "Combining all airport CSVs into one bulk CSV...\n",
      "ðŸŽ¯ Bulk CSV saved: sw_airport_daily_weather_bulk\\SW_airports_all_daily_weather.csv\n",
      "         DATE  AWND  PRCP  TMAX  TMIN AIRPORT\n",
      "0  1931-03-01   NaN   0.0   7.2  -2.2     ABQ\n",
      "1  1931-03-02   NaN   0.0  13.3  -3.9     ABQ\n",
      "2  1931-03-03   NaN   0.0  17.8   1.1     ABQ\n",
      "3  1931-03-04   NaN   0.0  18.3   0.6     ABQ\n",
      "4  1931-03-05   NaN   0.0  11.1   5.0     ABQ\n",
      "ðŸŽ¯ Bulk CSV saved: sw_airport_daily_weather_bulk\\SW_airports_all_daily_weather.csv\n",
      "         DATE  AWND  PRCP  TMAX  TMIN AIRPORT\n",
      "0  1931-03-01   NaN   0.0   7.2  -2.2     ABQ\n",
      "1  1931-03-02   NaN   0.0  13.3  -3.9     ABQ\n",
      "2  1931-03-03   NaN   0.0  17.8   1.1     ABQ\n",
      "3  1931-03-04   NaN   0.0  18.3   0.6     ABQ\n",
      "4  1931-03-05   NaN   0.0  11.1   5.0     ABQ\n"
     ]
    }
   ],
   "source": [
    "# Combined weather data\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import glob\n",
    "\n",
    "# -----------------------------\n",
    "# Airport â†’ GHCNâ€‘Daily Station ID mapping\n",
    "# -----------------------------\n",
    "stations = {\n",
    "    \"ABQ\": \"USW00023050\",\n",
    "    \"AMA\": \"USW00023020\",\n",
    "    \"ATL\": \"USW00013874\",\n",
    "    \"AUS\": \"USW00013958\",\n",
    "    \"BWI\": \"USW00093721\",\n",
    "    \"BUR\": \"USW00093195\",\n",
    "    \"CHS\": \"USW00093824\",\n",
    "    \"CLE\": \"USW00014830\",\n",
    "    \"CMH\": \"USW00094846\",\n",
    "    \"DAL\": \"USW00013960\",\n",
    "    \"DEN\": \"USW00023040\",\n",
    "    \"HOU\": \"USW00012918\",\n",
    "    \"LAS\": \"USW00024049\",\n",
    "    \"LAX\": \"USW00023174\",\n",
    "    \"MDW\": \"USW00094846\",\n",
    "    \"MCO\": \"USW00012839\",\n",
    "    \"OAK\": \"USW00023272\",\n",
    "    \"PHX\": \"USW00023183\",\n",
    "    \"SAN\": \"USW00023188\",\n",
    "    \"SFO\": \"USW00023234\",\n",
    "    \"SJC\": \"USW00023282\",\n",
    "    \"SMF\": \"USW00023266\",\n",
    "    \"STL\": \"USW00093928\",\n",
    "    \"TPA\": \"USW00012839\",\n",
    "    # Add more airports and station IDs as needed\n",
    "}\n",
    "\n",
    "BASE_URL = \"https://www.ncei.noaa.gov/pub/data/ghcn/daily/all/{}.dly\"\n",
    "OUTPUT_DIR = \"sw_airport_daily_weather_bulk\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Variables to extract\n",
    "VARS = {\"TMAX\", \"TMIN\", \"PRCP\", \"AWND\"}\n",
    "\n",
    "def parse_ghcn_dly(file_path):\n",
    "    rows = []\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            year = int(line[11:15])\n",
    "            month = int(line[15:17])\n",
    "            variable = line[17:21]\n",
    "            if variable not in VARS:\n",
    "                continue\n",
    "            for day in range(31):\n",
    "                raw = line[21 + day*8:26 + day*8]\n",
    "                try:\n",
    "                    value = int(raw)\n",
    "                except:\n",
    "                    continue\n",
    "                if value == -9999:\n",
    "                    continue\n",
    "                date = f\"{year}-{month:02d}-{day+1:02d}\"\n",
    "                if variable in [\"TMAX\", \"TMIN\"]:\n",
    "                    val = value / 10.0\n",
    "                elif variable == \"PRCP\":\n",
    "                    val = value / 10.0\n",
    "                elif variable == \"AWND\":\n",
    "                    val = value / 10.0\n",
    "                else:\n",
    "                    val = value\n",
    "                rows.append({\"DATE\": date, \"VARIABLE\": variable, \"VALUE\": val})\n",
    "    df = pd.DataFrame(rows)\n",
    "    df = df.pivot(index=\"DATE\", columns=\"VARIABLE\", values=\"VALUE\").reset_index()\n",
    "    return df\n",
    "\n",
    "# -----------------------------\n",
    "# Download, parse, save per airport\n",
    "# -----------------------------\n",
    "for airport, stn in stations.items():\n",
    "    url = BASE_URL.format(stn)\n",
    "    local_file = os.path.join(OUTPUT_DIR, f\"{stn}.dly\")\n",
    "    \n",
    "    if not os.path.exists(local_file):\n",
    "        print(f\"Downloading {airport} ({stn}) ...\")\n",
    "        r = requests.get(url, timeout=60)\n",
    "        if r.status_code != 200:\n",
    "            print(f\"âŒ Failed to download station {stn} for airport {airport}\")\n",
    "            continue\n",
    "        with open(local_file, \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "    else:\n",
    "        print(f\"âœ… Station file already exists for {airport} ({stn})\")\n",
    "    \n",
    "    print(f\"Parsing data for {airport} ({stn}) ...\")\n",
    "    try:\n",
    "        df = parse_ghcn_dly(local_file)\n",
    "        df[\"AIRPORT\"] = airport  # add airport column\n",
    "        out_csv = os.path.join(OUTPUT_DIR, f\"{airport}_daily_weather.csv\")\n",
    "        df.to_csv(out_csv, index=False)\n",
    "        print(f\"Saved CSV for {airport}: {out_csv}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error parsing {airport} ({stn}): {e}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Combine all airport CSVs into one bulk CSV\n",
    "# -----------------------------\n",
    "print(\"Combining all airport CSVs into one bulk CSV...\")\n",
    "all_files = glob.glob(os.path.join(OUTPUT_DIR, \"*_daily_weather.csv\"))\n",
    "df_all = pd.concat([pd.read_csv(f) for f in all_files], ignore_index=True)\n",
    "bulk_csv_path = os.path.join(OUTPUT_DIR, \"SW_airports_all_daily_weather.csv\")\n",
    "df_all.to_csv(bulk_csv_path, index=False)\n",
    "print(f\"ðŸŽ¯ Bulk CSV saved: {bulk_csv_path}\")\n",
    "print(df_all.head())\n"
   ]
=======
>>>>>>> 62e6e95 (Changes with combined delay data)
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
