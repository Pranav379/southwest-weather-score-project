{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsMLKjt6kVS9"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7n2BrKv9kWff"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
        "import numpy as np\n",
        "import polars as pl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import PowerTransformer, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, root_mean_squared_error, r2_score, accuracy_score, recall_score, roc_auc_score, ConfusionMatrixDisplay, RocCurveDisplay, precision_score, classification_report\n",
        "from lightgbm import LGBMRegressor as lgbm, early_stopping\n",
        "import optuna\n",
        "from boruta import BorutaPy\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "import os\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "from datetime import datetime\n",
        "from meteostat import Point, Daily\n",
        "from pathlib import Path\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3StZHpukYdn"
      },
      "source": [
        "# Step 1: Data Collection and Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJIXoh8bkbDp"
      },
      "source": [
        "## Data Collection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vjccpwgkcpk"
      },
      "source": [
        "### Import Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgAzR9nqkZX6"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"FINAL_DATASET.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIOjS0ypkf2h"
      },
      "source": [
        "### Display Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_w2xfrK4kgxe"
      },
      "outputs": [],
      "source": [
        "print(df.head())\n",
        "print(\"Data Range:\")\n",
        "print(df['FlightDate'].iloc[[0, -1]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2xlQ7QEkiFF"
      },
      "outputs": [],
      "source": [
        "df_grouped = df.groupby(['Origin', 'Dest', 'Month', 'DayOfWeek']).agg({\n",
        "    'DepDelayMinutes': 'mean',\n",
        "    'wspd': 'mean', 'tavg': 'mean', 'tmin': 'mean', 'tmax': 'mean', 'prcp': 'mean',\n",
        "    'snow': 'mean', 'wdir': 'mean', 'wspd': 'mean', 'wpgt': 'mean', 'pres': 'mean',\n",
        "}).reset_index()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEHGNAP6kxHc"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AykIlX1gkzqC"
      },
      "source": [
        "### Converting to Imperial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMfOdhCVki5L"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"FINAL_DATASET.csv\")\n",
        "\n",
        "# Temperature: Convert ¬∞C to ¬∞F\n",
        "for col in [\"tavg\", \"tmin\", \"tmax\"]:\n",
        "    if col in df.columns:\n",
        "        df[col] = (df[col] * 9/5 + 32).round().astype(\"Int64\")\n",
        "\n",
        "# Precipitation & Snow: Convert mm to inches\n",
        "for col in [\"prcp\", \"snow\"]:\n",
        "    if col in df.columns:\n",
        "        df[col] = df[col] / 25.4\n",
        "\n",
        "# Wind Speed: Convert km/h to mph\n",
        "if \"wspd\" in df.columns:\n",
        "    df[\"wspd\"] = df[\"wspd\"] / 1.60934\n",
        "\n",
        "# Pressure: Convert hPa to inHg\n",
        "if \"pres\" in df.columns:\n",
        "    df[\"pres\"] = df[\"pres\"] * 0.02953\n",
        "\n",
        "print(df.head(1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwmC0gY9k1fu"
      },
      "source": [
        "### Converting Time to Cyclical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jl1zK4I3lBUY"
      },
      "outputs": [],
      "source": [
        "# Transforming departure time to cyclical\n",
        "print(df['CRSDepTime'])\n",
        "\n",
        "df['DepHour'] = df['CRSDepTime'] // 100\n",
        "df['DepMinute'] = df['CRSDepTime'] % 100\n",
        "df['DepTotalMinutes'] = df['DepHour'] * 60 + df['DepMinute']\n",
        "\n",
        "df['DepTime_sin'] = np.sin(2*np.pi*df['DepTotalMinutes'] / 1440)\n",
        "df['DepTime_cos'] = np.cos(2*np.pi*df['DepTotalMinutes'] / 1440)\n",
        "\n",
        "print(df['DepTime_sin'].head())\n",
        "print(df['DepTime_cos'].head())\n",
        "\n",
        "# Same for arrival\n",
        "print(df['CRSArrTime'])\n",
        "\n",
        "df['ArrHour'] = df['CRSArrTime'] // 100\n",
        "df['ArrMinute'] = df['CRSArrTime'] % 100\n",
        "df['ArrTotalMinutes'] = df['ArrHour'] * 60 + df['ArrMinute']\n",
        "\n",
        "df['ArrTime_sin'] = np.sin(2*np.pi*df['ArrTotalMinutes'] / 1440)\n",
        "df['ArrTime_cos'] = np.cos(2*np.pi*df['ArrTotalMinutes'] / 1440)\n",
        "\n",
        "print(df['ArrTime_sin'].head())\n",
        "print(df['ArrTime_cos'].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBIGaNE2lCZT"
      },
      "source": [
        "### Treating Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4BwwksQlDa9"
      },
      "outputs": [],
      "source": [
        "# Percentage NA by column\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "percent_na = df.isna().mean() * 100\n",
        "print(percent_na.sort_values(ascending = False).head(75))\n",
        "\n",
        "# Removing >95% missing cols and IATA Code column due to redundancy\n",
        "cols_to_drop = percent_na[percent_na > 95].index\n",
        "df = df.drop(columns = cols_to_drop)\n",
        "df = df.drop(columns = [\"IATA_CODE_Reporting_Airline\"])\n",
        "\n",
        "print(f\"\\nDropped {len(cols_to_drop)} columns\")\n",
        "print(\"Remaining columns: \", len(df.columns))\n",
        "\n",
        "# Printing percentage again\n",
        "percent_na = df.isna().mean() * 100\n",
        "print(percent_na.sort_values(ascending = False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 2: EDA & Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65Xep8JLlE3k"
      },
      "source": [
        "### Weather Graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHNfDZ1glFv3"
      },
      "outputs": [],
      "source": [
        "df['wspd'] = np.clip(df['wspd'], 0, 50) # Clip for realism\n",
        "df['prcp'] = np.clip(df['prcp'], 0, 50) # Clip for realism\n",
        "# Convert Snow_Presence to a descriptive category for better plotting\n",
        "df['snow'] = df['snow'].map({0: 'No Snow', 1: 'Snow'})\n",
        "# ----------------------------------------------------\n",
        "\n",
        "# Set a style for better visualization\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "## üìä Plotting Continuous Variables (Temperature, Wind Speed, Precipitation)\n",
        "\n",
        "# Create a figure with subplots\n",
        "fig, axes = plt.subplots(3, 1, figsize=(10, 15))\n",
        "plt.suptitle('Distribution of Key Weather Factors', fontsize=16, y=1.02)\n",
        "\n",
        "# --- 1. Temperature Distribution (Histogram with KDE) ---\n",
        "sns.histplot(df['tavg'], bins=30, kde=True, color='skyblue', ax=axes[0])\n",
        "axes[0].set_title('Temperature Distribution', fontsize=14)\n",
        "axes[0].set_xlabel('Temperature (¬∞F)')\n",
        "axes[0].set_ylabel('Count / Density')\n",
        "#\n",
        "\n",
        "# --- 2. Wind Speed Distribution (Histogram with KDE) ---\n",
        "sns.histplot(df['wspd'], bins=30, kde=True, color='orange', ax=axes[1])\n",
        "axes[1].set_title('Wind Speed Distribution', fontsize=14)\n",
        "axes[1].set_xlabel('Wind Speed (mph)')\n",
        "axes[1].set_ylabel('Count / Density')\n",
        "# Add a vertical line for the mean to highlight the average condition\n",
        "axes[1].axvline(df['wspd'].mean(), color='r', linestyle='--', label=f\"Mean: {df['wspd'].mean():.2f}\")\n",
        "axes[1].legend()\n",
        "\n",
        "# --- 3. Precipitation Distribution (KDE Plot for Skewed Data) ---\n",
        "# A KDE plot is often better for highly skewed data like precipitation,\n",
        "# where many values are zero or near-zero.\n",
        "sns.kdeplot(df['prcp'], fill=True, color='green', ax=axes[2],\n",
        "            bw_adjust=0.5) # bw_adjust controls smoothness\n",
        "axes[2].set_title('Precipitation Distribution', fontsize=14)\n",
        "axes[2].set_xlabel('Precipitation (in)')\n",
        "axes[2].set_ylabel('Density')\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pJ5Sl0elHXk"
      },
      "outputs": [],
      "source": [
        "weather_df = df[['tavg', 'tmin', 'tmax', 'prcp', 'snow', 'wspd', 'pres']]\n",
        "plt.figure(figsize = (15, 10))\n",
        "sns.heatmap(weather_df.corr(), annot = True, cmap = \"coolwarm\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLb1UHzLlIKL"
      },
      "outputs": [],
      "source": [
        "weather_cols = ['tavg', 'tmin', 'tmax', 'prcp', 'snow', 'wspd', 'pres']\n",
        "n_cols = len(weather_cols)\n",
        "fig, axes = plt.subplots(\n",
        "    nrows=(n_cols + 1) // 2,\n",
        "    ncols=2,\n",
        "    figsize=(15, 5 * ((n_cols + 1) // 2))\n",
        ")\n",
        "\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, col in enumerate(weather_cols):\n",
        "    # 1. Create Bins (Categories) for the continuous variable\n",
        "\n",
        "    # Use qcut to create 4 bins with roughly equal number of flights in each (quartiles)\n",
        "    if col in ['tavg', 'tmin', 'tmax', 'wspd', 'pres']:\n",
        "        df[f'{col}_Category'] = pd.qcut(df[col], q=4, labels=[f'Q1 ({col})', f'Q2 ({col})', f'Q3 ({col})', f'Q4 ({col})'])\n",
        "        x_label = f'Quartile of {col} (e.g., Temperature, Wind Speed)'\n",
        "    # Special handling for precipitation (prcp) and snow, as they often have many zeros\n",
        "    elif col in ['prcp', 'snow']:\n",
        "        # Create simple categories: Zero, Low, Medium, High\n",
        "        if col == 'prcp':\n",
        "            bins = [-np.inf, 0.01, 0.2, 1.0, np.inf]\n",
        "            labels = ['None', 'Light', 'Moderate', 'Heavy']\n",
        "        else: # snow\n",
        "            bins = [-np.inf, 0.1, 1.0, 5.0, np.inf]\n",
        "            labels = ['None/Trace', 'Light', 'Medium', 'Heavy']\n",
        "\n",
        "        df[f'{col}_Category'] = pd.cut(df[col], bins=bins, labels=labels, right=True)\n",
        "        x_label = f'{col} Category'\n",
        "\n",
        "    # 2. Calculate the delay rate (mean of DepDel15) for each new category\n",
        "    delay_summary = df.groupby(f'{col}_Category')['DepDel15'].mean().reset_index()\n",
        "    delay_summary['Delay_Rate_Pct'] = delay_summary['DepDel15'] * 100\n",
        "\n",
        "    # 3. Plot the result\n",
        "    sns.barplot(\n",
        "        x=f'{col}_Category',\n",
        "        y='Delay_Rate_Pct',\n",
        "        data=delay_summary,\n",
        "        ax=axes[i],\n",
        "        palette='coolwarm',\n",
        "        edgecolor='black'\n",
        "    )\n",
        "\n",
        "    # Add values on top of the bars\n",
        "    for index, row in delay_summary.iterrows():\n",
        "        axes[i].text(row.name, row['Delay_Rate_Pct'] + 0.5, f\"{row['Delay_Rate_Pct']:.1f}%\",\n",
        "                        color='black', ha=\"center\", fontsize=8)\n",
        "\n",
        "    axes[i].set_title(f'Delay Rate vs. {col.upper()}', fontsize=12)\n",
        "    axes[i].set_xlabel(x_label, fontsize=10)\n",
        "    axes[i].set_ylabel('Delay Rate (%)', fontsize=10)\n",
        "    axes[i].set_ylim(0, delay_summary['Delay_Rate_Pct'].max() + 5)\n",
        "    axes[i].tick_params(axis='x', rotation=15)\n",
        "    axes[i].grid(axis='y', linestyle=':', alpha=0.6)\n",
        "\n",
        "# Hide any unused subplots\n",
        "for i in range(n_cols, len(axes)):\n",
        "    fig.delaxes(axes[i])\n",
        "\n",
        "fig.suptitle('Impact of Individual Weather Factors', fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.subplots_adjust(bottom=1, right=0.8, top=1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ft4fEODwlKPy"
      },
      "outputs": [],
      "source": [
        "delay_col='DepDel15'\n",
        "weather_cols=['tavg', 'prcp']\n",
        "# Calculate the number of subplots needed\n",
        "n_cols = len(weather_cols)\n",
        "# Determine the number of rows based on the number of factors (2 plots per row)\n",
        "nrows = (n_cols + 1) // 2\n",
        "fig, axes = plt.subplots(\n",
        "    nrows=nrows,\n",
        "    ncols=2,\n",
        "    figsize=(15, 5 * nrows)\n",
        ")\n",
        "\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, col in enumerate(weather_cols):\n",
        "\n",
        "    # 1. Create Bins (Categories) for the continuous variable based on the factor\n",
        "\n",
        "    if col in ['tavg', 'tmin', 'tmax']:\n",
        "        # Custom bins for Temperature to capture cold and hot extremes (bimodal impact)\n",
        "        bins = [-np.inf, 30, 45, 75, 90, np.inf]\n",
        "        labels = ['Very Cold (<30¬∞F)', 'Cold (30-45¬∞F)', 'Moderate (45-75¬∞F)', 'Hot (75-90¬∞F)', 'Very Hot (>90¬∞F)']\n",
        "        # x_label = f'{col.upper()} Range (¬∞F)'\n",
        "        x_label = ''\n",
        "        df[f'{col}_Category'] = pd.cut(df[col], bins=bins, labels=labels, right=True)\n",
        "\n",
        "    elif col == 'wspd':\n",
        "        # Custom bins for Wind Speed to capture critical thresholds (nonlinear impact)\n",
        "        bins = [-np.inf, 10, 20, 30, np.inf]\n",
        "        labels = ['Low (<10 mph)', 'Medium (10-20 mph)', 'High (20-30 mph)', 'Severe (>30 mph)']\n",
        "        # x_label = 'Wind Speed Range (MPH)'\n",
        "        df[f'{col}_Category'] = pd.cut(df[col], bins=bins, labels=labels, right=True)\n",
        "\n",
        "\n",
        "    # 2. Calculate the delay rate (mean of DepDel15) for each new category\n",
        "    category_col = f'{col}_Category'\n",
        "    delay_summary = df.groupby(category_col)[delay_col].mean().reset_index()\n",
        "    delay_summary['Delay_Rate_Pct'] = delay_summary[delay_col] * 100\n",
        "\n",
        "    # 3. Plot the result\n",
        "    sns.barplot(\n",
        "        x=category_col,\n",
        "        y='Delay_Rate_Pct',\n",
        "        data=delay_summary,\n",
        "        ax=axes[i],\n",
        "        palette='coolwarm',\n",
        "        edgecolor='black'\n",
        "    )\n",
        "\n",
        "    # Add values on top of the bars\n",
        "    for index, row in delay_summary.iterrows():\n",
        "        axes[i].text(row.name, row['Delay_Rate_Pct'] + 0.5, f\"{row['Delay_Rate_Pct']:.1f}%\",\n",
        "                        color='black', ha=\"center\", fontsize=8)\n",
        "\n",
        "    axes[i].set_title(f'Delay Rate vs. {col.upper()}', fontsize=12)\n",
        "    axes[i].set_xlabel(x_label, fontsize=10)\n",
        "    axes[i].set_ylabel('Delay Rate (%)', fontsize=10)\n",
        "    axes[i].set_ylim(0, delay_summary['Delay_Rate_Pct'].max() + 5)\n",
        "    axes[i].tick_params(axis='x', rotation=15)\n",
        "    axes[i].grid(axis='y', linestyle=':', alpha=0.6)\n",
        "\n",
        "# Hide any unused subplots\n",
        "for i in range(n_cols, len(axes)):\n",
        "    fig.delaxes(axes[i])\n",
        "\n",
        "fig.suptitle('Impact of Individual Weather Factors', fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctdV_nN5lLSX"
      },
      "outputs": [],
      "source": [
        "df_delayed = df\n",
        "df['TotalDisruptionMinutes'] = np.where(\n",
        "    (df['Cancelled'] == 1.0) | (df['Diverted'] == 1.0),\n",
        "    1000,\n",
        "    df['DepDelayMinutes']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWv6o-ULlMPk"
      },
      "outputs": [],
      "source": [
        "# 1.1 IsSevereDisruption (Total disruption flag, used for reference)\n",
        "df['IsSevereDisruption'] = np.where(\n",
        "    (df['Cancelled'] == 1.0) | (df['Diverted'] == 1.0) | (df['DepDel15'] == 1.0),\n",
        "    1.0,\n",
        "    0.0\n",
        ")\n",
        "\n",
        "# 1.2 IsCancelledOrDiverted (The worst outcomes)\n",
        "df['IsCancelledOrDiverted'] = np.where(\n",
        "    (df['Cancelled'] == 1.0) | (df['Diverted'] == 1.0),\n",
        "    1.0,\n",
        "    0.0\n",
        ")\n",
        "\n",
        "# 1.3 IsDelayedOnly (15+ minute delays that were NOT Cancelled/Diverted)\n",
        "df['IsDelayedOnly'] = np.where(\n",
        "    (df['DepDel15'] == 1.0) & (df['IsCancelledOrDiverted'] == 0.0),\n",
        "    1.0,\n",
        "    0.0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCKL4bG9lNJe"
      },
      "outputs": [],
      "source": [
        "weather_cols = ['wspd']\n",
        "n_cols = len(weather_cols)\n",
        "\n",
        "# Set up the plot figure for a single bar chart\n",
        "fig, axes = plt.subplots(\n",
        "    nrows=1,\n",
        "    ncols=1,\n",
        "    figsize=(10, 7) # Increased size for clarity\n",
        ")\n",
        "axes = [axes]\n",
        "\n",
        "for i, col in enumerate(weather_cols):\n",
        "\n",
        "    # 1. Create Bins (Categories) for Wind Speed\n",
        "    if col == 'wspd':\n",
        "        bins = [-np.inf, 10, 20, 30, np.inf]\n",
        "        labels = ['Low (<10 mph)', 'Medium (10-20 mph)', 'High (20-30 mph)', 'Severe (>30 mph)']\n",
        "        x_label = 'Wind Speed Range (MPH)'\n",
        "        df[f'{col}_Category'] = pd.cut(df[col], bins=bins, labels=labels, right=True)\n",
        "\n",
        "    # 2. Calculate the rate for each mutually exclusive category\n",
        "    category_col = f'{col}_Category'\n",
        "    delay_summary = df.groupby(category_col).agg(\n",
        "        Cancelled_Diverted_Rate=('IsCancelledOrDiverted', 'mean'),\n",
        "        Delayed_Only_Rate=('IsDelayedOnly', 'mean'),\n",
        "        Total_Disruption_Rate=('IsSevereDisruption', 'mean') # Calculate total for annotations\n",
        "    ).reset_index()\n",
        "\n",
        "    # 3. Reshape the data for a stacked bar plot (melt)\n",
        "    summary_melted = delay_summary.melt(\n",
        "        id_vars=[category_col, 'Total_Disruption_Rate'],\n",
        "        value_vars=['Cancelled_Diverted_Rate', 'Delayed_Only_Rate'],\n",
        "        var_name='Disruption_Type',\n",
        "        value_name='Rate'\n",
        "    )\n",
        "    summary_melted['Rate_Pct'] = summary_melted['Rate'] * 100\n",
        "\n",
        "    # Rename types for legend clarity\n",
        "    summary_melted['Disruption_Type'] = summary_melted['Disruption_Type'].replace({\n",
        "        'Cancelled_Diverted_Rate': 'Cancellation / Diversion',\n",
        "        'Delayed_Only_Rate': '15+ Minute Delay Only'\n",
        "    })\n",
        "\n",
        "    # 4. Plot the stacked result\n",
        "    sns.barplot(\n",
        "        x=category_col,\n",
        "        y='Rate_Pct',\n",
        "        hue='Disruption_Type', # Use hue to create the segments\n",
        "        data=summary_melted,\n",
        "        ax=axes[i],\n",
        "        palette='Set2', # A clearer palette for two groups\n",
        "        edgecolor='black',\n",
        "        dodge=False # CRITICAL: Set to False to make the bars stack\n",
        "    )\n",
        "\n",
        "    axes[i].set_title(f'Composition of Flight Disruption by Wind Speed', fontsize=14)\n",
        "    axes[i].set_xlabel(x_label, fontsize=12)\n",
        "    axes[i].set_ylabel('Disruption Rate (%)', fontsize=12)\n",
        "    axes[i].set_ylim(0, delay_summary['Total_Disruption_Rate'].max() * 100 + 5)\n",
        "    axes[i].tick_params(axis='x', rotation=15)\n",
        "    axes[i].legend(title='Disruption Type')\n",
        "    axes[i].grid(axis='y', linestyle=':', alpha=0.6)\n",
        "\n",
        "fig.suptitle('Breakdown of Severe Disruption Risk (Delay vs. Cancellation/Diversion)', fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qSrDDuzlOL-"
      },
      "outputs": [],
      "source": [
        "delay_magnitude_col = 'TotalDisruptionMinutes'\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# --- Option A: Scatter Plot (Delay Magnitude vs. Wind Speed) ---\n",
        "sns.scatterplot(\n",
        "    x='wspd',\n",
        "    y=delay_magnitude_col,\n",
        "    data=df_delayed,\n",
        "    alpha=0.4,\n",
        "    ax=ax1,\n",
        "    color='darkblue'\n",
        ")\n",
        "# Add a linear regression line for context\n",
        "sns.regplot(\n",
        "    x='wspd',\n",
        "    y=delay_magnitude_col,\n",
        "    data=df_delayed,\n",
        "    scatter=False,\n",
        "    color='red',\n",
        "    line_kws={'linestyle': '--', 'label': 'Linear Trend'},\n",
        "    ax=ax1\n",
        ")\n",
        "\n",
        "ax1.set_title('Scatter Plot: Wind Speed (wspd) vs. Delay Magnitude', fontsize=14)\n",
        "ax1.set_xlabel('Wind Speed (Knots/MPH)', fontsize=12)\n",
        "ax1.set_ylabel('Delay Magnitude (Minutes)', fontsize=12)\n",
        "ax1.grid(axis='both', linestyle=':', alpha=0.6)\n",
        "\n",
        "# --- Option B: Box Plot (Delay Magnitude by Wind Speed Bin) ---\n",
        "wind_bins = [-np.inf, 10, 20, 30, np.inf]\n",
        "wind_labels = ['<10 MPH', '10-20 MPH', '20-30 MPH', '>30 MPH']\n",
        "\n",
        "# Use the same logic as in Slide 4 to create ordered bins\n",
        "df_delayed['Wind_Bin_Label'] = pd.cut(\n",
        "    df_delayed['wspd'],\n",
        "    bins=wind_bins,\n",
        "    labels=wind_labels,\n",
        "    right=True,\n",
        "    duplicates='drop'\n",
        ")\n",
        "\n",
        "sns.boxplot(\n",
        "    x='Wind_Bin_Label', # Use the fixed bins\n",
        "    y=delay_magnitude_col,\n",
        "    data=df_delayed,\n",
        "    ax=ax2,\n",
        "    palette='plasma',\n",
        "    notch=True\n",
        ")\n",
        "\n",
        "ax2.set_title('Box Plot: Delay Magnitude by Wind Speed Range', fontsize=14)\n",
        "ax2.set_xlabel('Wind Speed Range', fontsize=12)\n",
        "ax2.set_ylabel('Delay Magnitude (Minutes)', fontsize=12)\n",
        "ax2.set_xticklabels(ax2.get_xticklabels(), rotation=15, ha='right')\n",
        "ax2.grid(axis='y', linestyle=':', alpha=0.6)\n",
        "\n",
        "fig.suptitle('Correlation Between Continuous Weather Factors and Delay Magnitude', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.95]) # Adjust layout for suptitle\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjO7Vx45lPMr"
      },
      "source": [
        "### Heatmap After Removing Highly Correlated and Unwanted Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dpmt2m-VlPhn"
      },
      "outputs": [],
      "source": [
        "numeric_df = df.select_dtypes(include = ['number']).drop(columns = ['DepTime', 'Year', 'Quarter', 'DayofMonth', 'DivAirportLandings', 'DestAirportSeqID', 'Flights', 'OriginAirportID', 'OriginStateFips', 'DestWac', 'OriginWac', 'CRSDepTime', 'CRSArrTime', 'Cancelled', 'DestStateFips', 'DayOfWeek', 'Month', 'DOT_ID_Reporting_Airline', 'Flight_Number_Reporting_Airline', 'OriginCityMarketID', 'Diverted', 'OriginAirportSeqID', 'DestAirportID', 'DestCityMarketID'], errors = 'ignore')\n",
        "plt.figure(figsize = (30, 12))\n",
        "sns.heatmap(numeric_df.corr(method='spearman'), annot = True, cmap = \"coolwarm\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RoiXlgA0lTNt"
      },
      "outputs": [],
      "source": [
        "sns.histplot(df['DepDelayMinutes'], bins=100)\n",
        "plt.xlim(0, 300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OOAK2jelUkX"
      },
      "source": [
        "The extreme right skew, combined with the low correlation coefficients, indicates we need to perform some feature engineering to help the model find relations between weather data and delay lengths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXlV1p24lVNU"
      },
      "outputs": [],
      "source": [
        "percent_na = df.isna().mean() * 100\n",
        "print(percent_na.sort_values(ascending = False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8GSaQYmlWWa"
      },
      "source": [
        "### Delays Over Time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqnK1IDwlXPe"
      },
      "outputs": [],
      "source": [
        "# Ensure date column is datetime\n",
        "df['FlightDate'] = pd.to_datetime(df['FlightDate'])\n",
        "\n",
        "# ------------------------\n",
        "# 2Ô∏è‚É£ Aggregate: average delay per day per airport\n",
        "# ------------------------\n",
        "avg_delay = (\n",
        "    df.groupby(['FlightDate', 'Origin'])['DepDelayMinutes']\n",
        "      .mean()\n",
        "      .reset_index()\n",
        ")\n",
        "\n",
        "# ------------------------\n",
        "# 3Ô∏è‚É£ Get unique airports\n",
        "# ------------------------\n",
        "airports = avg_delay['Origin'].unique()\n",
        "num_airports = len(airports)\n",
        "\n",
        "# ------------------------\n",
        "# 4Ô∏è‚É£ Create subplots (one per airport)\n",
        "# ------------------------\n",
        "# Calculate number of rows and columns for subplots\n",
        "cols = 2  # Adjust if you want more columns\n",
        "rows = (num_airports + cols - 1) // cols\n",
        "\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(14, 4*rows), sharex=True, sharey=True)\n",
        "axes = axes.flatten()  # Flatten to easily index\n",
        "\n",
        "# Plot each airport\n",
        "for i, airport in enumerate(airports):\n",
        "    airport_data = avg_delay[avg_delay['Origin'] == airport]\n",
        "    sns.lineplot(\n",
        "        data=airport_data,\n",
        "        x='FlightDate',\n",
        "        y='DepDelayMinutes',\n",
        "        ax=axes[i],\n",
        "        color='blue'\n",
        "    )\n",
        "    axes[i].set_title(f\"Airport: {airport}\")\n",
        "    axes[i].set_xlabel(\"Date\")\n",
        "    axes[i].set_ylabel(\"Avg Departure Delay (mins)\")\n",
        "    axes[i].grid(True)\n",
        "\n",
        "# Remove unused subplots if any\n",
        "for j in range(i+1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle(\"Average Departure Delay Over Time by Airport\", y=1.02, fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlZiGF-wlp13"
      },
      "source": [
        "## Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zz49tPqYlZlM"
      },
      "source": [
        "### Transform Target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_JRGpxClboc"
      },
      "source": [
        "#### Log Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3F8ZM3KGlagW"
      },
      "outputs": [],
      "source": [
        "df['DepDelayMinutes_log'] = np.log1p(df['DepDelayMinutes'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pmh1-egDlf08"
      },
      "source": [
        "#### Box-Cox Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJ6gUHh-lhar"
      },
      "outputs": [],
      "source": [
        "df['DepDelay_shifted'] = df['DepDelayMinutes'] + 1e-3  # avoid zeros\n",
        "pt_boxcox = PowerTransformer(method='box-cox', standardize=False)\n",
        "df['DepDelayMinutes_boxcox'] = pt_boxcox.fit_transform(df['DepDelay_shifted'].values.reshape(-1,1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0N_WicjRlinA"
      },
      "source": [
        "#### Yeo-Johnson Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xovMTwrgllHq"
      },
      "outputs": [],
      "source": [
        "pt_yeojohnson = PowerTransformer(method='yeo-johnson', standardize=False)\n",
        "df['DepDelayMinutes_yeojohnson'] = pt_yeojohnson.fit_transform(df['DepDelayMinutes'].values.reshape(-1,1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wul10msils9v"
      },
      "source": [
        "### Extra Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tovfhxSBlo_K"
      },
      "outputs": [],
      "source": [
        "df['FlightDate'] = pd.to_datetime(df['FlightDate'])\n",
        "\n",
        "# WeatherCancellation: 1 if cancelled due to weather (code 'B'), else 0\n",
        "df['weatherCancellation'] = (df['Cancelled'] == 1) & (df['CancellationCode'] == 'B')\n",
        "\n",
        "# DayOfYear: Day of the year (1-365/366)\n",
        "df['DayOfYear'] = df['FlightDate'].dt.dayofyear\n",
        "\n",
        "# IsWeekend: 1 if the flight date is Saturday or Sunday, else 0\n",
        "df['IsWeekend'] = df['DayOfWeek'].isin([6,7]).astype(int)\n",
        "\n",
        "# IsHoliday: 1 if the flight date is a US federal holiday, else 0\n",
        "cal = USFederalHolidayCalendar()\n",
        "holidays = cal.holidays(start=df['FlightDate'].min(), end=df['FlightDate'].max())\n",
        "df['IsHoliday'] = df['FlightDate'].isin(holidays)\n",
        "\n",
        "# IsHolidayWindow: within specified day range of a holiday\n",
        "expanded_holidays = set()\n",
        "\n",
        "for d in range(25, 32):\n",
        "    expanded_holidays.add((5, d))   # Memorial Day\n",
        "for d in range(2, 7):\n",
        "    expanded_holidays.add((7, d))   # Independence Day\n",
        "for d in range(1, 8):\n",
        "    expanded_holidays.add((9, d))   # Labor Day\n",
        "for d in range(22, 29):\n",
        "    expanded_holidays.add((11, d))  # Thanksgiving\n",
        "for d in range(19, 32):\n",
        "    expanded_holidays.add((12, d))  # Christmas\n",
        "expanded_holidays.add((1, 1))       # New Year\n",
        "for d in range(28, 32):\n",
        "    expanded_holidays.add((3, d))    # Easter\n",
        "for d in range(1, 3):\n",
        "    expanded_holidays.add((4, d))    # Easter\n",
        "\n",
        "# Create a tuple column and check membership in set\n",
        "df[\"IsHolidayWindow\"] = list(zip(df[\"Month\"], df[\"DayofMonth\"]))\n",
        "df[\"IsHolidayWindow\"] = df[\"IsHolidayWindow\"].isin(expanded_holidays).astype(int)\n",
        "\n",
        "# NumDepartures: number of departures from the origin airport on that day\n",
        "daily_departures = (\n",
        "    df.groupby(['OriginAirportID', 'FlightDate'])\n",
        "      .size()\n",
        "      .rename('NumDepartures')\n",
        "      .reset_index()\n",
        ")\n",
        "\n",
        "df = df.merge(daily_departures, on=['OriginAirportID', 'FlightDate'], how='left')\n",
        "\n",
        "# Ensure Date column exists and is datetime\n",
        "df['Date'] = pd.to_datetime(df['FlightDate'])\n",
        "\n",
        "# Sort so rolling ops are time-consistent\n",
        "df = df.sort_values(['OriginAirportID', 'Date'])\n",
        "\n",
        "# RouteDelayMean_7d: Route-level rolling delay mean\n",
        "df['Route'] = df['OriginAirportID'].astype(str) + '_' + df['DestAirportID'].astype(str)\n",
        "df['RouteDelayMean_7d'] = (\n",
        "    df.groupby('Route')['DepDelayMinutes']\n",
        "      .transform(lambda x: x.shift().rolling(7, min_periods=1).mean())\n",
        ")\n",
        "\n",
        "# OriginDelayMean_7d: Airport-level (origin) rolling delay mean\n",
        "df['OriginDelayMean_7d'] = (\n",
        "    df.groupby('OriginAirportID')['DepDelayMinutes']\n",
        "      .transform(lambda x: x.shift().rolling(7, min_periods=1).mean())\n",
        ")\n",
        "\n",
        "# DestArrivals_7d: Destination congestion indicator\n",
        "df['DestArrivals_7d'] = (\n",
        "    df.groupby('DestAirportID')['Flight_Number_Reporting_Airline']\n",
        "      .transform(lambda x: x.shift().rolling(7, min_periods=1).count())\n",
        ")\n",
        "\n",
        "# Replace any remaining NaN (from shift/rolling) with reasonable defaults\n",
        "df.fillna({\n",
        "    'RouteDelayMean_7d': 0,\n",
        "    'OriginDelayMean_7d': 0,\n",
        "    'DestArrivals_7d': 0\n",
        "}, inplace=True)\n",
        "\n",
        "# Departures_Today: Day-level departure volume (can help with congestion)\n",
        "df['Departures_Today'] = df.groupby(['OriginAirportID', 'Date'])['Flight_Number_Reporting_Airline'].transform('count')\n",
        "\n",
        "# Interaction features (useful for nonlinear models like LightGBM)\n",
        "df['Dist_x_Wspd'] = df['Distance'] * df['wspd']\n",
        "df['TempRange'] = df['tmax'] - df['tmin']\n",
        "df['MonthxWeekday'] = df['Month'] * df['DayOfWeek']\n",
        "\n",
        "# CongestionRatio: Ratio of today's departures to average departures for that airport\n",
        "df['AvgDepartures_Past30d'] = (\n",
        "    df.groupby('OriginAirportID')['NumDepartures']\n",
        "      .transform(lambda x: x.shift().rolling(30, min_periods=1).mean())\n",
        ")\n",
        "df['CongestionRatio'] = df['NumDepartures'] / df['AvgDepartures_Past30d']\n",
        "\n",
        "# OriginDelayTrend_3d: Captures worsening congestion trend\n",
        "df['OriginDelayTrend_3d'] = df.groupby('OriginAirportID')['OriginDelayMean_7d'].transform(lambda x: x.diff(3))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIsWGA28lvjY"
      },
      "source": [
        "# Step 3: Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8T9jTXf4lw9V"
      },
      "source": [
        "## LightGBM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t45u2Jftlx6P"
      },
      "source": [
        "### Feature Selection & Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3F43mTZlmzW"
      },
      "outputs": [],
      "source": [
        "features = [\n",
        "\n",
        "    # 'Year', Removing Year to focus on dynamic predictors\n",
        "    'Month', 'DayOfWeek', 'Flight_Number_Reporting_Airline',\n",
        "    'OriginAirportID', 'OriginCityMarketID', 'OriginStateFips',\n",
        "    'DestAirportID', 'DestCityMarketID', 'DestStateFips',\n",
        "    'CRSDepTime', 'CRSArrTime', 'Distance', 'tavg', 'tmin', 'tmax',\n",
        "    'prcp', 'wspd', 'pres', 'IsWeekend', 'IsHoliday', 'IsHolidayWindow',\n",
        "    'NumDepartures', 'RouteDelayMean_7d', 'OriginDelayMean_7d',\n",
        "    'DestArrivals_7d', 'Departures_Today', 'Dist_x_Wspd', 'TempRange',\n",
        "    'MonthxWeekday',\n",
        "]\n",
        "# One-hot encoded categorical variables\n",
        "one_hot_cols = [col for col in df.columns if col.startswith('Origin_') or\n",
        "                col.startswith('Month_') or\n",
        "                col.startswith('DayOfWeek_') or\n",
        "                col.startswith('Dest_')]\n",
        "\n",
        "target = 'DepDelayMinutes_log'\n",
        "\n",
        "# Define X and y\n",
        "X = df[features + one_hot_cols]\n",
        "y = df[target]\n",
        "\n",
        "# Keep FlightDate in a separate series for splitting\n",
        "flight_dates = df['FlightDate']\n",
        "\n",
        "# Subsample for speed\n",
        "df_model = df[features + one_hot_cols + [target]].dropna().sample(500_000, random_state=42)\n",
        "flight_dates_model = flight_dates.loc[df_model.index]\n",
        "\n",
        "# Time-based split\n",
        "train_end = '2023-12-31'\n",
        "valid_end = '2024-12-31'\n",
        "\n",
        "train_mask = flight_dates_model <= train_end\n",
        "valid_mask = (flight_dates_model > train_end) & (flight_dates_model <= valid_end)\n",
        "test_mask = flight_dates_model > valid_end\n",
        "\n",
        "# Define X and y\n",
        "X = df_model[features + one_hot_cols]  # do NOT include FlightDate\n",
        "y = df_model[target]\n",
        "\n",
        "# Split and convert numeric to float32\n",
        "X_train = X[train_mask].astype(np.float32)\n",
        "X_valid = X[valid_mask].astype(np.float32)\n",
        "X_test  = X[test_mask].astype(np.float32)\n",
        "\n",
        "y_train = y[train_mask]\n",
        "y_valid = y[valid_mask]\n",
        "y_test  = y[test_mask]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8l2KPnfpl0ij"
      },
      "source": [
        "### Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcCg79hYl1fO"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "    params = {\n",
        "        'n_estimators': 2000,\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.05, log=True),\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 200),\n",
        "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 20, 500),\n",
        "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
        "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
        "        'lambda_l1': trial.suggest_float('lambda_l1', 0.0, 1.0),\n",
        "        'lambda_l2': trial.suggest_float('lambda_l2', 0.0, 1.0),\n",
        "        'random_state': 42,\n",
        "        'n_jobs': -1\n",
        "    }\n",
        "\n",
        "    model = lgbm(**params)\n",
        "    model.fit(\n",
        "        X_train, y_train,  # y_train is log-transformed\n",
        "        eval_set=[(X_valid, y_valid)],\n",
        "        eval_metric='mae',\n",
        "        callbacks=[early_stopping(stopping_rounds=100)]\n",
        "    )\n",
        "\n",
        "    # Back-transform predictions to minutes\n",
        "    y_pred_log = model.predict(X_valid)\n",
        "    y_pred = np.expm1(y_pred_log)\n",
        "    y_valid_orig = np.expm1(y_valid)\n",
        "\n",
        "    mae = mean_absolute_error(y_valid_orig, y_pred)\n",
        "    return mae  # Optuna minimizes MAE in minutes\n",
        "\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=10)  # adjust trials as needed\n",
        "\n",
        "print(\"Best hyperparameters:\")\n",
        "print(study.best_params)\n",
        "print(\"Best MAE:\", study.best_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfXqnnJrl41f"
      },
      "source": [
        "### Forecasting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrM_oKyyl7-r"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Train LightGBM\n",
        "# =========================\n",
        "best_params = study.best_params\n",
        "lgb = lgbm(**best_params)\n",
        "\n",
        "lgb.fit(X_train, y_train)\n",
        "\n",
        "# =========================\n",
        "# Predict, back-transform, and evaluate\n",
        "# =========================\n",
        "\n",
        "y_pred_log = lgb.predict(X_test)\n",
        "y_pred = np.expm1(y_pred_log)       # back to minutes\n",
        "y_test_original = np.expm1(y_test)  # back to minutes\n",
        "\n",
        "# Evaluate in log space\n",
        "mae_log = mean_absolute_error(y_test, y_pred_log)\n",
        "rmse_log = np.sqrt(mean_squared_error(y_test, y_pred_log))\n",
        "r2_log = r2_score(y_test, y_pred_log)\n",
        "\n",
        "# Evaluate in original minutes\n",
        "mae = mean_absolute_error(y_test_original, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_test_original, y_pred))\n",
        "r2 = r2_score(y_test_original, y_pred)\n",
        "\n",
        "print(\"\\nModel Performance (LightGBM):\")\n",
        "print(\"\\nIn Log Space:\")\n",
        "print(f\"R¬≤:  {r2_log:.4f}\")\n",
        "print(\"\\nIn Original Minutes:\")\n",
        "print(f\"MAE:  {mae:.2f}\")\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"R¬≤:   {r2:.4f}\")\n",
        "\n",
        "# =========================\n",
        "# Feature importance plot\n",
        "# =========================\n",
        "importances = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'Importance': lgb.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x='Importance', y='Feature', data=importances.head(15), palette='viridis', legend=False)\n",
        "plt.title(\"Top 15 Most Important Features (LightGBM)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ibw3bTkql-44"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOB13rJDmAZS"
      },
      "source": [
        "### Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1C3DCNcl_RH"
      },
      "outputs": [],
      "source": [
        "# --- New, Simplified Training Pipeline ---\n",
        "print(\"Starting classification training...\")\n",
        "\n",
        "# 1. Initialize a simple Classifier\n",
        "# We use 'class_weight' because most flights are NOT delayed\n",
        "rf_classifier = RandomForestClassifier(\n",
        "    n_estimators=100,        # Start simple\n",
        "    max_depth=10,            # A reasonable depth to prevent overfitting\n",
        "    min_samples_leaf=50,     # Don't let it learn from tiny groups\n",
        "    n_jobs=-1,               # Use all cores\n",
        "    random_state=42,\n",
        "    class_weight='balanced'  # <-- CRITICAL!\n",
        ")\n",
        "\n",
        "# 2. Train the model\n",
        "start_time = time.time()\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "print(f\"Training finished in {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "# 3. Get predictions for the Test set\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "y_pred_proba = rf_classifier.predict_proba(X_test)[:, 1] # Get 'Delayed' probability\n",
        "\n",
        "# 4. Print new Classification Metrics\n",
        "print(\"\\n--- Test Set Metrics ---\")\n",
        "print(f\"Accuracy:  {accuracy_score(y_test, y_pred):.3f}\")\n",
        "print(f\"Recall:    {recall_score(y_test, y_pred):.3f}\")     # <-- How many delays did we catch?\n",
        "print(f\"Precision: {precision_score(y_test, y_pred):.3f}\")  # <-- How many of our alarms were real?\n",
        "print(f\"ROC-AUC:   {roc_auc_score(y_test, y_pred_proba):.3f}\") # <-- The best overall metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wrrGdsOmCqL"
      },
      "source": [
        "### Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVJP_Rd9mEPC"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "    # 1. Define the parameters to search\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 100, 500), # Shortened for speed\n",
        "        'max_depth': trial.suggest_int('max_depth', 5, 20),\n",
        "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 10, 100),\n",
        "    }\n",
        "\n",
        "    # 2. Create the classifier\n",
        "    model = RandomForestClassifier(\n",
        "        **params,\n",
        "        n_jobs=-1,\n",
        "        random_state=42,\n",
        "        class_weight='balanced' # <-- Still critical\n",
        "    )\n",
        "\n",
        "    # 3. Train on a smaller sample to make tuning fast\n",
        "    X_train_sample = X_train.sample(n=100000, random_state=42)\n",
        "    y_train_sample = y_train.loc[X_train_sample.index]\n",
        "\n",
        "    model.fit(X_train_sample, y_train_sample)\n",
        "\n",
        "    # 4. Score it on the validation set\n",
        "    # We want to maximize the ROC-AUC score\n",
        "    val_preds_proba = model.predict_proba(X_val)[:, 1]\n",
        "    auc = roc_auc_score(y_val, val_preds_proba)\n",
        "\n",
        "    return auc\n",
        "\n",
        "# --- Run the study ---\n",
        "print(\"Starting Optuna study...\")\n",
        "# We want to MAXIMIZE the score\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=20) # Run for 20 trials\n",
        "\n",
        "print(\"\\nBest trial:\")\n",
        "print(f\"  Value (ROC-AUC): {study.best_value:.4f}\")\n",
        "print(\"  Params: \")\n",
        "for key, value in study.best_params.items():\n",
        "    print(f\"    {key}: {value}\")\n",
        "\n",
        "# You can now use these best_params in your final model\n",
        "best_params = study.best_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klBpPHrAmFgx"
      },
      "source": [
        "### Model Training & Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wtn3vjKOmFyX"
      },
      "outputs": [],
      "source": [
        "# --- FINAL: Train Tuned Model, Print Metrics, and Plot ---\n",
        "\n",
        "# 1. Initialize the FINAL Tuned Classifier\n",
        "print(\"Using best params from Optuna study...\")\n",
        "rf_classifier = RandomForestClassifier(\n",
        "    n_estimators=442,        # <-- From Optuna\n",
        "    max_depth=8,             # <-- From Optuna\n",
        "    min_samples_leaf=34,     # <-- From Optuna\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        "    class_weight='balanced'\n",
        ")\n",
        "\n",
        "# 2. Train the model\n",
        "print(\"Training the final model...\")\n",
        "start_time = time.time()\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "print(f\"Training finished in {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "# 3. Get predictions for the Test set\n",
        "print(\"Generating predictions...\")\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "y_pred_proba = rf_classifier.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# 4. Print new Classification Metrics\n",
        "print(\"\\n--- Test Set Metrics ---\")\n",
        "print(f\"Accuracy:  {accuracy_score(y_test, y_pred):.3f}\")\n",
        "print(f\"Recall:    {recall_score(y_test, y_pred):.3f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred):.3f}\")\n",
        "print(f\"ROC-AUC:   {roc_auc_score(y_test, y_pred_proba):.3f}\")\n",
        "\n",
        "# --- Plotting Results ---\n",
        "print(\"\\n--- Plotting Results ---\")\n",
        "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# 5. Plot Confusion Matrix\n",
        "ConfusionMatrixDisplay.from_estimator(\n",
        "    rf_classifier,\n",
        "    X_test,\n",
        "    y_test,\n",
        "    ax=ax[0],\n",
        "    cmap='Blues',\n",
        "    normalize='true'\n",
        ")\n",
        "ax[0].set_title('Confusion Matrix (Normalized)')\n",
        "\n",
        "# 6. Plot ROC Curve\n",
        "RocCurveDisplay.from_estimator(\n",
        "    rf_classifier,\n",
        "    X_test,\n",
        "    y_test,\n",
        "    ax=ax[1]\n",
        ")\n",
        "ax[1].set_title('ROC-AUC Curve')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-vnmXSGmHX5"
      },
      "source": [
        "## Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define & Compile Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kU_-NgjGmLWH"
      },
      "outputs": [],
      "source": [
        "# Create a Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Add the input layer using Input()\n",
        "model.add(Input(shape=(X_train.shape[1],)))\n",
        "model.add(Dense(units=128, activation='relu'))\n",
        "\n",
        "# Add hidden layers\n",
        "model.add(Dense(units=64, activation='relu'))\n",
        "model.add(Dense(units=32, activation='relu'))\n",
        "\n",
        "# Add the output layer (1 unit for regression, no activation)\n",
        "model.add(Dense(units=1))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse', 'mae'])\n",
        "\n",
        "print(\"Neural network model defined and compiled.\")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train with Early Stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instantiate an EarlyStopping callback\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    verbose=1,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=200,  # A reasonably large number, early stopping will stop it sooner if no improvement\n",
        "    batch_size=64,\n",
        "    validation_split=0.2, # Use 20% of training data for validation\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "print(\"Model training complete with early stopping.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the model on the test data\n",
        "loss, mse, mae = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "# Make predictions on the test set for R-squared calculation\n",
        "y_pred = model.predict(X_test).flatten()\n",
        "\n",
        "# Calculate R-squared\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Mean Squared Error (MSE): {mse:.4f}\")\n",
        "print(f\"Test Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "print(f\"Test R-squared: {r2:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the path to save the model in Google Drive\n",
        "model_save_path = '/content/drive/MyDrive/weather_prediction_model.h5'\n",
        "\n",
        "# Save the model\n",
        "model.save(model_save_path)\n",
        "\n",
        "print(f\"Model saved successfully to {model_save_path}\")\n",
        "     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separate DataFrame into rows where weatherScore is 0 and where it's not\n",
        "df_weather_score_zero = df[df['weatherScore'] == 0]\n",
        "df_weather_score_non_zero = df[df['weatherScore'] != 0]\n",
        "\n",
        "# Calculate the number of rows to sample (1/10th of zero weatherScore rows)\n",
        "sample_size = int(len(df_weather_score_zero) * x)\n",
        "\n",
        "# Randomly sample 1/10th of the zero weatherScore rows\n",
        "df_weather_score_zero_sampled = df_weather_score_zero.sample(n=sample_size, random_state=42)\n",
        "\n",
        "# Concatenate the sampled zero weatherScore rows with the non-zero weatherScore rows\n",
        "df_sampled = pd.concat([df_weather_score_non_zero, df_weather_score_zero_sampled])\n",
        "\n",
        "# Calculate the correlation of all columns with 'weatherScore' in the new sampled DataFrame\n",
        "correlation_with_weatherScore_sampled = df_sampled.corr(numeric_only=True)['weatherScore'].sort_values(ascending=False)\n",
        "\n",
        "print(f\"Original DataFrame size: {len(df)} rows\")\n",
        "print(f\"Rows with weatherScore = 0: {len(df_weather_score_zero)} rows\")\n",
        "print(f\"Rows with weatherScore != 0: {len(df_weather_score_non_zero)} rows\")\n",
        "print(f\"Sampled zero weatherScore rows: {len(df_weather_score_zero_sampled)} rows\")\n",
        "print(f\"New sampled DataFrame size: {len(df_sampled)} rows\")\n",
        "\n",
        "display(correlation_with_weatherScore_sampled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Create a binary target for classification\n",
        "#    0 if weatherScore is 0 (no weather delay), 1 if weatherScore > 0 (weather delay)\n",
        "y_binary = (df_sampled['weatherScore'] > 0).astype(int)\n",
        "\n",
        "# 2. Define features (X) by dropping 'weatherScore' from df_sampled\n",
        "X = df_sampled.drop('weatherScore', axis=1)\n",
        "\n",
        "# 3. Split X and y_binary into training and testing sets\n",
        "X_train, X_test, y_train_binary, y_test_binary = train_test_split(X, y_binary, test_size=0.2, random_state=42, stratify=y_binary)\n",
        "\n",
        "print(f\"Original training set shape: {X_train.shape}, Target distribution: {y_train_binary.value_counts()}\")\n",
        "\n",
        "# 4. Apply MinMaxScaler for feature scaling\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 5. Apply SMOTE for minority oversampling on the scaled training data\n",
        "sm = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = sm.fit_resample(X_train_scaled, y_train_binary)\n",
        "\n",
        "print(f\"Resampled training set shape: {X_train_res.shape}, Target distribution after SMOTE: {y_train_res.value_counts()}\")\n",
        "\n",
        "# 6. Initialize and train a Logistic Regression model\n",
        "logistic_model = LogisticRegression(solver='liblinear', random_state=42, max_iter=200)\n",
        "logistic_model.fit(X_train_res, y_train_res)\n",
        "\n",
        "# 7. Make predictions on the scaled test set\n",
        "y_pred_binary = logistic_model.predict(X_test_scaled)\n",
        "\n",
        "# 8. Evaluate the model\n",
        "accuracy = accuracy_score(y_test_binary, y_pred_binary)\n",
        "report = classification_report(y_test_binary, y_pred_binary)\n",
        "\n",
        "print(f\"\\nLogistic Regression Model Accuracy: {accuracy:.4f}\")\n",
        "print(\"\\nClassification Report:\\n\", report)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generalized Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply log transformation after adding 1 to handle zero values\n",
        "df['logScaledWeatherDelay'] = np.log(df['WeatherDelay'] + 1)\n",
        "\n",
        "# Display the head of the new column to verify\n",
        "print(df[['WeatherDelay', 'logScaledWeatherDelay']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Count values < 0\n",
        "count_less_than_zero = (df['logScaledWeatherDelay'] < 0).sum()\n",
        "\n",
        "# 2. Count values == 0\n",
        "count_exactly_zero = (df['logScaledWeatherDelay'] == 0).sum()\n",
        "\n",
        "# 3. Create bins for values > 0\n",
        "df_positive_delays = df[df['logScaledWeatherDelay'] > 0].copy()\n",
        "\n",
        "positive_delay_counts = pd.Series(dtype=int)\n",
        "\n",
        "if not df_positive_delays.empty:\n",
        "    max_positive_delay = df_positive_delays['logScaledWeatherDelay'].max()\n",
        "\n",
        "    # Define bin edges for pd.cut: (0, 50], (50, 100], etc.\n",
        "    positive_bin_edges = [0]\n",
        "    current_upper = 50\n",
        "    # Ensure the upper bound covers the max_delay by going slightly past it\n",
        "    while current_upper <= max_positive_delay + 50:\n",
        "        positive_bin_edges.append(current_upper)\n",
        "        current_upper += 50\n",
        "    positive_bin_edges.append(float('inf')) # Catch any values beyond the last explicit bin\n",
        "\n",
        "    # Create labels for these bins (e.g., '1-50', '51-100', etc.)\n",
        "    positive_bin_labels = []\n",
        "    for i in range(len(positive_bin_edges) - 1):\n",
        "        lower = positive_bin_edges[i]\n",
        "        upper = positive_bin_edges[i+1]\n",
        "        if upper == float('inf'):\n",
        "            positive_bin_labels.append(f'> {int(lower)}')\n",
        "        else:\n",
        "            positive_bin_labels.append(f'{int(lower+1)}-{int(upper)}')\n",
        "\n",
        "    # Cut the positive delays into these bins\n",
        "    positive_delay_counts = pd.cut(df_positive_delays['logScaledWeatherDelay'],\n",
        "                                   bins=positive_bin_edges,\n",
        "                                   labels=positive_bin_labels,\n",
        "                                   right=True, # (lower, upper]\n",
        "                                   include_lowest=False # Already filtered for > 0\n",
        "                                   ).value_counts().sort_index()\n",
        "\n",
        "# Combine all results into a single Series in the requested order\n",
        "final_distribution_dict = {'< 0': count_less_than_zero, '0': count_exactly_zero}\n",
        "\n",
        "# Add positive delay counts, ensuring correct order if possible (pd.Series handles this during construction)\n",
        "# Convert positive_delay_counts to a dictionary for easy merging\n",
        "final_distribution_dict.update(positive_delay_counts.to_dict())\n",
        "\n",
        "# Create a list of sorted keys to maintain consistent order for the final Series\n",
        "# This ensures categories like '1-50', '51-100' appear sequentially\n",
        "sorted_keys = ['< 0', '0'] + sorted([k for k in final_distribution_dict.keys() if k not in ['< 0', '0']],\n",
        "                                    key=lambda x: int(x.split('-')[0]) if '-' in x else int(x.split(' ')[1]) if '> ' in x else 0)\n",
        "\n",
        "weather_delay_distribution = pd.Series({k: final_distribution_dict[k] for k in sorted_keys})\n",
        "\n",
        "print(\"Weather Delay Distribution (counts):\")\n",
        "print(weather_delay_distribution)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter out rows where logScaledWeatherDelay is 0\n",
        "df_positive_log_delay = df[df['logScaledWeatherDelay'] > 0]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df_positive_log_delay['logScaledWeatherDelay'], bins=500, kde=True)\n",
        "plt.title('Distribution of Log-Scaled Weather Delay (Excluding Zeros)', fontsize=16)\n",
        "plt.xlabel('Log-Scaled Weather Delay', fontsize=12)\n",
        "plt.ylabel('Frequency', fontsize=12)\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Count values < 0\n",
        "count_less_than_zero = (df['logScaledWeatherDelay'] < 0).sum()\n",
        "\n",
        "# 2. Count values == 0\n",
        "count_exactly_zero = (df['logScaledWeatherDelay'] == 0).sum()\n",
        "\n",
        "# 3. Create bins for values > 0\n",
        "df_positive_delays = df[df['logScaledWeatherDelay'] > 0].copy()\n",
        "\n",
        "positive_delay_counts = pd.Series(dtype=int)\n",
        "\n",
        "if not df_positive_delays.empty:\n",
        "    max_positive_delay = df_positive_delays['logScaledWeatherDelay'].max()\n",
        "\n",
        "    # Define bin edges for pd.cut: (0, 50], (50, 100], etc.\n",
        "    positive_bin_edges = [0]\n",
        "    current_upper = 1\n",
        "    # Ensure the upper bound covers the max_delay by going slightly past it\n",
        "    while current_upper <= max_positive_delay + 1:\n",
        "        positive_bin_edges.append(current_upper)\n",
        "        current_upper += 1\n",
        "    positive_bin_edges.append(float('inf')) # Catch any values beyond the last explicit bin\n",
        "\n",
        "    # Create labels for these bins (e.g., '1-50', '51-100', etc.)\n",
        "    positive_bin_labels = []\n",
        "    for i in range(len(positive_bin_edges) - 1):\n",
        "        lower = positive_bin_edges[i]\n",
        "        upper = positive_bin_edges[i+1]\n",
        "        if upper == float('inf'):\n",
        "            positive_bin_labels.append(f'> {int(lower)}')\n",
        "        else:\n",
        "            positive_bin_labels.append(f'{int(lower+1)}-{int(upper)}')\n",
        "\n",
        "    # Cut the positive delays into these bins\n",
        "    positive_delay_counts = pd.cut(df_positive_delays['logScaledWeatherDelay'],\n",
        "                                   bins=positive_bin_edges,\n",
        "                                   labels=positive_bin_labels,\n",
        "                                   right=True, # (lower, upper]\n",
        "                                   include_lowest=False # Already filtered for > 0\n",
        "                                   ).value_counts().sort_index()\n",
        "\n",
        "# Combine all results into a single Series in the requested order\n",
        "final_distribution_dict = {'< 0': count_less_than_zero, '0': count_exactly_zero}\n",
        "\n",
        "# Add positive delay counts, ensuring correct order if possible (pd.Series handles this during construction)\n",
        "# Convert positive_delay_counts to a dictionary for easy merging\n",
        "final_distribution_dict.update(positive_delay_counts.to_dict())\n",
        "\n",
        "# Create a list of sorted keys to maintain consistent order for the final Series\n",
        "# This ensures categories like '1-50', '51-100' appear sequentially\n",
        "sorted_keys = ['< 0', '0'] + sorted([k for k in final_distribution_dict.keys() if k not in ['< 0', '0']],\n",
        "                                    key=lambda x: int(x.split('-')[0]) if '-' in x else int(x.split(' ')[1]) if '> ' in x else 0)\n",
        "\n",
        "weather_delay_distribution = pd.Series({k: final_distribution_dict[k] for k in sorted_keys})\n",
        "\n",
        "print(\"Weather Delay Distribution (counts):\")\n",
        "print(weather_delay_distribution)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter out rows where WeatherDelay is 0\n",
        "df_positive_original_delay = df[df['WeatherDelay'] > 0]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df_positive_original_delay['WeatherDelay'], bins=500, kde=True)\n",
        "plt.title('Distribution of Original Weather Delay (Excluding Zeros)', fontsize=16)\n",
        "plt.xlabel('Weather Delay (Minutes)', fontsize=12)\n",
        "plt.ylabel('Frequency', fontsize=12)\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find the maximum log-scaled weather delay\n",
        "max_log_scaled_weather_delay = df['logScaledWeatherDelay'].max()\n",
        "\n",
        "# Calculate weatherScore\n",
        "if max_log_scaled_weather_delay > 0:\n",
        "    df['weatherScore'] = (df['logScaledWeatherDelay'] / max_log_scaled_weather_delay) * 100\n",
        "else:\n",
        "    # If all logScaledWeatherDelay values are 0 (i.e., all original WeatherDelay were 0),\n",
        "    # then all weatherScore should be 0.\n",
        "    df['weatherScore'] = 0.0\n",
        "\n",
        "# Display the head of the new column to verify\n",
        "print(df[['WeatherDelay', 'logScaledWeatherDelay', 'weatherScore']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Replace weatherScore with 100 if weatherCancellation is True\n",
        "df.loc[df['weatherCancellation'] == True, 'weatherScore'] = 100\n",
        "\n",
        "# Display the head of the relevant columns to verify the change\n",
        "print(df[['weatherCancellation', 'weatherScore']].head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate the average WeatherDelay for each individual weatherScore point\n",
        "avg_weatherdelay_per_score_point = df.groupby('weatherScore')['WeatherDelay'].mean()\n",
        "\n",
        "# Convert the Series to a DataFrame for easier plotting with seaborn\n",
        "plot_data = avg_weatherdelay_per_score_point.reset_index()\n",
        "plot_data.columns = ['weatherScore', 'Average Weather Delay']\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.lineplot(x='weatherScore', y='Average Weather Delay', data=plot_data, marker='o')\n",
        "sns.scatterplot(x='weatherScore', y='Average Weather Delay', data=plot_data, color='red', s=50)\n",
        "\n",
        "plt.title('Average Weather Delay per Weather Score Point (including cancelled flights)', fontsize=16)\n",
        "plt.xlabel('Weather Score', fontsize=12)\n",
        "plt.ylabel('Average Weather Delay (Minutes)', fontsize=12)\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate the proportion of cancellations for each weatherScore\n",
        "cancellation_rate_per_score = df.groupby('weatherScore')['weatherCancellation'].mean().reset_index()\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.lineplot(x='weatherScore', y='weatherCancellation', data=cancellation_rate_per_score, marker='o')\n",
        "\n",
        "plt.title('Cancellation Rate vs. Weather Score', fontsize=16)\n",
        "plt.xlabel('Weather Score', fontsize=12)\n",
        "plt.ylabel('Proportion of Cancellations', fontsize=12)\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find the maximum weatherScore where the cancellation rate is not 100%\n",
        "# This will be the score just before the definitive spike at 100\n",
        "max_score_before_spike = cancellation_rate_per_score[cancellation_rate_per_score['weatherScore'] < 100]['weatherScore'].max()\n",
        "\n",
        "print(f\"The highest weatherScore before the cancellation rate spikes at 100 is: {max_score_before_spike}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confirm the count and the average delay at this score one more time:\n",
        "weather_cancelled_count = df[df['weatherCancellation'] == True].shape[0]\n",
        "average_delay_for_cancelled_score = df[df['weatherScore'] == 100]['WeatherDelay'].mean()\n",
        "\n",
        "print(f\"Number of flights with weatherCancellation=True: {weather_cancelled_count}\")\n",
        "print(f\"Average WeatherDelay for flights with weatherScore=100: {average_delay_for_cancelled_score:.2f} minutes\")\n",
        "print(\"\\nConclusion: A weatherScore of 100 directly indicates a weather-related cancellation.\")\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "weather_cancelled_flights = df[df['weatherCancellation'] == True]\n",
        "\n",
        "# Check if all weather-cancelled flights have a weatherScore of 100\n",
        "all_100_score = (weather_cancelled_flights['weatherScore'] == 100).all()\n",
        "\n",
        "# Print the result\n",
        "if all_100_score:\n",
        "    print(\"All weather-cancelled flights have a weatherScore of 100.\")\n",
        "else:\n",
        "    print(\"Not all weather-cancelled flights have a weatherScore of 100.\")\n",
        "\n",
        "# Optionally, print a count or head of such flights to verify\n",
        "print(f\"Number of weather-cancelled flights: {len(weather_cancelled_flights)}\")\n",
        "if not all_100_score and not weather_cancelled_flights.empty:\n",
        "    print(\"Weather-cancelled flights with weatherScore not equal to 100:\")\n",
        "    print(weather_cancelled_flights[weather_cancelled_flights['weatherScore'] != 100][['weatherCancellation', 'weatherScore']].head())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}