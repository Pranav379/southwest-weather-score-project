{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "221711d5",
   "metadata": {},
   "source": [
    "## Step 1: Data Collection and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541b61fb",
   "metadata": {},
   "source": [
    "### BTS Data for Southwest Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5bbf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from datetime import datetime\n",
    "\n",
    "SOUTHWEST_STATE_AIRPORTS = {\n",
    "    # Arizona\n",
    "    \"PHX\", \"TUS\", \"FLG\", \"YUM\", \"AZA\",\n",
    "    # New Mexico\n",
    "    \"ABQ\", \"SAF\", \"ROW\", \"HOB\",\n",
    "    # Texas\n",
    "    \"DFW\", \"DAL\", \"HOU\", \"AUS\", \"SAT\", \"ELP\", \"LBB\", \"MAF\", \"AMA\", \"CRP\", \"HRL\", \"MFE\", \"BRO\", \"TYR\", \"SJT\", \"ACT\",\n",
    "    # Oklahoma\n",
    "    \"OKC\", \"TUL\", \"LAW\", \"SWO\", \"END\",\n",
    "    # California\n",
    "    \"LAX\", \"SAN\", \"SFO\", \"OAK\", \"SJC\", \"BUR\", \"LGB\", \"SMF\", \"PSP\", \"SNA\", \"FAT\", \"ONT\", \"SBA\", \"BFL\"\n",
    "}\n",
    "\n",
    "OUTPUT_FOLDER = \"new_bts_data\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "def download_bts_data(year, month):\n",
    "    \"\"\"Download BTS data for a specific month\"\"\"\n",
    "    url = f'https://www.transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_{year}_{month}.zip'\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(\n",
    "            url, \n",
    "            headers={'User-Agent': 'Mozilla/5.0'},\n",
    "            timeout=300,\n",
    "            stream=True\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "            csv_file = next((name for name in z.namelist() if name.endswith('.csv')), None)\n",
    "            if not csv_file:\n",
    "                return (year, month, None, \"No CSV in zip\")\n",
    "            \n",
    "            with z.open(csv_file) as f:\n",
    "                df = pd.read_csv(f, encoding='utf-8', low_memory=False)\n",
    "            \n",
    "            df = df[\n",
    "                (df[\"Reporting_Airline\"] == \"WN\") &\n",
    "                (df[\"Origin\"].isin(SOUTHWEST_STATE_AIRPORTS))\n",
    "            ]\n",
    "            \n",
    "            if not df.empty:\n",
    "                month_filename = os.path.join(OUTPUT_FOLDER, f\"bts_wn_{year}_{month:02d}.csv\")\n",
    "                df.to_csv(month_filename, index=False)\n",
    "                print(f\"Saved {year}-{month:02d}: {len(df):,} rows to {month_filename}\")\n",
    "            \n",
    "            return (year, month, df, None)\n",
    "            \n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        return (year, month, None, f\"HTTP {e.response.status_code}\")\n",
    "    except Exception as e:\n",
    "        return (year, month, None, str(e)[:50])\n",
    "\n",
    "def download_years_parallel(start_year, end_year, max_workers=12):\n",
    "    \"\"\"Download multiple years of data with maximum parallelization\"\"\"\n",
    "    months_to_download = []\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        for month in range(1, 13):\n",
    "            current_date = datetime.now()\n",
    "            if year > current_date.year or (year == current_date.year and month > current_date.month):\n",
    "                continue\n",
    "            months_to_download.append((year, month))\n",
    "    \n",
    "    total_months = len(months_to_download)\n",
    "    print(f\"Downloading {total_months} months ({start_year}-{end_year})\")\n",
    "    print(f\"Using {max_workers} parallel workers\\n\")\n",
    "    \n",
    "    results = {}\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {\n",
    "            executor.submit(download_bts_data, year, month): (year, month)\n",
    "            for year, month in months_to_download\n",
    "        }\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            year, month, df, error = future.result()\n",
    "            if df is not None and not df.empty:\n",
    "                results[(year, month)] = df\n",
    "            else:\n",
    "                print(f\"✗ {year}-{month:02d}: {error or 'No matching rows'}\")\n",
    "    \n",
    "    elapsed = (datetime.now() - start_time).total_seconds() / 60\n",
    "    print(f\"\\nFinished in {elapsed:.1f} minutes\")\n",
    "    \n",
    "    successful = [df for df in results.values() if df is not None]\n",
    "    if successful:\n",
    "        combined_df = pd.concat(successful, ignore_index=True)\n",
    "        print(f\"Combined total: {len(combined_df):,} rows\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"No data downloaded\")\n",
    "        return None\n",
    "\n",
    "def save_to_csv_fast(df, filename):\n",
    "    \"\"\"Save filtered DataFrame to CSV\"\"\"\n",
    "    print(f\"\\nSaving combined dataset to {filename}...\")\n",
    "    start = datetime.now()\n",
    "    df.to_csv(filename, index=False)\n",
    "    elapsed = (datetime.now() - start).total_seconds()\n",
    "    size_mb = os.path.getsize(filename) / (1024 * 1024)\n",
    "    print(f\"Saved in {elapsed:.1f}s ({size_mb:.1f} MB)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    START_YEAR = 2015 # 10 years worth of data\n",
    "    END_YEAR = 2025\n",
    "    MAX_WORKERS = 10\n",
    "\n",
    "    combined_data = download_years_parallel(START_YEAR, END_YEAR, max_workers=MAX_WORKERS)\n",
    "    \n",
    "    if combined_data is not None:\n",
    "        output_file = os.path.join(OUTPUT_FOLDER, f\"bts_wn_southwest_combined_{START_YEAR}_{END_YEAR}.csv\")\n",
    "        save_to_csv_fast(combined_data, output_file)\n",
    "        print(f\"\\nCombined file saved as {output_file} ({len(combined_data):,} rows)\")\n",
    "        print(\"\\nHead of combined dataset:\")\n",
    "        print(combined_data.head())\n",
    "    else:\n",
    "        print(\"No data available after filtering.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b86904",
   "metadata": {},
   "source": [
    "### Meteostat Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bea1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from meteostat import Point, Daily\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "\n",
    "SWA_AIRPORTS = {\n",
    "    \"DAL\": (32.8471, -96.8517),\n",
    "    \"AUS\": (30.1975, -97.6664),\n",
    "    \"HOU\": (29.6454, -95.2789),\n",
    "    \"SAT\": (29.5337, -98.4698),\n",
    "    \"ELP\": (31.8070, -106.3779),\n",
    "    \"LBB\": (33.6609, -101.8214),\n",
    "    \"MAF\": (31.9369, -102.2016),\n",
    "    \"HRL\": (26.2285, -97.6544),\n",
    "    \"LAX\": (33.9425, -118.4081),\n",
    "    \"OAK\": (37.7126, -122.2197),\n",
    "    \"SAN\": (32.7338, -117.1933),\n",
    "    \"SJC\": (37.3639, -121.9289),\n",
    "    \"SMF\": (38.6950, -121.5908),\n",
    "    \"BUR\": (34.2007, -118.3587),\n",
    "    \"ONT\": (34.0559, -117.6009),\n",
    "    \"SNA\": (33.6757, -117.8682),\n",
    "    \"MCO\": (28.4312, -81.3081),\n",
    "    \"TPA\": (27.9755, -82.5332),\n",
    "    \"FLL\": (26.0726, -80.1527),\n",
    "    \"PBI\": (26.6832, -80.0956),\n",
    "    \"RSW\": (26.5362, -81.7552),\n",
    "    \"PNS\": (30.4734, -87.1867),\n",
    "    \"JAX\": (30.4941, -81.6879),\n",
    "    \"PHX\": (33.4342, -112.0116),\n",
    "    \"TUS\": (32.1161, -110.9410),\n",
    "    \"DEN\": (39.8561, -104.6737),\n",
    "    \"LAS\": (36.0801, -115.1522),\n",
    "    \"RNO\": (39.4993, -119.7681),\n",
    "    \"MDW\": (41.7868, -87.7522),\n",
    "    \"ATL\": (33.6407, -84.4277),\n",
    "    \"BWI\": (39.1754, -76.6684),\n",
    "    \"STL\": (38.7487, -90.3700),\n",
    "    \"MCI\": (39.2976, -94.7139),\n",
    "    \"BNA\": (36.1245, -86.6782),\n",
    "    \"MEM\": (35.0425, -89.9767),\n",
    "    \"ABQ\": (35.0494, -106.6172),\n",
    "    \"OKC\": (35.3931, -97.6008),\n",
    "    \"TUL\": (36.1986, -95.8880),\n",
    "    \"MSY\": (29.9934, -90.2580),\n",
    "    \"RDU\": (35.8776, -78.7875),\n",
    "    \"CLT\": (35.2140, -80.9431),\n",
    "    \"PHL\": (39.8719, -75.2411),\n",
    "    \"PIT\": (40.4915, -80.2328),\n",
    "    \"BOS\": (42.3656, -71.0096),\n",
    "    \"BUF\": (42.9405, -78.7322),\n",
    "    \"ALB\": (42.7483, -73.8026),\n",
    "    \"ROC\": (43.1181, -77.6721),\n",
    "    \"ISP\": (40.7953, -73.1000),\n",
    "    \"CMH\": (39.9973, -82.8876),\n",
    "    \"CLE\": (41.4117, -81.8498),\n",
    "    \"IND\": (39.7173, -86.2944),\n",
    "    \"MSP\": (44.8848, -93.2223),\n",
    "    \"MKE\": (42.9473, -87.8960),\n",
    "    \"DTW\": (42.2162, -83.3554),\n",
    "    \"GRR\": (42.8808, -85.5228),\n",
    "    \"SLC\": (40.7884, -111.9777),\n",
    "    \"SEA\": (47.4490, -122.3093),\n",
    "    \"GEG\": (47.6269, -117.5331),\n",
    "    \"PDX\": (45.5898, -122.5951),\n",
    "    \"ICT\": (37.6528, -97.4331),\n",
    "    \"LIT\": (34.7275, -92.2241),\n",
    "    \"SDF\": (38.1740, -85.7368),\n",
    "    \"DSM\": (41.5341, -93.6600),\n",
    "    \"OMA\": (41.3025, -95.8941),\n",
    "    \"BHM\": (33.5629, -86.7536),\n",
    "    \"RIC\": (37.5052, -77.3191),\n",
    "    \"ORF\": (36.8946, -76.2015),\n",
    "    \"BDL\": (41.9389, -72.6839),\n",
    "    \"PVD\": (41.7246, -71.4283),\n",
    "    \"MHT\": (42.9327, -71.4350)\n",
    "}\n",
    "\n",
    "OUTPUT_DIR = Path(\"meteostat_daily\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "START_YEAR = 2015\n",
    "END_YEAR = 2025\n",
    "MAX_WORKERS = 4\n",
    "\n",
    "def fetch_airport_daily(airport, coords):\n",
    "    lat, lon = coords\n",
    "    point = Point(lat, lon)\n",
    "    all_data = []\n",
    "    \n",
    "    for year in range(START_YEAR, END_YEAR + 1):\n",
    "        start = datetime(year, 1, 1)\n",
    "        end = datetime(year, 12, 31)\n",
    "        try:\n",
    "            data = Daily(point, start, end).fetch()\n",
    "            if not data.empty:\n",
    "                data = data.reset_index().rename(columns = {\"time\": \"date\"})\n",
    "                data['airport'] = airport\n",
    "                all_data.append(data)\n",
    "                print(f\"Fetched {airport} {year} ({len(data)} rows)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {airport} {year}: {e}\")\n",
    "        time.sleep(0.05)\n",
    "    \n",
    "    if all_data:\n",
    "        df = pd.concat(all_data)\n",
    "        out_file = OUTPUT_DIR / f\"{airport}.csv\"\n",
    "        df.to_csv(out_file, index=True)\n",
    "        print(f\"Saved {airport} to {out_file}\")\n",
    "\n",
    "def main():\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        futures = [executor.submit(fetch_airport_daily, airport, coords)\n",
    "                   for airport, coords in SWA_AIRPORTS.items()]\n",
    "        for fut in as_completed(futures):\n",
    "            fut.result()\n",
    "\n",
    "    all_files = list(OUTPUT_DIR.glob(\"*.csv\"))\n",
    "    combined = pd.concat((pd.read_csv(f, index_col=0) for f in all_files), ignore_index=True)\n",
    "    combined.to_csv(OUTPUT_DIR / \"weather_data.csv\", index=False)\n",
    "    print(f\"\\nAll airports combined: {len(combined)} rows\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8f0289",
   "metadata": {},
   "source": [
    "### Joining BTS and Meteostat Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f1b609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "with zipfile.ZipFile(\"combined_data.zip\") as z:\n",
    "    csv_name = next(name for name in z.namelist() if name.endswith(\".csv\"))\n",
    "    flights = pl.scan_csv(io.BytesIO(z.read(csv_name)), low_memory=True)\n",
    "\n",
    "weather = pl.scan_csv(\"Meteostat_daily_2000-2025.csv\", low_memory=True)\n",
    "\n",
    "joined = flights.join(\n",
    "    weather,\n",
    "    how=\"inner\",\n",
    "    left_on=[\"Origin\", \"FlightDate\"],\n",
    "    right_on=[\"airport\", \"date\"]\n",
    ")\n",
    "\n",
    "joined.sink_csv(\"joined_data.csv\")\n",
    "\n",
    "print(\"Streaming join completed successfully. File saved as 'joined_data.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a49cb9",
   "metadata": {},
   "source": [
    "### Converting to Imperial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a191d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"Data/FINAL_DATASET.csv\")\n",
    "\n",
    "# Temperature: Convert °C to °F\n",
    "for col in [\"tavg\", \"tmin\", \"tmax\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = (df[col] * 9/5 + 32).round().astype(\"Int64\")\n",
    "\n",
    "# Precipitation & Snow: Convert mm to inches\n",
    "for col in [\"prcp\", \"snow\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col] / 25.4\n",
    "\n",
    "# Wind Speed: Convert km/h to mph\n",
    "if \"wspd\" in df.columns:\n",
    "    df[\"wspd\"] = df[\"wspd\"] / 1.60934\n",
    "\n",
    "# Pressure: Convert hPa to inHg\n",
    "if \"pres\" in df.columns:\n",
    "    df[\"pres\"] = df[\"pres\"] * 0.02953\n",
    "\n",
    "print(df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd13681",
   "metadata": {},
   "source": [
    "### Treating Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346e2f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage NA by column\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "percent_na = df.isna().mean() * 100\n",
    "print(percent_na.sort_values(ascending = False).head(75))\n",
    "\n",
    "# Removing >95% missing cols and IATA Code column due to redundancy\n",
    "cols_to_drop = percent_na[percent_na > 95].index\n",
    "df = df.drop(columns = cols_to_drop)\n",
    "df = df.drop(columns = [\"IATA_CODE_Reporting_Airline\"])\n",
    "\n",
    "print(f\"\\nDropped {len(cols_to_drop)} columns\")\n",
    "print(\"Remaining columns: \", len(df.columns))\n",
    "\n",
    "# Printing percentage again\n",
    "percent_na = df.isna().mean() * 100\n",
    "print(percent_na.sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec5a6ce",
   "metadata": {},
   "source": [
    "### Heatmap After Removing Highly Correlated and Unwanted Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3152f374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "df = df.drop(columns = ['Tail_Number', 'ArrDel15', 'DepDelay', 'WheelsOn', 'WheelsOff', 'TaxiIn', 'TaxiOut', 'ArrTime', 'ArrDelay', 'ArrDelayMinutes', 'ArrivalDelayGroups', 'CRSElapsedTime', 'ActualElapsedTime', 'AirTime', 'DistanceGroup'])\n",
    "numeric_df = df.select_dtypes(include = ['number']).drop(columns = ['DepTime', 'Year', 'Quarter', 'DayofMonth', 'DivAirportLandings', 'DestAirportSeqID', 'Flights', 'OriginAirportID', 'OriginStateFips', 'DestWac', 'OriginWac', 'CRSDepTime', 'CRSArrTime', 'Cancelled', 'DestStateFips', 'DayOfWeek', 'Month', 'DOT_ID_Reporting_Airline', 'Flight_Number_Reporting_Airline', 'OriginCityMarketID', 'Diverted', 'OriginAirportSeqID', 'DestAirportID', 'DestCityMarketID'], errors = 'ignore')\n",
    "plt.figure(figsize = (30, 12))\n",
    "sns.heatmap(numeric_df.corr(), annot = True, cmap = \"coolwarm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6250911b",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_na = df.isna().mean() * 100\n",
    "print(percent_na.sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e313cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dimension: \", df.shape)\n",
    "# tavg missing rows\n",
    "num_missing = df['tavg'].isna().sum()\n",
    "print(\"tavg missing rows:\", num_missing)\n",
    "\n",
    "# wspd missing rows\n",
    "num_missing = df['tmin'].isna().sum()\n",
    "print(\"wspd missing rows:\", num_missing)\n",
    "\n",
    "# tmax missing rows\n",
    "num_missing = df['tmax'].isna().sum()\n",
    "print(\"tmax missing rows:\", num_missing)\n",
    "\n",
    "# tmin missing rows\n",
    "num_missing = df['tmin'].isna().sum()\n",
    "print(\"tmin missing rows:\", num_missing)\n",
    "\n",
    "# pres missing rows\n",
    "num_missing = df['pres'].isna().sum()\n",
    "print(\"pres missing rows:\", num_missing)\n",
    "\n",
    "# tmin missing rows\n",
    "num_missing = df['prcp'].isna().sum()\n",
    "print(\"prcp missing rows:\", num_missing)\n",
    "\n",
    "# snow missing rows\n",
    "num_missing = df['snow'].isna().sum()\n",
    "print(\"snow missing rows:\", num_missing)\n",
    "\n",
    "# delay missing rows\n",
    "num_missing = df['DepDelayMinutes'].isna().sum()\n",
    "print(\"delay missing rows:\", num_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7078aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['tavg'] = df['tavg'].fillna(((df['tmin'] + df['tmax']) / 2).round().astype('Int64'))\n",
    "\n",
    "# Snow\n",
    "# df['snow'] = df['snow'].fillna(\n",
    "#     df.groupby(['Origin', 'FlightDate'])['snow'].transform('mean')\n",
    "# )\n",
    "\n",
    "# df['tmin'] = df['tmin'].fillna(\n",
    "#     df.groupby(['Origin', 'FlightDate'])['tmin'].transform('mean')\n",
    "# )\n",
    "\n",
    "# df['tmax'] = df['tmax'].fillna(\n",
    "#     df.groupby(['Origin', 'FlightDate'])['tmax'].transform('mean')\n",
    "# )\n",
    "\n",
    "# df['wspd'] = df['wspd'].fillna(\n",
    "#     df.groupby(['Origin', 'FlightDate'])['wspd'].transform('mean')\n",
    "# )\n",
    "\n",
    "# df['pres'] = df['pres'].fillna(\n",
    "#     df.groupby(['Origin', 'FlightDate'])['pres'].transform('mean')\n",
    "# )\n",
    "\n",
    "# df['prcp'] = df['prcp'].fillna(\n",
    "#     df.groupby(['Origin', 'FlightDate'])['prcp'].transform('mean')\n",
    "# )\n",
    "\n",
    "# df['snow'] = df['snow'].fillna(\n",
    "#     df.groupby('Origin')['snow'].transform('mean')\n",
    "# )\n",
    "\n",
    "# df['DepDelayMinutes'] = df['DepDelayMinutes'].fillna(\n",
    "#     df.groupby(['Origin', 'FlightDate'])['DepDelayMinutes'].transform('mean')\n",
    "# )\n",
    "\n",
    "df = df.dropna(subset = ['DepDelayMinutes', 'snow', 'wspd', 'tmax', 'tmin', 'pres', 'prcp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2ee727",
   "metadata": {},
   "source": [
    "## Delays Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57745b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Ensure date column is datetime\n",
    "df['FlightDate'] = pd.to_datetime(df['FlightDate'])\n",
    "\n",
    "# ------------------------\n",
    "# 2️⃣ Aggregate: average delay per day per airport\n",
    "# ------------------------\n",
    "avg_delay = (\n",
    "    df.groupby(['FlightDate', 'Origin'])['DepDelayMinutes']\n",
    "      .mean()\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "# ------------------------\n",
    "# 3️⃣ Get unique airports\n",
    "# ------------------------\n",
    "airports = avg_delay['Origin'].unique()\n",
    "num_airports = len(airports)\n",
    "\n",
    "# ------------------------\n",
    "# 4️⃣ Create subplots (one per airport)\n",
    "# ------------------------\n",
    "# Calculate number of rows and columns for subplots\n",
    "cols = 2  # Adjust if you want more columns\n",
    "rows = (num_airports + cols - 1) // cols\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(14, 4*rows), sharex=True, sharey=True)\n",
    "axes = axes.flatten()  # Flatten to easily index\n",
    "\n",
    "# Plot each airport\n",
    "for i, airport in enumerate(airports):\n",
    "    airport_data = avg_delay[avg_delay['Origin'] == airport]\n",
    "    sns.lineplot(\n",
    "        data=airport_data,\n",
    "        x='FlightDate',\n",
    "        y='DepDelayMinutes',\n",
    "        ax=axes[i],\n",
    "        color='blue'\n",
    "    )\n",
    "    axes[i].set_title(f\"Airport: {airport}\")\n",
    "    axes[i].set_xlabel(\"Date\")\n",
    "    axes[i].set_ylabel(\"Avg Departure Delay (mins)\")\n",
    "    axes[i].grid(True)\n",
    "\n",
    "# Remove unused subplots if any\n",
    "for j in range(i+1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Average Departure Delay Over Time by Airport\", y=1.02, fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75693293",
   "metadata": {},
   "source": [
    "## Step 2: Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974142b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Select relevant numeric features\n",
    "features = ['Distance', 'wspd', 'tavg', 'prcp', 'pres', 'snow', 'tmin', 'tmax']\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "one_hot_cols = [col for col in df.columns if col.startswith('Origin_') or col.startswith('Month_') or col.startswith('DayOfWeek_') or col.startswith('Dest_')]\n",
    "\n",
    "# Target variable\n",
    "target = 'DepDelayMinutes'\n",
    "\n",
    "# Define X and y\n",
    "X = df[features + one_hot_cols]\n",
    "y = df[target]\n",
    "\n",
    "X_sample = X.sample(500_000, random_state = 42)\n",
    "y_sample = y.loc[X_sample.index]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_sample, y_sample, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# 5️⃣ Train Random Forest\n",
    "# =========================\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    max_depth=30,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# =========================\n",
    "# 6️⃣ Evaluate model\n",
    "# =========================\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "# rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"✅ Model Performance:\")\n",
    "print(f\"MAE:  {mae:.2f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"R²:   {r2:.3f}\")\n",
    "\n",
    "# =========================\n",
    "# 7️⃣ Feature importance plot\n",
    "# =========================\n",
    "importances = pd.DataFrame({\n",
    "    'Feature': X_sample.columns,\n",
    "    'Importance': rf.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x='Importance', y='Feature', data=importances.head(15), palette='viridis')\n",
    "plt.title(\"Top 15 Most Important Features (Random Forest)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
