{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "from datetime import datetime\n",
    "from meteostat import Point, Daily\n",
    "from pathlib import Path\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BTS Data for Southwest Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOUTHWEST_STATE_AIRPORTS = {\n",
    "    # Arizona\n",
    "    \"PHX\", \"TUS\", \"FLG\", \"YUM\", \"AZA\",\n",
    "    # New Mexico\n",
    "    \"ABQ\", \"SAF\", \"ROW\", \"HOB\",\n",
    "    # Texas\n",
    "    \"DFW\", \"DAL\", \"HOU\", \"AUS\", \"SAT\", \"ELP\", \"LBB\", \"MAF\", \"AMA\", \"CRP\", \"HRL\", \"MFE\", \"BRO\", \"TYR\", \"SJT\", \"ACT\",\n",
    "    # Oklahoma\n",
    "    \"OKC\", \"TUL\", \"LAW\", \"SWO\", \"END\",\n",
    "    # California\n",
    "    \"LAX\", \"SAN\", \"SFO\", \"OAK\", \"SJC\", \"BUR\", \"LGB\", \"SMF\", \"PSP\", \"SNA\", \"FAT\", \"ONT\", \"SBA\", \"BFL\"\n",
    "}\n",
    "\n",
    "OUTPUT_FOLDER = \"new_bts_data\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "def download_bts_data(year, month):\n",
    "    \"\"\"Download BTS data for a specific month\"\"\"\n",
    "    url = f'https://www.transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_{year}_{month}.zip'\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(\n",
    "            url, \n",
    "            headers={'User-Agent': 'Mozilla/5.0'},\n",
    "            timeout=300,\n",
    "            stream=True\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "            csv_file = next((name for name in z.namelist() if name.endswith('.csv')), None)\n",
    "            if not csv_file:\n",
    "                return (year, month, None, \"No CSV in zip\")\n",
    "            \n",
    "            with z.open(csv_file) as f:\n",
    "                df = pd.read_csv(f, encoding='utf-8', low_memory=False)\n",
    "            \n",
    "            df = df[\n",
    "                (df[\"Reporting_Airline\"] == \"WN\") &\n",
    "                (df[\"Origin\"].isin(SOUTHWEST_STATE_AIRPORTS))\n",
    "            ]\n",
    "            \n",
    "            if not df.empty:\n",
    "                month_filename = os.path.join(OUTPUT_FOLDER, f\"bts_wn_{year}_{month:02d}.csv\")\n",
    "                df.to_csv(month_filename, index=False)\n",
    "                print(f\"Saved {year}-{month:02d}: {len(df):,} rows to {month_filename}\")\n",
    "            \n",
    "            return (year, month, df, None)\n",
    "            \n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        return (year, month, None, f\"HTTP {e.response.status_code}\")\n",
    "    except Exception as e:\n",
    "        return (year, month, None, str(e)[:50])\n",
    "\n",
    "def download_years_parallel(start_year, end_year, max_workers=12):\n",
    "    \"\"\"Download multiple years of data with maximum parallelization\"\"\"\n",
    "    months_to_download = []\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        for month in range(1, 13):\n",
    "            current_date = datetime.now()\n",
    "            if year > current_date.year or (year == current_date.year and month > current_date.month):\n",
    "                continue\n",
    "            months_to_download.append((year, month))\n",
    "    \n",
    "    total_months = len(months_to_download)\n",
    "    print(f\"Downloading {total_months} months ({start_year}-{end_year})\")\n",
    "    print(f\"Using {max_workers} parallel workers\\n\")\n",
    "    \n",
    "    results = {}\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {\n",
    "            executor.submit(download_bts_data, year, month): (year, month)\n",
    "            for year, month in months_to_download\n",
    "        }\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            year, month, df, error = future.result()\n",
    "            if df is not None and not df.empty:\n",
    "                results[(year, month)] = df\n",
    "            else:\n",
    "                print(f\"âœ— {year}-{month:02d}: {error or 'No matching rows'}\")\n",
    "    \n",
    "    elapsed = (datetime.now() - start_time).total_seconds() / 60\n",
    "    print(f\"\\nFinished in {elapsed:.1f} minutes\")\n",
    "    \n",
    "    successful = [df for df in results.values() if df is not None]\n",
    "    if successful:\n",
    "        combined_df = pd.concat(successful, ignore_index=True)\n",
    "        print(f\"Combined total: {len(combined_df):,} rows\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"No data downloaded\")\n",
    "        return None\n",
    "\n",
    "def save_to_csv_fast(df, filename):\n",
    "    \"\"\"Save filtered DataFrame to CSV\"\"\"\n",
    "    print(f\"\\nSaving combined dataset to {filename}...\")\n",
    "    start = datetime.now()\n",
    "    df.to_csv(filename, index=False)\n",
    "    elapsed = (datetime.now() - start).total_seconds()\n",
    "    size_mb = os.path.getsize(filename) / (1024 * 1024)\n",
    "    print(f\"Saved in {elapsed:.1f}s ({size_mb:.1f} MB)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    START_YEAR = 2015 # 10 years worth of data\n",
    "    END_YEAR = 2025\n",
    "    MAX_WORKERS = 10\n",
    "\n",
    "    combined_data = download_years_parallel(START_YEAR, END_YEAR, max_workers=MAX_WORKERS)\n",
    "    \n",
    "    if combined_data is not None:\n",
    "        output_file = os.path.join(OUTPUT_FOLDER, f\"bts_wn_southwest_combined_{START_YEAR}_{END_YEAR}.csv\")\n",
    "        save_to_csv_fast(combined_data, output_file)\n",
    "        print(f\"\\nCombined file saved as {output_file} ({len(combined_data):,} rows)\")\n",
    "        print(\"\\nHead of combined dataset:\")\n",
    "        print(combined_data.head())\n",
    "    else:\n",
    "        print(\"No data available after filtering.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meteostat Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SWA_AIRPORTS = {\n",
    "    \"DAL\": (32.8471, -96.8517),\n",
    "    \"AUS\": (30.1975, -97.6664),\n",
    "    \"HOU\": (29.6454, -95.2789),\n",
    "    \"SAT\": (29.5337, -98.4698),\n",
    "    \"ELP\": (31.8070, -106.3779),\n",
    "    \"LBB\": (33.6609, -101.8214),\n",
    "    \"MAF\": (31.9369, -102.2016),\n",
    "    \"HRL\": (26.2285, -97.6544),\n",
    "    \"LAX\": (33.9425, -118.4081),\n",
    "    \"OAK\": (37.7126, -122.2197),\n",
    "    \"SAN\": (32.7338, -117.1933),\n",
    "    \"SJC\": (37.3639, -121.9289),\n",
    "    \"SMF\": (38.6950, -121.5908),\n",
    "    \"BUR\": (34.2007, -118.3587),\n",
    "    \"ONT\": (34.0559, -117.6009),\n",
    "    \"SNA\": (33.6757, -117.8682),\n",
    "    \"MCO\": (28.4312, -81.3081),\n",
    "    \"TPA\": (27.9755, -82.5332),\n",
    "    \"FLL\": (26.0726, -80.1527),\n",
    "    \"PBI\": (26.6832, -80.0956),\n",
    "    \"RSW\": (26.5362, -81.7552),\n",
    "    \"PNS\": (30.4734, -87.1867),\n",
    "    \"JAX\": (30.4941, -81.6879),\n",
    "    \"PHX\": (33.4342, -112.0116),\n",
    "    \"TUS\": (32.1161, -110.9410),\n",
    "    \"DEN\": (39.8561, -104.6737),\n",
    "    \"LAS\": (36.0801, -115.1522),\n",
    "    \"RNO\": (39.4993, -119.7681),\n",
    "    \"MDW\": (41.7868, -87.7522),\n",
    "    \"ATL\": (33.6407, -84.4277),\n",
    "    \"BWI\": (39.1754, -76.6684),\n",
    "    \"STL\": (38.7487, -90.3700),\n",
    "    \"MCI\": (39.2976, -94.7139),\n",
    "    \"BNA\": (36.1245, -86.6782),\n",
    "    \"MEM\": (35.0425, -89.9767),\n",
    "    \"ABQ\": (35.0494, -106.6172),\n",
    "    \"OKC\": (35.3931, -97.6008),\n",
    "    \"TUL\": (36.1986, -95.8880),\n",
    "    \"MSY\": (29.9934, -90.2580),\n",
    "    \"RDU\": (35.8776, -78.7875),\n",
    "    \"CLT\": (35.2140, -80.9431),\n",
    "    \"PHL\": (39.8719, -75.2411),\n",
    "    \"PIT\": (40.4915, -80.2328),\n",
    "    \"BOS\": (42.3656, -71.0096),\n",
    "    \"BUF\": (42.9405, -78.7322),\n",
    "    \"ALB\": (42.7483, -73.8026),\n",
    "    \"ROC\": (43.1181, -77.6721),\n",
    "    \"ISP\": (40.7953, -73.1000),\n",
    "    \"CMH\": (39.9973, -82.8876),\n",
    "    \"CLE\": (41.4117, -81.8498),\n",
    "    \"IND\": (39.7173, -86.2944),\n",
    "    \"MSP\": (44.8848, -93.2223),\n",
    "    \"MKE\": (42.9473, -87.8960),\n",
    "    \"DTW\": (42.2162, -83.3554),\n",
    "    \"GRR\": (42.8808, -85.5228),\n",
    "    \"SLC\": (40.7884, -111.9777),\n",
    "    \"SEA\": (47.4490, -122.3093),\n",
    "    \"GEG\": (47.6269, -117.5331),\n",
    "    \"PDX\": (45.5898, -122.5951),\n",
    "    \"ICT\": (37.6528, -97.4331),\n",
    "    \"LIT\": (34.7275, -92.2241),\n",
    "    \"SDF\": (38.1740, -85.7368),\n",
    "    \"DSM\": (41.5341, -93.6600),\n",
    "    \"OMA\": (41.3025, -95.8941),\n",
    "    \"BHM\": (33.5629, -86.7536),\n",
    "    \"RIC\": (37.5052, -77.3191),\n",
    "    \"ORF\": (36.8946, -76.2015),\n",
    "    \"BDL\": (41.9389, -72.6839),\n",
    "    \"PVD\": (41.7246, -71.4283),\n",
    "    \"MHT\": (42.9327, -71.4350)\n",
    "}\n",
    "\n",
    "OUTPUT_DIR = Path(\"meteostat_daily\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "START_YEAR = 2015\n",
    "END_YEAR = 2025\n",
    "MAX_WORKERS = 4\n",
    "\n",
    "def fetch_airport_daily(airport, coords):\n",
    "    lat, lon = coords\n",
    "    point = Point(lat, lon)\n",
    "    all_data = []\n",
    "    \n",
    "    for year in range(START_YEAR, END_YEAR + 1):\n",
    "        start = datetime(year, 1, 1)\n",
    "        end = datetime(year, 12, 31)\n",
    "        try:\n",
    "            data = Daily(point, start, end).fetch()\n",
    "            if not data.empty:\n",
    "                data = data.reset_index().rename(columns = {\"time\": \"date\"})\n",
    "                data['airport'] = airport\n",
    "                all_data.append(data)\n",
    "                print(f\"Fetched {airport} {year} ({len(data)} rows)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {airport} {year}: {e}\")\n",
    "        time.sleep(0.05)\n",
    "    \n",
    "    if all_data:\n",
    "        df = pd.concat(all_data)\n",
    "        out_file = OUTPUT_DIR / f\"{airport}.csv\"\n",
    "        df.to_csv(out_file, index=True)\n",
    "        print(f\"Saved {airport} to {out_file}\")\n",
    "\n",
    "def main():\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        futures = [executor.submit(fetch_airport_daily, airport, coords)\n",
    "                   for airport, coords in SWA_AIRPORTS.items()]\n",
    "        for fut in as_completed(futures):\n",
    "            fut.result()\n",
    "\n",
    "    all_files = list(OUTPUT_DIR.glob(\"*.csv\"))\n",
    "    combined = pd.concat((pd.read_csv(f, index_col=0) for f in all_files), ignore_index=True)\n",
    "    combined.to_csv(OUTPUT_DIR / \"weather_data.csv\", index=False)\n",
    "    print(f\"\\nAll airports combined: {len(combined)} rows\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining BTS and Meteostat Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(\"combined_data.zip\") as z:\n",
    "    csv_name = next(name for name in z.namelist() if name.endswith(\".csv\"))\n",
    "    flights = pl.scan_csv(io.BytesIO(z.read(csv_name)), low_memory=True)\n",
    "\n",
    "weather = pl.scan_csv(\"Meteostat_daily_2000-2025.csv\", low_memory=True)\n",
    "\n",
    "joined = flights.join(\n",
    "    weather,\n",
    "    how=\"inner\",\n",
    "    left_on=[\"Origin\", \"FlightDate\"],\n",
    "    right_on=[\"airport\", \"date\"]\n",
    ")\n",
    "\n",
    "joined.sink_csv(\"joined_data.csv\")\n",
    "\n",
    "print(\"Streaming join completed successfully. File saved as 'joined_data.csv'\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
